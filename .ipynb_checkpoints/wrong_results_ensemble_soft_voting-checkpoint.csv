,text,true_labels,pred_labels
0,"  Network virtualization techniques allow for the coexistence of many virtual
networks (VNs) jointly sharing the resources of an underlying substrate
network. The Virtual Network Embedding problem (VNE) arises when looking for
the most profitable set of VNs to embed onto the substrate. In this paper, we
address the offline version of the problem. We propose a Mixed-Integer Linear
Programming formulation to solve it to optimality which accounts for acceptance
and rejection of virtual network requests, allowing for both splittable and
unsplittable (single path) routing schemes. Our formulation also considers a
Rent-at-Bulk (RaB) model for the rental of substrate capacities where economies
of scale apply. To better emphasize the importance of RaB, we also compare our
method to a baseline one which only takes RaB into account a posteriori, once a
solution to VNE, oblivious to RaB, has been found. Computational experiments
show the viability of our approach, stressing the relevance of addressing RaB
directly with an exact formulation
",1,3
1,"  In a significant minority of cases, certain pronouns, especially the pronoun
it, can be used without referring to any specific entity. This phenomenon of
pleonastic pronoun usage poses serious problems for systems aiming at even a
shallow understanding of natural language texts. In this paper, a novel
approach is proposed to identify such uses of it: the extrapositional cases are
identified using a series of queries against the web, and the cleft cases are
identified using a simple set of syntactic rules. The system is evaluated with
four sets of news articles containing 679 extrapositional cases as well as 78
cleft constructs. The identification results are comparable to those obtained
by human efforts.
",6,1
2,"  In modern programming languages, exception handling is an effective mechanism
to avoid unexpected runtime errors. Thus, failing to catch and handle
exceptions could lead to serious issues like system crashing, resource leaking,
or negative end-user experiences. However, writing correct exception handling
code is often challenging in mobile app development due to the fast-changing
nature of API libraries for mobile apps and the insufficiency of their
documentation and source code examples. Our prior study shows that in practice
mobile app developers cause many exception-related bugs and still use bad
exception handling practices (e.g. catch an exception and do nothing). To
address such problems, in this paper, we introduce two novel techniques for
recommending correct exception handling code. One technique, XRank, recommends
code to catch an exception likely occurring in a code snippet. The other,
XHand, recommends correction code for such an occurring exception. We have
developed ExAssist, a code recommendation tool for exception handling using
XRank and XHand. The empirical evaluation shows that our techniques are highly
effective. For example, XRank has top-1 accuracy of 70% and top-3 accuracy of
87%. XHand's results are 89% and 96%, respectively.
",5,3
3,"  Developer forums contain opinions and information related to the usage of
APIs. API names in forum posts are often not explicitly linked to their
official resources. Automatic linking of an API mention to its official
resources can be challenging for various reasons, such as, name overloading. We
present a technique, ANACE, to automatically resolve API mentions in the
textual contents of forum posts. Given a database of APIs, we first detect all
words in a forum post that are potential references to an API. We then use a
combination of heuristics and machine learning to eliminate false positives and
to link true positives to the actual APIs and their resources.
",5,1
4,"  In this paper we propose a cycle redundancy technique that provides optical
networks almost fault-tolerant point-to-point and multipoint-to-multipoint
communications. The technique more importantly is shown to approximately halve
the necessary light-trail resources in the network while maintaining the
fault-tolerance and dependability expected from cycle-based routing. For
efficiency and distributed control, it is common in distributed systems and
algorithms to group nodes into intersecting sets referred to as quorum sets.
Optimal communication quorum sets forming optical cycles based on light-trails
have been shown to flexibly and efficiently route both point-to-point and
multipoint-to-multipoint traffic requests. Commonly cycle routing techniques
will use pairs of cycles to achieve both routing and fault-tolerance, which
uses substantial resources and creates the potential for underutilization.
Instead, we intentionally utilize redundancy within the quorum cycles for
fault-tolerance such that almost every point-to-point communication occurs in
more than one cycle. The result is a set of cycles with 96.60% - 99.37% fault
coverage, while using 42.9% - 47.18% fewer resources.
",4,3
5,"  We propose a new dataset for evaluating question answering models with
respect to their capacity to reason about beliefs. Our tasks are inspired by
theory-of-mind experiments that examine whether children are able to reason
about the beliefs of others, in particular when those beliefs differ from
reality. We evaluate a number of recent neural models with memory augmentation.
We find that all fail on our tasks, which require keeping track of inconsistent
states of the world; moreover, the models' accuracy decreases notably when
random sentences are introduced to the tasks at test.
",3,2
6,"  Public networks are exposed to port scans from the Internet. Attackers search
for vulnerable services they can exploit. In large scan campaigns, attackers
often utilize different machines to perform distributed scans, which impedes
their detection and might also camouflage the actual goal of the scanning
campaign. In this paper, we present a correlation algorithm to detect scans,
identify potential relations among them, and reassemble them to larger
campaigns. We evaluate our approach on real-world Internet traffic and our
results indicate that it can summarize and characterize standalone and
distributed scan campaigns based on their tools and intention.
",5,1
7,"  Cognitive radio networks (CRNs) have emerged as a promising solution to
enhance spectrum utilization by using unused or less used spectrum in radio
environments. The basic idea of CRNs is to allow secondary users (SUs) access
to licensed spectrum, under the condition that the interference perceived by
the primary users (PUs) is minimal. In CRNs, the channel availability is
uncertainty due to the existence of PUs, resulting in intermittent
communication. Transmission control protocol (TCP) performance may
significantly degrade in such conditions. To address the challenges, some
transport protocols have been proposed for reliable transmission in CRNs. In
this paper we survey the state-of-the-art transport protocols for CRNs. We
firstly highlight the unique aspects of CRNs, and describe the challenges of
transport protocols in terms of PU behavior, spectrum sensing, spectrum
changing and TCP mechanism itself over CRNs. Then, we provide a summary and
comparison of existing transport protocols for CRNs. Finally, we discuss
several open issues and research challenges. To the best of our knowledge, our
work is the first survey on transport protocols for CRNs.
",2,3
8,"  We show that the edit distance between two run-length encoded strings of
compressed lengths $m$ and $n$ respectively, can be computed in
$\mathcal{O}(mn\log(mn))$ time. This improves the previous record by a factor
of $\mathcal{O}(n/\log(mn))$. The running time of our algorithm is within
subpolynomial factors of being optimal, subject to the standard SETH-hardness
assumption. This effectively closes a line of algorithmic research first
started in 1993.
",1,6
9,"  We study an online knapsack problem where the items arrive sequentially and
must be either immediately packed into the knapsack or irrevocably discarded.
Each item has a different size and the objective is to maximize the total size
of items packed. We focus on the class of randomized algorithms which initially
draw a threshold from some distribution, and then pack every fitting item whose
size is at least that threshold. Threshold policies satisfy many desiderata
including simplicity, fairness, and incentive-alignment. We derive two optimal
threshold distributions, the first of which implies a competitive ratio of
0.432 relative to the optimal offline packing, and the second of which implies
a competitive ratio of 0.428 relative to the optimal fractional packing. We
also consider the generalization to multiple knapsacks, where an arriving item
has a different size in each knapsack and must be placed in at most one. This
is equivalent to the AdWords problem where item truncation is not allowed. We
derive a randomized threshold algorithm for this problem which is
0.214-competitive. We also show that any randomized algorithm for this problem
cannot be more than 0.461-competitive, providing the first upper bound which is
strictly less than 0.5. This online knapsack problem finds applications in many
areas, like supply chain ordering, online advertising, and healthcare
scheduling, refugee integration, and crowdsourcing. We show how our optimal
threshold distributions can be naturally implemented in the warehouses for a
Latin American chain department store. We run simulations on their large-scale
order data, which demonstrate the robustness of our proposed algorithms.
",4,6
10,"  Seasoned Excel developers were invited to participate in a challenge to
implement a spreadsheet with multi-dimensional variables. We analyzed their
spreadsheet to see the different implement strategies employed. We identified
two strategies: most participants used a projection of three or
four-dimensional variables on the two-dimensional plane used by Excel. A few
participants used a database approach where the multi-dimensional variables are
presented in the form of a dataset table with the appropriate primary key. This
approach leads to simpler formulas.
",3,4
11,"  In this paper, we introduce the concept of a virtual machine with
graph-organised memory as a versatile backend for both explicit-state and
abstraction-driven verification of software. Our virtual machine uses the LLVM
IR as its instruction set, enriched with a small set of hypercalls. We show
that the provided hypercalls are sufficient to implement a small operating
system, which can then be linked with applications to provide a
POSIX-compatible verification environment. Finally, we demonstrate the
viability of the approach through a comparison with a more
traditionally-designed LLVM model checker.
",1,2
12,"  Multilingual text processing is useful because the information content found
in different languages is complementary, both regarding facts and opinions.
While Information Extraction and other text mining software can, in principle,
be developed for many languages, most text analysis tools have only been
applied to small sets of languages because the development effort per language
is large. Self-training tools obviously alleviate the problem, but even the
effort of providing training data and of manually tuning the results is usually
considerable. In this paper, we gather insights by various multilingual system
developers on how to minimise the effort of developing natural language
processing applications for many languages. We also explain the main guidelines
underlying our own effort to develop complex text mining software for tens of
languages. While these guidelines - most of all: extreme simplicity - can be
very restrictive and limiting, we believe to have shown the feasibility of the
approach through the development of the Europe Media Monitor (EMM) family of
applications (http://emm.newsbrief.eu/overview.html). EMM is a set of complex
media monitoring tools that process and analyse up to 100,000 online news
articles per day in between twenty and fifty languages. We will also touch upon
the kind of language resources that would make it easier for all to develop
highly multilingual text mining applications. We will argue that - to achieve
this - the most needed resources would be freely available, simple, parallel
and uniform multilingual dictionaries, corpora and software tools.
",2,5
13,"  This paper considers peer-to-peer scheduling for a network with multiple
wireless devices. A subset of the devices are mobile users that desire specific
files. Each user may already have certain popular files in its cache. The
remaining devices are access points that typically have access to a larger set
of files. Users can download packets of their requested file from an access
point or from a nearby user. Our prior work optimizes peer scheduling in a
general setting, but the resulting delay can be large when applied to mobile
networks. This paper focuses on the mobile case, and develops a new algorithm
that reduces delay by opportunistically grabbing packets from current
neighbors. However, it treats a simpler model where each user desires a single
file with infinite length. An algorithm that provably optimizes throughput
utility while incentivizing participation is developed for this case. The
algorithm extends as a simple heuristic in more general cases with finite file
sizes and random active and idle periods.
",2,6
14,"  BIP70 is the Bitcoin payment protocol for communication between a merchant
and a pseudonymous customer. McCorry et al. (FC~2016) showed that BIP70 is
prone to refund attacks and proposed a fix that requires the customer to sign
their refund request. They argued that this minimal change will provide
resistance against refund attacks. In this paper, we point out the drawbacks of
McCorry et al.'s fix and propose a new approach for protection against refund
attacks using the Bitcoin multi-signature mechanism. Our solution does not rely
on merchants storing refund requests, and unlike the previous solution, allows
updating refund addresses through email. We discuss the security of our
proposed method and compare it with the previous solution. We also propose a
novel application of our refund mechanism in providing anonymity for payments
between a payer and payee in which merchants act as mixing servers. We finally
discuss how to combine the above two mechanisms in a single payment protocol to
have an anonymous payment protocol secure against refund attacks.
",6,0
15,"  Although public clouds still occupy the largest portion of the total cloud
infrastructure, private clouds are attracting increasing interest from both
industry and academia because of their better security and privacy control.
According to the existing studies, the high upfront cost is among the most
critical challenges associated with private clouds. To reduce cost and improve
performance, virtual machine placement (VMP) methods have been extensively
investigated, however, few of these methods have focused on private clouds.
This paper proposes a heterogeneous and multidimensional clairvoyant dynamic
bin packing (CDBP) model, in which the scheduler can conduct more efficient VMP
processes using additional information on the arrival time and duration of
virtual machines to reduce the datacenter scale and thereby decrease the
upfront cost of private clouds. In addition, a novel branch-and-bound algorithm
with a divide-and-conquer strategy (DCBB) is proposed to effectively and
efficiently handle the derived problem. One state-of-the-art and several
classic VMP methods are also modified to adapt to the proposed model to observe
their performance and compare with our proposed algorithm. Extensive
experiments are conducted on both real-world and synthetic workloads to
evaluate the accuracy and efficiency of the algorithms. The experimental
results demonstrate that DCBB delivers near-optimal solutions with a
convergence rate that is much faster than those of the other search-based
algorithms evaluated. In particular, DCBB yields the optimal solution for a
real-world workload with an execution time that is an order of magnitude
shorter than that required by the original branch-and-bound (BB) algorithm.
",2,1
16,"  Virtualization promises significant benefits in security, efficiency,
dependability, and cost. Achieving these benefits depends upon the reliability
of the underlying virtual machine monitors (hypervisors). This paper describes
an ongoing project to develop and verify MinVisor, a simple but functional
Type-I x86 hypervisor, proving protection properties at the assembly level
using ACL2. Originally based on an existing research hypervisor, MinVisor
provides protection of its own memory from a malicious guest. Our long-term
goal is to fully verify MinVisor, providing a vehicle to investigate the
modeling and verification of hypervisors at the implementation level, and also
a basis for further systems research. Functional segments of the MinVisor C
code base are translated into Y86 assembly, and verified with respect to the
Y86 model. The inductive assertions (also known as ""compositional cutpoints"")
methodology is used to prove the correctness of the code. The proof of the code
that sets up the nested page tables is described. We compare this project to
related efforts in systems code verification and outline some useful steps
forward.
",2,6
17,"  In this paper we provide an abstract model theory for the untyped
differential lambda-calculus and the resource calculus. In particular we
propose a general definition of model of these calculi, namely the notion of
linear reflexive object in a Cartesian closed differential category. Examples
of models based on relations are provided.
",3,5
18,"  The detection of a volumetric attack involves collecting statistics on the
network traffic, and identifying suspicious activities. We assume that
available statistical information includes the number of packets and the number
of bytes passed per flow. We apply methods of machine learning to detect
malicious traffic. A prototype project is implemented as a module for the
Floodlight controller. The prototype was tested on the Mininet simulation
platform. The simulated topology includes a number of edge switches, a
connected graph of core switches, and a number of server and user hosts. The
server hosts run simple web servers. The user hosts simulate web clients. The
controller employs Dijkstra's algorithm to find the best flow in the graph. The
controller periodically polls the edge switches and provides current and
historical statistics on each active flow. The streaming analytics evaluates
the traffic volume and detects volumetric attacks.
",1,3
19,"  Internet-scale services rely on data partitioning and replication to provide
scalable performance and high availability. Moreover, to reduce user-perceived
response times and tolerate disasters (i.e., the failure of a whole
datacenter), services are increasingly becoming geographically distributed.
Data partitioning and replication, combined with local and geographical
distribution, introduce daunting challenges, including the need to carefully
order requests among replicas and partitions. One way to tackle this problem is
to use group communication primitives that encapsulate order requirements. This
paper presents a detailed performance evaluation of Multi-Ring Paxos, a
scalable group communication primitive. We focus our analysis on ""extreme
conditions"" with deployments including high-end 10 Gbps networks, a large
number of combined rings (i.e., independent Paxos instances), a large number of
replicas in a ring, and a global deployment. We also report on the performance
of recovery under peak load and present two novel extensions to boost
Multi-Ring Paxos's performance.
",1,2
20,"  Access control is an important component for web services such as a cloud.
Current clouds tend to design the access control mechanism together with the
policy language on their own. It leads to two issues: (i) a cloud user has to
learn different policy languages to use multiple clouds, and (ii) a cloud
service provider has to customize an authorization mechanism based on its
business requirement, which brings high development cost. In this work, a new
access control policy language called PERM modeling language (PML) is proposed
to express various access control models such as access control list (ACL),
role-based access control (RBAC) and attribute-based access control (ABAC),
etc. PML's enforcement mechanism is designed in an interpreter-on-interpreter
manner, which not only secures the authorization code with sandboxing, but also
extends PML to all programming languages that support Lua. PML is already
adopted by real-world projects such as Intel's RMD, VMware's Dispatch, Orange's
Gobis and so on, which proves PML's usability. The performance evaluation on
OpenStack, CloudStack and Amazon Web Services (AWS) shows PML's enforcement
overhead per request is under 5.9us.
",6,4
21,"  Selectional preferences have long been claimed to be essential for
coreference resolution. However, they are mainly modeled only implicitly by
current coreference resolvers. We propose a dependency-based embedding model of
selectional preferences which allows fine-grained compatibility judgments with
high coverage. We show that the incorporation of our model improves coreference
resolution performance on the CoNLL dataset, matching the state-of-the-art
results of a more complex system. However, it comes with a cost that makes it
debatable how worthwhile such improvements are.
",6,3
22,"  We focus on the problem of adding fault-tolerance to an existing concurrent
protocol in the presence of {\em unchangeable environment actions}. Such
unchangeable actions occur in practice due to several reasons. One instance
includes the case where only a subset of the components/processes can be
revised and other components/processes must be as is. Another instance includes
cyber-physical systems where revising physical components may be undesirable or
impossible. These actions differ from faults in that they are simultaneously
{\em assistive} and {\em disruptive}, whereas faults are only disruptive. For
example, if these actions are a part of a physical component, their execution
is essential for the normal operation of the system. However, they can
potentially disrupt actions taken by other components for dealing with faults.
Also, one can typically assume that fault actions will stop for a long enough
time for the program to make progress. Such an assumption is impossible in this
context.
  We present algorithms for adding stabilizing fault-tolerance, failsafe
fault-tolerance and masking fault-tolerance. Interestingly, we observe that the
previous approaches for adding stabilizing fault-tolerance and masking
fault-tolerance cannot be easily extended in this context. However, we find
that the overall complexity of adding these levels of fault-tolerance remains
in P (in the state space of the program). We also demonstrate that our
algorithms are sound and complete.
",5,1
23,"  Research in the field of blockchain technology and applications is increasing
at a fast pace. Although the Bitcoin whitepaper by Nakamoto is already ten
years old, the field can still be seen as immature and at an early stage.
Current research in this area is lacking a commonly shared knowledge and
consensus about terms used to describe the technology and its properties. At
the same time this research is challenging fundamental aspects of the Bitcoin
core concept. It has to be questioned whether all of these new approaches still
adequately could be described as blockchain technology. We propose to use the
term Decentralized Consensus Technology as a general category instead.
Decentralized Consensus Technology consists of decentralized ledger and
non-ledger technologies. Blockchain technology in turn is only one of multiple
implementations of the Decentralized Ledger Technology. Furthermore, we
identified three main characteristics of Decentralized Consensus Technology:
decentralization, trustlessness and ability to eventually reach consensus.
Depending on the use case of the specific implementation the following
additional properties have to be considered: privacy, participation incentive,
irreversibility and immutability, operation purpose, confirmation time,
transaction costs, ability to externalize transactions and computations and
scalability possibilities.
",5,1
24,"  Given a directed graph $G$ with arbitrary real-valued weights, the single
source shortest-path problem (SSSP) asks for, given a source $s$ in $G$,
finding a shortest path from $s$ to each vertex $v$ in $G$. A classical SSSP
algorithm detects a negative cycle of $G$ or constructs a shortest-path tree
(SPT) rooted at $s$ in $O(mn)$ time, where $m,n$ are the numbers of edges and
vertices in $G$ respectively. In many practical applications, new constraints
come from time to time and we need to update the SPT frequently. Given an SPT
$T$ of $G$, suppose the weight on a certain edge is modified. We show by
rigorous proof that the well-known {\sf Ball-String} algorithm for positively
weighted graphs can be adapted to solve the dynamic SPT problem for directed
graphs with arbitrary weights. Let $n_0$ be the number of vertices that are
affected (i.e., vertices that have different distances from $s$ or different
parents in the input and output SPTs) and $m_0$ the number of edges incident to
an affected vertex. The adapted algorithms terminate in $O(m_0+n_0 \log n_0)$
time, either detecting a negative cycle (only in the decremental case) or
constructing a new SPT $T'$ for the updated graph. We show by an example that
the output SPT $T'$ may have more than necessary edge changes to $T$. To remedy
this, we give a general method for transforming $T'$ into an SPT with minimal
edge changes in time $O(n_0)$ provided that $G$ has no cycles with zero length.
",5,1
25,"  Dead Reckoning mechanisms are usually used to estimate the position of
simulated entity in virtual environment. However, this technique often ignores
available contextual information that may be influential to the state of an
entity, sacrificing remote predictive accuracy in favor of low computational
complexity. A novel extension of Dead Reckoning is suggested in this paper to
increase the network availability and fulfill the required Quality of Service
in large scale distributed simulation application. The proposed algorithm is
referred to as ANFIS Dead Reckoning, which stands for Adaptive Neuro-based
Fuzzy Inference System Dead Reckoning is based on a fuzzy inference system
which is trained by the learning algorithm derived from the neuronal networks
and fuzzy inference theory. The proposed mechanism takes its based on the
optimization approach to calculate the error threshold violation in networking
games. Our model shows it primary benefits especially in the decision making of
the behavior of simulated entities and preserving the consistence of the
simulation.
",4,6
26,"  We revisit variable renaming from a practitioner's point of view, presenting
concepts we found useful in dealing with operational semantics of pure Prolog.
A concept of relaxed core representation is introduced, upon which a concept of
prenaming is built. Prenaming formalizes the intuitive practice of renaming
terms by just considering the necessary bindings, where now some passive
""bindings"" x/x may be necessary as well. As an application, a constructive
version of variant lemma for implemented Horn clause logic has been obtained.
There, prenamings made it possible to incrementally handle new (local)
variables.
",1,5
27,"  This paper concerns the analysis of information leaks in security systems. We
address the problem of specifying and analyzing large systems in the (standard)
channel model used in quantitative information flow (QIF). We propose several
operators which match typical interactions between system components. We
explore their algebraic properties with respect to the security-preserving
refinement relation defined by Alvim et al. and McIver et al.
  We show how the algebra can be used to simplify large system specifications
in order to facilitate the computation of information leakage bounds. We
demonstrate our results on the specification and analysis of the Crowds
Protocol. Finally, we use the algebra to justify a new algorithm to compute
leakage bounds for this protocol.
",3,2
28,"  Mobile devices are often presented with multiple connectivity options usually
making a selection either randomly or based on load/wireless conditions
metrics, as is the case of current offloading schemes. In this paper we claim
that link-layer connectivity can be associated with information-availability
and in this respect connectivity decisions should be information-aware. This
constitutes a next step for the Information-Centric Networking paradigm,
realizing the concept of Information-Centric Connectivity (ICCON). We elaborate
on different types of information availability and connectivity decisions in
the context of ICCON, present specific use cases and discuss emerging
opportunities, challenges and technical approaches. We illustrate the potential
benefits of ICCON through preliminary simulation and numerical results in an
example use case.
",2,3
29,"  Generally, combinatorial design concerns with the arrangement of a finite set
of elements into patterns (subsets, words, arrays) according to specified
rules. The usefulness of this design method is that the number of input
combination can be reduced dramatically but all of them are covered by the
combinatorial set. This paper presents the application of this design method in
communication networks. Communication engineers can use this novel method to
generate test cases for producing a cost-effective set of experiments to
recognize the factors that have the least and most impact on the system's
performance. A well-known scenario is used for the purpose of the experiment
and five factors with different values are chosen to qualify their effect on
the network performance. The experimental set is generated using combinatorial
design method and then it is been used to analyze the effect of each factor.
The experiments showed the effectiveness of the method to be used for analyzing
the effect of factors on the communication network.
",3,0
30,"  The correctness of networks is often described in terms of the individual
data flow of components instead of their global behavior. In software-defined
networks, it is far more convenient to specify the correct behavior of packets
than the global behavior of the entire network. Petri nets with transits extend
Petri nets and Flow-LTL extends LTL such that the data flows of tokens can be
tracked. We present the tool AdamMC as the first model checker for Petri nets
with transits against Flow-LTL. We describe how AdamMC can automatically encode
concurrent updates of software-defined networks as Petri nets with transits and
how common network specifications can be expressed in Flow-LTL. Underlying
AdamMC is a reduction to a circuit model checking problem. We introduce a new
reduction method that results in tremendous performance improvements compared
to a previous prototype. Thereby, AdamMC can handle software-defined networks
with up to 82 switches.
",1,0
31,"  MIL-STD-1553 is a military standard that defines the physical and logical
layers, and a command/response time division multiplexing of a communication
bus used in military and aerospace avionic platforms for more than 40 years. As
a legacy platform, MIL-STD-1553 was designed for high level of fault tolerance
while less attention was taken with regard to security. Recent studies already
addressed the impact of successful cyber attacks on aerospace vehicles that are
implementing MIL-STD-1553. In this study we present a security analysis of
MIL-STD-1553. In addition, we present a method for anomaly detection in
MIL-STD-1553 communication bus and its performance in the presence of several
attack scenarios implemented in a testbed, as well as results on real system
data. Moreover, we propose a general approach towards an intrusion detection
system (IDS) for a MIL-STD-1553 communication bus.
",0,4
32,"  The Ukraine power grid cyberattacks remind us that the smart Internet of
Things (IoT) can help us control our light-bulbs, but if under attacks it might
also take us into darkness. Nowadays, many literatures have tried to address
the concerns on IoT security, but few of them take into consideration the sever
threats to IoT coming from the advances of quantum computing. As a promising
candidate for the future post-quantum cryptography standard, lattice-based
cryptography enjoys the advantages of strong security guarantees and high
efficiency, which make it extremely suitable for IoT applications. In this
paper, we summarize the advantages of lattice-based cryptography and the state
of art of their implementations for IoT devices.
",2,6
33,"  Cloud computing is a recent paradigm based around the notion of delivery of
resources via a service model over the Internet. Despite being a new paradigm
of computation, cloud computing owes its origins to a number of previous
paradigms. The term cloud computing is well defined and no longer merits
rigorous taxonomies to furnish a definition. Instead this survey paper
considers the past, present and future of cloud computing. As an evolution of
previous paradigms, we consider the predecessors to cloud computing and what
significance they still hold to cloud services. Additionally we examine the
technologies which comprise cloud computing and how the challenges and future
developments of these technologies will influence the field. Finally we examine
the challenges that limit the growth, application and development of cloud
computing and suggest directions required to overcome these challenges in order
to further the success of cloud computing.
",4,3
34,"  This paper revisits the classical notion of sampling in the setting of
real-time temporal logics for the modeling and analysis of systems. The
relationship between the satisfiability of Metric Temporal Logic (MTL) formulas
over continuous-time models and over discrete-time models is studied. It is
shown to what extent discrete-time sequences obtained by sampling
continuous-time signals capture the semantics of MTL formulas over the two time
domains. The main results apply to ""flat"" formulas that do not nest temporal
operators and can be applied to the problem of reducing the verification
problem for MTL over continuous-time models to the same problem over
discrete-time, resulting in an automated partial practically-efficient
discretization technique.
",3,5
35,"  FreeBSD was one of the first widely deployed free operating systems to
provide mandatory access control. It supports a number of classic MAC models.
This tutorial paper addresses exploiting this implementation to enforce typical
enterprise security policies of varying complexities.
",1,3
36,"  We give the quantum subset construction of orthomodular lattice-valued finite
automata, then we show the equivalence between orthomodular lattice-valued
finite automata, orthomodular lattice-valued deterministic finite automata and
orthomodular lattice-valued finite automata with empty string-moves. Based on
these equivalences, we study the algebraic operations on orthomodular
lattice-valued regular languages, then we establish Kleene theorem in the frame
of quantum logic.
",1,2
37,"  Vehicular communications play a substantial role in providing safety
transportation by means of safety message exchange. Researchers have proposed
several solutions for securing safety messages. Protocols based on a fixed key
infrastructure are more efficient in implementation and maintain stronger
security in comparison with dynamic structures. The purpose of this paper
present a method based on a fixed key infrastructure for detection
impersonation attack, in other words, Sybil attack, in the vehicular ad hoc
network. This attack, puts a great impact on performance of the network. The
proposed method, using an cryptography mechanism to detection Sybil attack.
Finally, using Mat lab simulator the results of this approach are reviewed,
This method it has low delay for detection Sybil attack, because most
operations are done in Certification Authority, so this proposed schema is a
efficient method for detection Sybil attack.
",2,1
38,"  Efficient mobility management techniques are critical in providing seamless
connectivity and session continuity between a mobile node and the network
during its movement. Current mobility management solutions generally require a
central entity in the network core, tracking IP address movement and anchoring
traffic from source to destination through point-to-point tunnels. Intuitively,
this approach suffers from scalability limitations as it creates bottlenecks in
the network, due to sub-optimal routing via the anchor point. Meanwhile,
alternative anchorless, solutions are not feasible due to the current
limitations of the IP semantics, which strongly ties addressing information to
location. In contrast, novel path-based forwarding solutions may be exploited
for feasible anchorless solutions. In this paper, we propose a novel
network-based mobility management solution that facilitates IP mobility over
such a path-based forwarding substrate. Our solution exploits the advantages of
such substrates in decoupling path calculation from data transfer to eliminate
the need for anchoring traffic through the network core; thereby, allowing
flexible path calculation and service provisioning. Furthermore, by eliminating
the limitation of routing via the anchor point, our approach reduces the
network cost compared to anchored solution through bandwidth saving while
maintaining comparable handover delay. We evaluate our solution through
analytical and simulation models and compare it with the IETF standardized
solution, Proxy Mobile IPv6 (PMIPv6). Evaluation results illustrate a
significant saving in the total network cost when using our proposed solution,
compared to its counterpart.
",4,1
39,"  Typically, spoken language understanding (SLU) models are trained on
annotated data which are costly to gather. Aiming to reduce data needs for
bootstrapping a SLU system for a new language, we present a simple but
effective weight transfer approach using data from another language. The
approach is evaluated with our promising multi-task SLU framework developed
towards different languages. We evaluate our approach on the ATIS and a
real-world SLU dataset, showing that i) our monolingual models outperform the
state-of-the-art, ii) we can reduce data amounts needed for bootstrapping a SLU
system for a new language greatly, and iii) while multitask training improves
over separate training, different weight transfer settings may work best for
different SLU modules.
",2,5
40,"  Recently the topic of how to effectively offload cellular traffic onto
device-to-device (D2D) sharing among users in proximity has been gaining more
and more attention of global researchers and engineers. Users utilize wireless
short-range D2D communications for sharing contents locally, due to not only
the rapid sharing experience and free cost, but also high accuracy on
deliveries of interesting and popular contents, as well as strong social
impacts among friends. Nevertheless, the existing related studies are mostly
confined to small-scale datasets, limited dimensions of user features, or
unrealistic assumptions and hypotheses on user behaviors. In this article,
driven by emerging Big Data techniques, we propose to design a big data
platform, named D2D Big Data, in order to encourage the wireless D2D
communications among users effectively, to promote contents for providers
accurately, and to carry out offloading intelligence for operators efficiently.
We deploy a big data platform and further utilize a large-scale dataset (3.56
TBytes) from a popular D2D sharing application (APP), which contains 866
million D2D sharing activities on 4.5 million files disseminated via nearly 850
million users in 13 weeks. By abstracting and analyzing multidimensional
features, including online behaviors, content properties, location relations,
structural characteristics, meeting dynamics, social arborescence, privacy
preservation policies and so on, we verify and evaluate the D2D Big Data
platform regarding predictive content propagating coverage. Finally, we discuss
challenges and opportunities regarding D2D Big Data and propose to unveil a
promising upcoming future of wireless D2D communications.
",4,6
41,"  In this paper we present SABRINA (Sentiment Analysis: a Broad Resource for
Italian Natural language Applications) a manually annotated prior polarity
lexical resource for Italian natural language applications in the field of
opinion mining and sentiment induction. The resource consists in two different
sets, an Italian dictionary of more than 277.000 words tagged with their prior
polarity value, and a set of polarity modifiers, containing more than 200
words, which can be used in combination with non neutral terms of the
dictionary in order to induce the sentiment of Italian compound terms. To the
best of our knowledge this is the first prior polarity manually annotated
resource which has been developed for the Italian natural language.
",1,5
42,"  In the online matching on the line problem, the task is to match a set of
requests $R$ online to a given set of servers $S$. The distance metric between
any two points in $R\,\cup\, S$ is a line metric and the objective for the
online algorithm is to minimize the sum of distances between matched
server-request pairs. This problem is well-studied and - despite recent
improvements - there is still a large gap between the best known lower and
upper bounds: The best known deterministic algorithm for the problem is
$O(\log^2n)$-competitive, while the best known deterministic lower bound is
$9.001$. The lower and upper bounds for randomized algorithms are $4.5$ and
$O(\log n)$ respectively.
  We prove that any deterministic online algorithm which in each round: $(i)$
bases the matching decision only on information local to the current request,
and $(ii)$ is symmetric (in the sense that the decision corresponding to the
mirror image of some instance $I$ is the mirror image of the decision
corresponding to instance $I$), must be $\Omega(\log n)$-competitive. We then
extend the result by showing that it also holds when relaxing the symmetry
property so that the algorithm might prefer one side over the other, but only
up to some degree. This proves a barrier of $\Omega(\log n)$ on the competitive
ratio for a large class of ""natural"" algorithms. This class includes all
deterministic online algorithms found in the literature so far.
  Furthermore, we show that our result can be extended to randomized algorithms
that locally induce a symmetric distribution over the chosen servers. The
$\Omega(\log n)$-barrier on the competitive ratio holds for this class of
algorithms as well.
",3,4
43,"  The restless bandit problem is one of the most well-studied generalizations
of the celebrated stochastic multi-armed bandit problem in decision theory. In
its ultimate generality, the restless bandit problem is known to be PSPACE-Hard
to approximate to any non-trivial factor, and little progress has been made
despite its importance in modeling activity allocation under uncertainty.
  We consider a special case that we call Feedback MAB, where the reward
obtained by playing each of n independent arms varies according to an
underlying on/off Markov process whose exact state is only revealed when the
arm is played. The goal is to design a policy for playing the arms in order to
maximize the infinite horizon time average expected reward. This problem is
also an instance of a Partially Observable Markov Decision Process (POMDP), and
is widely studied in wireless scheduling and unmanned aerial vehicle (UAV)
routing. Unlike the stochastic MAB problem, the Feedback MAB problem does not
admit to greedy index-based optimal policies.
  We develop a novel and general duality-based algorithmic technique that
yields a surprisingly simple and intuitive 2+epsilon-approximate greedy policy
to this problem. We then define a general sub-class of restless bandit problems
that we term Monotone bandits, for which our policy is a 2-approximation. Our
technique is robust enough to handle generalizations of these problems to
incorporate various side-constraints such as blocking plays and switching
costs. This technique is also of independent interest for other restless bandit
problems. By presenting the first (and efficient) O(1) approximations for
non-trivial instances of restless bandits as well as of POMDPs, our work
initiates the study of approximation algorithms in both these contexts.
",2,1
44,"  This work is concerned with approximating constraint satisfaction problems
(CSPs) with an additional global cardinality constraints. For example, \maxcut
is a boolean CSP where the input is a graph $G = (V,E)$ and the goal is to find
a cut $S \cup \bar S = V$ that maximizes the numberof crossing edges,
$|E(S,\bar S)|$. The \maxbisection problem is a variant of \maxcut with an
additional global constraint that each side of the cut has exactly half the
vertices, i.e., $|S| = |V|/2$. Several other natural optimization problems like
\minbisection and approximating Graph Expansion can be formulated as CSPs with
global constraints.
  In this work, we formulate a general approach towards approximating CSPs with
global constraints using SDP hierarchies. To demonstrate the approach we
present the following results:
  Using the Lasserre hierarchy, we present an algorithm that runs in time
$O(n^{poly(1/\epsilon)})$ that given an instance of \maxbisection with value
$1-\epsilon$, finds a bisection with value $1-O(\sqrt{\epsilon})$. This
approximation is near-optimal (up to constant factors in $O()$) under the
Unique Games Conjecture.
  By a computer-assisted proof, we show that the same algorithm also achieves a
0.85-approximation for \maxbisection, improving on the previous bound of 0.70
(note that it is \uniquegames hard to approximate better than a 0.878 factor).
The same algorithm also yields a 0.92-approximation for \maxtwosat with
cardinality constraints.
  For every CSP with a global cardinality constraints, we present a generic
conversion from integrality gap instances for the Lasserre hierarchy to a {\it
dictatorship test} whose soundness is at most integrality gap. Dictatorship
testing gadgets are central to hardness results for CSPs, and a generic
conversion of the above nature lies at the core of the tight Unique Games based
hardness result for CSPs. \cite{Raghavendra08}
",1,4
45,"  This chapter is going to deal with enhancing the efficiency of Biometric by
integrating it with Salt Value (randomly generated value of varying length).
Normally at an enterprise level or data centres, the servers are maintained
with complex passwords and they are known only to the system administrators.
Even after applying lot of securities at an expert level, the hackers are able
to penetrate through the network and break the passwords easily. Here how the
biometric can play a vital role and that too with the inclusion of Salt value
can prevent the hacker from stealing the confidential data's of an
organization.
",6,2
46,"  Open communication over the Internet poses a serious threat to countries with
repressive regimes, leading them to develop and deploy censorship mechanisms
within their networks. Unfortunately, existing censorship circumvention systems
do not provide high availability guarantees to their users, as censors can
identify, hence disrupt, the traffic belonging to these systems using today's
advanced censorship technologies. In this paper we propose SWEET, a highly
available censorship-resistant infrastructure. SWEET works by encapsulating a
censored user's traffic to a proxy server inside email messages that are
carried over by public email service providers, like Gmail and Yahoo Mail. As
the operation of SWEET is not bound to specific email providers we argue that a
censor will need to block all email communications in order to disrupt SWEET,
which is infeasible as email constitutes an important part of today's Internet.
Through experiments with a prototype of our system we find that SWEET's
performance is sufficient for web traffic. In particular, regular websites are
downloaded within couple of seconds.
",4,6
47,"  A major challenge in paraphrase research is the lack of parallel corpora. In
this paper, we present a new method to collect large-scale sentential
paraphrases from Twitter by linking tweets through shared URLs. The main
advantage of our method is its simplicity, as it gets rid of the classifier or
human in the loop needed to select data before annotation and subsequent
application of paraphrase identification algorithms in the previous work. We
present the largest human-labeled paraphrase corpus to date of 51,524 sentence
pairs and the first cross-domain benchmarking for automatic paraphrase
identification. In addition, we show that more than 30,000 new sentential
paraphrases can be easily and continuously captured every month at ~70%
precision, and demonstrate their utility for downstream NLP tasks through
phrasal paraphrase extraction. We make our code and data freely available.
",1,6
48,"  Establishing trust between developers working at distant sites facilitates
team collaboration in distributed software development. While previous research
has focused on how to build and spread trust in absence of direct, face-to-face
communication, it has overlooked the effects of the propensity to trust, i.e.,
the trait of personality representing the individual disposition to perceive
the others as trustworthy. In this study, we present a preliminary,
quantitative analysis on how the propensity to trust affects the success of
collaborations in a distributed project, where the success is represented by
pull requests whose code changes and contributions are successfully merged into
the project's repository.
",0,4
49,"  Consider an $m$-bit query $q$ to a bitwise trie $T$. A wildcard $*$ is an
unspecified bit in $q$ for which the query asks the membership for both cases
$*=0$ and $*=1$. It is common that such partial-match queries with wildcards
are issued in tries. With uniformly random occurrences of $w$ wildcards in $q$
assumed, the obvious upper bound on the average number of traversal steps in
$T$ is $2^w m$. We show that the average does not exceed \[ \frac{m+1}{w+1}
\left( 2^{w+2} - 2 w - 4 \right) + m = O \left( \frac{2^w m}{w} \right), \] and
equals the value exactly when $T$ includes all the $m$-bit keys as the worst
case. Here the query $q$ performs with the naive backtracking algorithm in $T$.
It is similarly shown that the average is $O \left( \frac{k^w m}{w} \right)$ in
a general trie of maximum out-degree $k$. Our analysis for tries is extended to
a distributed hash table (DHT), which is among the most frequently used
decentralized data structures in networking. We show, under a natural
probabilistic assumption for the largest class of DHTs, that the average number
of hops required by an $m$-bit query $q$ to a DHT $D$ with random $w$ wildcards
meets the same asymptotic bound. As a result, $q$ is answered with average $O
\left( \frac{2^w m}{w} \right)$ hops rather than $\Theta \left( 2^w m \right)$
in the four major DHTs Chord, Pastry, Tapestry and Kademlia. In addition, with
a uniform key distribution for sufficiently many entries, we prove that a
lookup request to the DHT Chord is answered correctly with $O(m)$ hops and
probability $1 - 2^{-\Omega (m)}$. To the author's knowledge, the probability
$1 - 2^{-\Omega (m)}$ of correct lookup in Chord has not been identified so
far.
",6,5
50,"  Explaining the excellent practical performance of the simplex method for
linear programming has been a major topic of research for over 50 years. One of
the most successful frameworks for understanding the simplex method was given
by Spielman and Teng (JACM `04), who developed the notion of smoothed analysis.
Starting from an arbitrary linear program with $d$ variables and $n$
constraints, Spielman and Teng analyzed the expected runtime over random
perturbations of the LP (smoothed LP), where variance $\sigma^2$ Gaussian noise
is added to the LP data. In particular, they gave a two-stage shadow vertex
simplex algorithm which uses an expected $\widetilde{O}(d^{55} n^{86}
\sigma^{-30})$ number of simplex pivots to solve the smoothed LP. Their
analysis and runtime was substantially improved by Deshpande and Spielman (FOCS
`05) and later Vershynin (SICOMP `09). The fastest current algorithm, due to
Vershynin, solves the smoothed LP using an expected $O(d^3 \sigma^{-4} \log^3 n
+ d^9\log^7 n)$ number of pivots, improving the dependence on $n$ from
polynomial to poly-logarithmic.
  While the original proof of Spielman and Teng has now been substantially
simplified, the resulting analyses are still quite long and complex and the
parameter dependencies far from optimal. In this work, we make substantial
progress on this front, providing an improved and simpler analysis of shadow
simplex methods, where our algorithm requires an expected \[ O(d^2 \sqrt{\log
n} \sigma^{-2} + d^3 \log^{3/2} n) \] number of simplex pivots. We obtain our
results via an improved \emph{shadow bound}, key to earlier analyses as well,
combined with improvements on algorithmic techniques of Vershynin. As an added
bonus, our analysis is completely modular, allowing us to obtain non-trivial
bounds for perturbations beyond Gaussians, such as Laplace perturbations.
",2,3
51,"  Code voting was introduced by Chaum as a solution for using a possibly
infected-by-malware device to cast a vote in an electronic voting application.
Chaum's work on code voting assumed voting codes are physically delivered to
voters using the mail system, implicitly requiring to trust the mail system.
This is not necessarily a valid assumption to make - especially if the mail
system cannot be trusted. When conspiring with the recipient of the cast
ballots, privacy is broken.
  It is clear to the public that when it comes to privacy, computers and
""secure"" communication over the Internet cannot fully be trusted. This
emphasizes the importance of using: (1) Unconditional security for secure
network communication. (2) Reduce reliance on untrusted computers.
  In this paper we explore how to remove the mail system trust assumption in
code voting. We use PSMT protocols (SCN 2012) where with the help of visual
aids, humans can carry out $\mod 10$ addition correctly with a 99\% degree of
accuracy. We introduce an unconditionally secure MIX based on the combinatorics
of set systems.
  Given that end users of our proposed voting scheme construction are humans we
\emph{cannot use} classical Secure Multi Party Computation protocols.
  Our solutions are for both single and multi-seat elections achieving:
\begin{enumerate}[i)]
  \item An anonymous and perfectly secure communication network secure against
a $t$-bounded passive adversary used to deliver voting,
  \item The end step of the protocol can be handled by a human to evade the
threat of malware. \end{enumerate} We do not focus on active adversaries.
",2,1
52,"  We develop a novel method for measuring the similarity between complete
weighted graphs, which are probed by means of discrete-time quantum walks.
Directly probing complete graphs using discrete-time quantum walks is
intractable due to the cost of simulating the quantum walk. We overcome this
problem by extracting a commute-time minimum spanning tree from the complete
weighted graph. The spanning tree is probed by a discrete time quantum walk
which is initialised using a weighted version of the Perron-Frobenius operator.
This naturally encapsulates the edge weight information for the spanning tree
extracted from the original graph. For each pair of complete weighted graphs to
be compared, we simulate a discrete-time quantum walk on each of the
corresponding commute time minimum spanning trees, and then compute the
associated density matrices for the quantum walks. The probability of the walk
visiting each edge of the spanning tree is given by the diagonal elements of
the density matrices. The similarity between each pair of graphs is then
computed using either a) the inner product or b) the negative exponential of
the Jensen-Shannon divergence between the probability distributions. We show
that in both cases the resulting similarity measure is positive definite and
therefore corresponds to a kernel on the graphs. We perform a series of
experiments on publicly available graph datasets from a variety of different
domains, together with time-varying financial networks extracted from data for
the New York Stock Exchange. Our experiments demonstrate the effectiveness of
the proposed similarity measures.
",1,4
53,"  This document is one of the deliverable reports created for the ESCAPE
project. ESCAPE stands for Energy-efficient Scalable Algorithms for Weather
Prediction at Exascale. The project develops world-class, extreme-scale
computing capabilities for European operational numerical weather prediction
and future climate models. This is done by identifying Weather & Climate dwarfs
which are key patterns in terms of computation and communication (in the spirit
of the Berkeley dwarfs). These dwarfs are then optimised for different hardware
architectures (single and multi-node) and alternative algorithms are explored.
Performance portability is addressed through the use of domain specific
languages.
  In this deliverable we report on energy consumption measurements of a number
of NWP models/dwarfs on the Intel E5-2697v4 processor. The chosen energy
metrics and energy measurement methods are documented. Energy measurements are
performed on the Bi-Fourier dwarf (BiFFT), the Acraneb dwarf, the ALARO 2.5 km
Local Area Model reference configuration (B\'enard et al. 2010, Bubnova et al.
1995) and on the COSMO-EULAG Local Area Model reference configuration
(Piotrowski et al. 2018). The results show a U-shaped dependence of the
consumed energy on the wall-clock time performance. This shape can be explained
from the dependence of the average power of the compute nodes on the total
number of cores used.
  We compare the energy consumption of the BiFFT dwarf on the E5-2697v4
processor to that on the Optalysys optical processors. The latter are found to
be much less energy costly, but at the same time it is also the only metric
where they outperform the classical CPU. They are non-competitive as far as
wall-clock time and especially numerical precision are concerned.
",2,1
54,"  Detecting misbehavior (such as transmissions of false information) in
vehicular ad hoc networks (VANETs) is very important problem with wide range of
implications including safety related and congestion avoidance applications. We
discuss several limitations of existing misbehavior detection schemes (MDS)
designed for VANETs. Most MDS are concerned with detection of malicious nodes.
In most situations, vehicles would send wrong information because of selfish
reasons of their owners, e.g. for gaining access to a particular lane. Because
of this (\emph{rational behavior}), it is more important to detect false
information than to identify misbehaving nodes. We introduce the concept of
data-centric misbehavior detection and propose algorithms which detect false
alert messages and misbehaving nodes by observing their actions after sending
out the alert messages. With the data-centric MDS, each node can independently
decide whether an information received is correct or false. The decision is
based on the consistency of recent messages and new alert with reported and
estimated vehicle positions. No voting or majority decisions is needed, making
our MDS resilient to Sybil attacks. Instead of revoking all the secret
credentials of misbehaving nodes, as done in most schemes, we impose fines on
misbehaving nodes (administered by the certification authority), discouraging
them to act selfishly. This reduces the computation and communication costs
involved in revoking all the secret credentials of misbehaving nodes.
",2,1
55,"  We give a randomized algorithm that finds a minimum cut in an undirected
weighted $m$-edge $n$-vertex graph $G$ with high probability in $O(m \log^2 n)$
time. This is the first improvement to Karger's celebrated $O(m \log^3 n)$ time
algorithm from 1996. Our main technical contribution is a deterministic $O(m
\log n)$ time algorithm that, given a spanning tree $T$ of $G$, finds a minimum
cut of $G$ that 2-respects (cuts two edges of) $T$.
",1,6
56,"  We propose a hash function based on arithmetic coding and public-key
cryptography. The resistance of the hash function to second preimage attack,
collision and differential cryptanalysis is based on the properties of
arithmetic coding as a non-linear dynamical system. The resistance of the hash
function to first preimage attack is based on the public-key cryptography. The
new hash function uses the strength of HMAC with the difference that it didn't
need a secret key for calculating the hash (in this step, it uses one, two or
three public -keys) and in the classical attack, an adversary need to break the
public key algorithm or to have all the secret keys to perform his attack.
",2,3
57,"  This paper explores the problem of ranking short social media posts with
respect to user queries using neural networks. Instead of starting with a
complex architecture, we proceed from the bottom up and examine the
effectiveness of a simple, word-level Siamese architecture augmented with
attention-based mechanisms for capturing semantic ""soft"" matches between query
and post tokens. Extensive experiments on datasets from the TREC Microblog
Tracks show that our simple models not only achieve better effectiveness than
existing approaches that are far more complex or exploit a more diverse set of
relevance signals, but are also much faster. Implementations of our samCNN
(Simple Attention-based Matching CNN) models are shared with the community to
support future work.
",4,6
58,"  Standard approaches in entity identification hard-code boundary detection and
type prediction into labels (e.g., John/B-PER Smith/I-PER) and then perform
Viterbi. This has two disadvantages: 1. the runtime complexity grows
quadratically in the number of types, and 2. there is no natural segment-level
representation. In this paper, we propose a novel neural architecture that
addresses these disadvantages. We frame the problem as multitasking, separating
boundary detection and type prediction but optimizing them jointly. Despite its
simplicity, this architecture performs competitively with fully structured
models such as BiLSTM-CRFs while scaling linearly in the number of types.
Furthermore, by construction, the model induces type-disambiguating embeddings
of predicted mentions.
",2,3
59,"  Vehicle-to-infrastructure (V2I) communication may provide high data rates to
vehicles via millimeter-wave (mmWave) microcellular networks. This paper uses
stochastic geometry to analyze the coverage of urban mmWave microcellular
networks. Prior work used a pathloss model with a line-of-sight probability
function based on randomly oriented buildings, to determine whether a link was
line-of-sight or non-line-of-sight. In this paper, we use a pathloss model
inspired by measurements, which uses a Manhattan distance pathloss model and
accounts for differences in pathloss exponents and losses when turning corners.
In our model, streets are randomly located as a Manhattan Poisson line process
(MPLP) and the base stations (BSs) are distributed according to a Poisson point
process. Our model is well suited for urban microcellular networks where the
BSs are deployed at street level. Based on this new approach, we derive the
coverage probability under certain BS association rules to obtain closed-form
solutions without much complexity. In addition, we draw two main conclusions
from our work. First, non-line-of-sight BSs are not a major benefit for
association or source of interference most of the time. Second, there is an
ultra-dense regime where deploying active BSs does not enhance coverage.
",1,6
60,"  We show that for every fixed undirected graph $H$, there is a $O(|V(G)|^3)$
time algorithm that tests, given a graph $G$, if $G$ contains $H$ as a
topological subgraph (that is, a subdivision of $H$ is subgraph of $G$). This
shows that topological subgraph testing is fixed-parameter tractable, resolving
a longstanding open question of Downey and Fellows from 1992. As a corollary,
for every $H$ we obtain an $O(|V(G)|^3)$ time algorithm that tests if there is
an immersion of $H$ into a given graph $G$. This answers another open question
raised by Downey and Fellows in 1992.
",6,5
61,"  Edit distance is a fundamental measure of distance between strings and has
been widely studied in computer science. While the problem of estimating edit
distance has been studied extensively, the equally important question of
actually producing an alignment (i.e., the sequence of edits) has received far
less attention. Somewhat surprisingly, we show that any algorithm to estimate
edit distance can be used in a black-box fashion to produce an approximate
alignment of strings, with modest loss in approximation factor and small loss
in run time. Plugging in the result of Andoni, Krauthgamer, and Onak, we obtain
an alignment that is a $(\log n)^{O(1/\varepsilon^2)}$ approximation in time
$\tilde{O}(n^{1 + \varepsilon})$.
  Closely related to the study of approximation algorithms is the study of
metric embeddings for edit distance. We show that min-hash techniques can be
useful in designing edit distance embeddings through three results: (1) An
embedding from Ulam distance (edit distance over permutations) to Hamming space
that matches the best known distortion of $O(\log n)$ and also implicitly
encodes a sequence of edits between the strings; (2) In the case where the edit
distance between the input strings is known to have an upper bound $K$, we show
that embeddings of edit distance into Hamming space with distortion $f(n)$ can
be modified in a black-box fashion to give distortion
$O(f(\operatorname{poly}(K)))$ for a class of periodic-free strings; (3) A
randomized dimension-reduction map with contraction $c$ and asymptotically
optimal expected distortion $O(c)$, improving on the previous $\tilde{O}(c^{1 +
2 / \log \log \log n})$ distortion result of Batu, Ergun, and Sahinalp.
",6,2
62,"  The popularity of distance education programs is increasing at a fast pace.
En par with this development, online communication in fora, social media and
reviewing platforms between students is increasing as well. Exploiting this
information to support fellow students or institutions requires to extract the
relevant opinions in order to automatically generate reports providing an
overview of pros and cons of different distance education programs. We report
on an experiment involving distance education experts with the goal to develop
a dataset of reviews annotated with relevant categories and aspects in each
category discussed in the specific review together with an indication of the
sentiment.
  Based on this experiment, we present an approach to extract general
categories and specific aspects under discussion in a review together with
their sentiment. We frame this task as a multi-label hierarchical text
classification problem and empirically investigate the performance of different
classification architectures to couple the prediction of a category with the
prediction of particular aspects in this category. We evaluate different
architectures and show that a hierarchical approach leads to superior results
in comparison to a flat model which makes decisions independently.
",1,0
63,"  This extended abstract reports on current progress of SMTCoq, a communication
tool between the Coq proof assistant and external SAT and SMT solvers. Based on
a checker for generic first-order certificates implemented and proved correct
in Coq, SMTCoq offers facilities both to check external SAT and SMT answers and
to improve Coq's automation using such solvers, in a safe way. Currently
supporting the SAT solver zChaff, and the SMT solver veriT for the combination
of the theories of congruence closure and linear integer arithmetic, SMTCoq is
meant to be extendable with a reasonable amount of effort: we present work in
progress to support the SMT solver CVC4 and the theory of bit vectors.
",2,6
64,"  VoIP (Voice over Internet Protocol) is a growing technology during last
decade. It provides the audio, video streaming facility on successful
implementation in the network. However, it provides the text transport facility
over the network. Due to implementation of it the cost effective solution, it
can be developed for the intercommunication among the employees of a
prestigious organization. The proposed idea has been implemented on the audio
streaming area of the VoIP technology. In the audio streaming, the security
vulnerabilities are possible on the VoIP server during communication between
two parties. In the proposed model, first the VoIP system has been implemented
with IVR (Interactive Voice Response) as a case study and with the
implementation of the security parameters provided to the asterisk server which
works as a VoIP service provider. The asterisk server has been configured with
different security parameters like VPN server, Firewall iptable rules,
Intrusion Detection and Intrusion Prevention System. Every parameter will be
monitored by the system administrator of the VoIP server along with the MySQL
database. The system admin will get every update related to the attacks on the
server through Mail server attached to the asterisk server. The main beauty of
the proposed system is VoIP server alone is configured as a VoIP server, IVR
provider, Mail Server with IDS and IPS, VPN server, connection with database
server in a single asterisk server inside virtualization environment. The VoIP
system is implemented for a Local Area Network inside the university system
",2,1
65,"  In this paper we have proposed clock error mitigation from the measurements
in the scheduled based self localization system. We propose measurement model
with clock errors while following a scheduled transmission among anchor nodes.
Further, RLS algorithm is proposed to estimate clock error and to calibrate
measurements of self localizing node against relative clock errors of anchor
nodes. A full-scale experimental validation is provided based on commercial
off-the-shelf UWB radios under IEEE-standardized protocols.
",6,0
66,"  Mental health research can benefit increasingly fruitfully from computational
linguistics methods, given the abundant availability of language data in the
internet and advances of computational tools. This interdisciplinary project
will collect and analyse social media data of individuals diagnosed with
bipolar disorder with regard to their recovery experiences. Personal recovery -
living a satisfying and contributing life along symptoms of severe mental
health issues - so far has only been investigated qualitatively with structured
interviews and quantitatively with standardised questionnaires with mainly
English-speaking participants in Western countries. Complementary to this
evidence, computational linguistic methods allow us to analyse first-person
accounts shared online in large quantities, representing unstructured settings
and a more heterogeneous, multilingual population, to draw a more complete
picture of the aspects and mechanisms of personal recovery in bipolar disorder.
",1,5
67,"  One of the key problems in migrating multi-component enterprise applications
to Clouds is selecting the best mix of VM images and Cloud infrastructure
services. A migration process has to ensure that Quality of Service (QoS)
requirements are met, while satisfying conflicting selection criteria, e.g.
throughput and cost. When selecting Cloud services, application engineers must
consider heterogeneous sets of criteria and complex dependencies across
multiple layers impossible to resolve manually. To overcome this challenge, we
present the generic recommender framework CloudGenius and an implementation
that leverage well known multi-criteria decision making technique Analytic
Hierarchy Process to automate the selection process based on a model, factors,
and QoS requirements related to enterprise applications. In particular, we
introduce a structured migration process for multi-component enterprise
applications, clearly identify the most important criteria relevant to the
selection problem and present a multi-criteria-based selection algorithm.
Experiments with the software prototype CumulusGenius show time complexities.
",4,6
68,"  This paper presents a self-organizing protocol for dynamic (unstructured P2P)
overlay networks, which allows to react to the variability of node arrivals and
departures. Through local interactions, the protocol avoids that the departure
of nodes causes a partitioning of the overlay. We show that it is sufficient to
have knowledge about 1st and 2nd neighbours, plus a simple interaction P2P
protocol, to make unstructured networks resilient to node faults. A simulation
assessment over different kinds of overlay networks demonstrates the viability
of the proposal.
",1,5
69,"  It is customary to assess the reliability of underground oil and gas
pipelines in the presence of excessive loading and corrosion effects to ensure
a leak-free transport of hazardous materials. The main idea behind this
reliability analysis is to model the given pipeline system as a Reliability
Block Diagram (RBD) of segments such that the reliability of an individual
pipeline segment can be represented by a random variable. Traditionally,
computer simulation is used to perform this reliability analysis but it
provides approximate results and requires an enormous amount of CPU time for
attaining reasonable estimates. Due to its approximate nature, simulation is
not very suitable for analyzing safety-critical systems like oil and gas
pipelines, where even minor analysis flaws may result in catastrophic
consequences. As an accurate alternative, we propose to use a
higher-order-logic theorem prover (HOL) for the reliability analysis of
pipelines. As a first step towards this idea, this paper provides a
higher-order-logic formalization of reliability and the series RBD using the
HOL theorem prover. For illustration, we present the formal analysis of a
simple pipeline that can be modeled as a series RBD of segments with
exponentially distributed failure times.
",1,2
70,"  We consider the problem of choosing Euclidean points to maximize the sum of
their weighted pairwise distances, when each point is constrained to a ball
centered at the origin. We derive a dual minimization problem and show strong
duality holds (i.e., the resulting upper bound is tight) when some locally
optimal configuration of points is affinely independent. We sketch a polynomial
time algorithm for finding a near-optimal set of points.
",2,5
71,"  Text-based password schemes have inherent security and usability problems,
leading to the development of graphical password schemes. However, most of
these alternate schemes are vulnerable to spyware attacks. We propose a new
scheme, using CAPTCHA (Completely Automated Public Turing tests to tell
Computers and Humans Apart) that retaining the advantages of graphical password
schemes, while simultaneously raising the cost of adversaries by orders of
magnitude. Furthermore, some primary experiments are conducted and the results
indicate that the usability should be improved in the future work.
",2,3
72,"  The recently created IETF 6TiSCH working group combines the high reliability
and low-energy consumption of IEEE 802.15.4e Time Slotted Channel Hopping with
IPv6 for industrial Internet of Things. We propose a distributed link
scheduling algorithm, called Local Voting, for 6TiSCH networks that adapts the
schedule to the network conditions. The algorithm tries to equalize the link
load (defined as the ratio of the queue length over the number of allocated
cells) through cell reallocation. Local Voting calculates the number of cells
to be added or released by the 6TiSCH Operation Sublayer (6top). Compared to a
representative algorithm from the literature, Local Voting provides
simultaneously high reliability and low end-to-end latency while consuming
significantly less energy. Its performance has been examined and compared to
On-the-fly algorithm in 6TiSCH simulator by modeling an industrial environment
with 50 sensors.
",2,3
73,"  The Network Simulator (NS-2) is a most widely used network simulator. It has
the capabilities to simulate a range of networks including wired and wireless
networks. In this tutorial, we present the implementation of Ad Hoc On-Demand
Distance Vector (AODV) Protocol in NS-2. This tutorial is targeted to the
novice user who wants to understand the implementation of AODV Protocol in
NS-2.
",3,2
74,"  We consider a multiple depot, multiple vehicle routing problem with fuel
constraints. We are given a set of targets, a set of depots and a set of
homogeneous vehicles, one for each depot. The depots are also allowed to act as
refueling stations. The vehicles are allowed to refuel at any depot, and our
objective is to determine a route for each vehicle with a minimum total cost
such that each target is visited at least once by some vehicle, and the
vehicles never run out fuel as it traverses its route. We refer this problem as
Multiple Depot, Fuel-Constrained, Multiple Vehicle Routing Problem (FCMVRP).
This paper presents four new mixed integer linear programming formulations to
compute an optimal solution for the problem. Extensive computational results
for a large set of instances are also presented.
",1,2
75,"  The process of knowledge acquisition can be viewed as a question-answer game
between a student and a teacher in which the student typically starts by asking
broad, open-ended questions before drilling down into specifics (Hintikka,
1981; Hakkarainen and Sintonen, 2002). This pedagogical perspective motivates a
new way of representing documents. In this paper, we present SQUASH
(Specificity-controlled Question-Answer Hierarchies), a novel and challenging
text generation task that converts an input document into a hierarchy of
question-answer pairs. Users can click on high-level questions (e.g., ""Why did
Frodo leave the Fellowship?"") to reveal related but more specific questions
(e.g., ""Who did Frodo leave with?""). Using a question taxonomy loosely based on
Lehnert (1978), we classify questions in existing reading comprehension
datasets as either ""general"" or ""specific"". We then use these labels as input
to a pipelined system centered around a conditional neural language model. We
extensively evaluate the quality of the generated QA hierarchies through
crowdsourced experiments and report strong empirical results.
",1,6
76,"  We first explain the notion of secret sharing and also threshold schemes,
which can be implemented with the Shamir's secret sharing. Subsequently, we
review social secret sharing (NSG'10,NS'10) and its trust function. In a secret
sharing scheme, a secret is shared among a group of players who can later
recover the secret. We review the construction of a social secret sharing
scheme and its application for resource management in cloud, as explained in
NS'12. To clarify the social secret sharing scheme, we first review its trust
function according to NL'06. In this scheme, a secret is maintained by
assigning a trust value to each player based on his behavior, i.e.,
availability.
",5,2
77,"  We investigate the use of hierarchical phrase-based SMT lattices in
end-to-end neural machine translation (NMT). Weight pushing transforms the
Hiero scores for complete translation hypotheses, with the full translation
grammar score and full n-gram language model score, into posteriors compatible
with NMT predictive probabilities. With a slightly modified NMT beam-search
decoder we find gains over both Hiero and NMT decoding alone, with practical
advantages in extending NMT to very large input and output vocabularies.
",5,2
78,"  Model checking is a widespread automatic formal analysis that has been
successful in discovering flaws in security protocols. However existing
possibilities for state space explosion still hinder analyses of complex
protocols and protocol configurations. Message Inspection, is a technique that
delimits the branching of the state space due to the intruder model without
excluding possible attacks. In a preliminary simulation, the intruder model
tags the eavesdropped messages with specific metadata that enable validation of
feasibility of possible attack actions. The Message Inspection algorithm then
decides based on these metadata, which attacks will certainly fail according to
known security principles. Thus, it is a priori known that i.e. an encryption
scheme attack cannot succeed if the intruder does not posses the right key in
his knowledge. The simulation terminates with a report of the attack actions
that can be safely removed, resulting in a model with a reduced state space.
",5,1
79,"  We consider the language of $\Delta_0$-formulas with list terms interpreted
over hereditarily finite list superstructures. We study the complexity of
reasoning in extensions of the language of $\Delta_0$-formulas with
non-standard list terms, which represent bounded list search, bounded
iteration, and bounded recursion. We prove a number of results on the
complexity of model checking and satisfiability for these formulas. In
particular, we show that the set of $\Delta_0$-formulas with bounded recursive
terms true in a given list superstructure $HW(\mathcal{M})$ is non-elementary
(it contains the class kEXPTIME, for all $k\geqslant 1$). For
$\Delta_0$-formulas with restrictions on the usage of iterative and recursive
terms, we show lower complexity.
",1,6
80,"  Maximizing satisfaction from offering features as part of the upcoming
release(s) is different from minimizing dissatisfaction gained from not
offering features. This asymmetric behavior has never been utilized for product
release planning. We study Asymmetric Release Planning (ARP) by accommodating
asymmetric feature evaluation. We formulated and solved ARP as a bi-criteria
optimization problem. In its essence, it is the search for optimized trade-offs
between maximum stakeholder satisfaction and minimum dissatisfaction. Different
techniques including a continuous variant of Kano analysis are available to
predict the impact on satisfaction and dissatisfaction with a product release
from offering or not offering a feature. As a proof of concept, we validated
the proposed solution approach called Satisfaction-Dissatisfaction Optimizer
(SDO) via a real-world case study project. From running three replications with
varying effort capacities, we demonstrate that SDO generates optimized
trade-off solutions being (i) of a different value profile and different
structure, (ii) superior to the application of random search and heuristics in
terms of quality and completeness, and (iii) superior to the usage of manually
generated solutions generated from managers of the case study company. A survey
with 20 stakeholders evaluated the applicability and usefulness of the
generated results.
",1,5
81,"  Serialized messages are processed at the server and sent as objects over the
network to the client to be consumed.
",2,3
82,"  The enormous success of advanced wireless devices is pushing the demand for
higher wireless data rates. Denser spectrum reuse through the deployment of
more access points per square mile has the potential to successfully meet the
increasing demand for more bandwidth. In theory, the best approach to density
increase is via distributed multiuser MIMO, where several access points are
connected to a central server and operate as a large distributed multi-antenna
access point, ensuring that all transmitted signal power serves the purpose of
data transmission, rather than creating ""interference."" In practice, while
enterprise networks offer a natural setup in which distributed MIMO might be
possible, there are serious implementation difficulties, the primary one being
the need to eliminate phase and timing offsets between the jointly coordinated
access points.
  In this paper we propose AirSync, a novel scheme which provides not only time
but also phase synchronization, thus enabling distributed MIMO with full
spatial multiplexing gains. AirSync locks the phase of all access points using
a common reference broadcasted over the air in conjunction with a Kalman filter
which closely tracks the phase drift. We have implemented AirSync as a digital
circuit in the FPGA of the WARP radio platform. Our experimental testbed,
comprised of two access points and two clients, shows that AirSync is able to
achieve phase synchronization within a few degrees, and allows the system to
nearly achieve the theoretical optimal multiplexing gain. We also discuss MAC
and higher layer aspects of a practical deployment. To the best of our
knowledge, AirSync offers the first ever realization of the full multiuser MIMO
gain, namely the ability to increase the number of wireless clients linearly
with the number of jointly coordinated access points, without reducing the per
client rate.
",6,2
83,"  This paper aims to discuss the pilot study and analysis of the current
development and measurement practices in Jordanian small software firms. It is
conducted because most developers build web applications without using any
specific development method and don't know how to integrate the suitable
measurements inside the process to improve and reduce defect, time and rework
of the development life cycle. Furthermore the objectives of this pilot study
are firstly; determine the real characteristics of small software firms in
Jordan. Secondly, investigate the current development and measurement
practices. Thirdly, examine the need of new development methodology for
building web application in small software firms. Consequently, Pilot survey
was conducted in Jordanian small software firms. Descriptive statistics
analysis was used to rank the development and measurements methods according to
their importance. This paper presents the data, analysis and finding based on
pilot survey. These actual findings of this survey will contribute to build new
methodology for developing web applications in small software firms taking to
account how to integrate the suitable measurement program to the whole
development process and also will provide useful information to those who are
doing research in the same area.
",3,4
84,"  Crowdtesting has grown to be an effective alter-native to traditional
testing, especially in mobile apps. However,crowdtesting is hard to manage in
nature. Given the complexity of mobile applications and unpredictability of
distributed, parallel crowdtesting process, it is difficult to estimate (a) the
remaining number of bugs as yet undetected or (b) the required cost to find
those bugs. Experience-based decisions may result in ineffective crowdtesting
process.
  This paper aims at exploring automated decision support to effectively manage
crowdtesting process. The proposed ISENSE applies incremental sampling
technique to process crowdtesting reports arriving in chronological order,
organizes them into fixed-size groups as dynamic inputs, and predicts two test
completion indicators in an incrementally manner. The two indicators are:
1)total number of bugs predicted with Capture-ReCapture (CRC)model, and 2)
required test cost for achieving certain test objectives predicted with
AutoRegressive Integrated Moving Average(ARIMA) model. We assess ISENSE using
46,434 reports of 218 crowdtesting tasks from one of the largest crowdtesting
platforms in China. Its effectiveness is demonstrated through two applications
for automating crowdtesting management, i.e. automation oftask closing
decision, and semi-automation of task closing trade-off analysis. The results
show that decision automation using ISENSE will provide managers with greater
opportunities to achieve cost-effectiveness gains of crowdtesting.
Specifically, a median of 100% bugs can be detected with 30% saved cost basedon
the automated close prediction
",1,5
85,"  In Europe and North America, the most widely used stream cipher to ensure
privacy and confidentiality of conversations in GSM mobile phones is the A5/1.
In this paper, we present a new attack on the A5/1 stream cipher with an
average time complexity of 2^(48.5), which is much less than the brute-force
attack with a complexity of 2^(64). The attack has a 100% success rate and
requires about 5.65GB storage. We provide a detailed description of our new
attack along with its implementation and results.
",2,1
86,"  Cloud computing datacenters provide thousands to millions of virtual machines
(VMs) on-demand in highly dynamic environments, requiring quick placement of
requested VMs into available physical machines (PMs). Due to the randomness of
customer requests, the Virtual Machine Placement (VMP) should be formulated as
an online optimization problem.
  The first part of this work analyzes alternatives to solve the formulated
problem, an experimental comparison of five different online deterministic
heuristics against an offline memetic algorithm with migration of VMs was
performed, considering several experimental workloads. Simulations indicate
that First-Fit Decreasing algorithm (A4) outperforms other evaluated heuristics
on average.
  This work presents a two-phase schema formulation of a VMP problem
considering the optimization of three objective functions in an IaaS
environment with elasticity and overbooking capabilities. The two-phase schema
formulation describes that the allocation of the VMs can be separated into two
sub-problems, the incremental allocation (iVMP) and the reconfiguration of a
placement (VMPr).
  To analyze alternatives to solve the formulated problem, an experimental
comparison of three different objective function scalarization methods as part
of the iVMP and VMPr was performed considering several experimental workloads.
Simulations indicate that the Euclidean distance to origin outperforms other
evaluated scalarization methods on average.
  In order to portray the dynamic nature of an IaaS environment a customizable
workload trace generator was developed to simulate uncertainty in the scenarios
with elasticity and overbooking of resources in VM requests.
  Experimental results proved that the Euclidean distance is preferable over
the other scalarizatiom methods to improve the values of the power consumption
objective function.
",1,6
87,"  Similarity search is a core component in various applications such as image
matching, product recommendation and low-shot classification. However, single
machine solutions are usually insufficient due to the large cardinality of
modern datasets and stringent latency requirement of on-line query processing.
We present Pyramid, a general and efficient framework for distributed
similarity search. Pyramid supports search with popular similarity functions
including Euclidean distance, angular distance and inner product. Different
from existing distributed solutions that are based on KD-tree or locality
sensitive hashing (LSH), Pyramid is based on Hierarchical Navigable Small World
graph (HNSW), which is the state of the art similarity search algorithm on a
single machine. To achieve high query processing throughput, Pyramid partitions
a dataset into sub-datasets containing similar items for index building and
assigns a query to only some of the sub-datasets for query processing. To
provide the robustness required by production deployment, Pyramid also supports
failure recovery and straggler mitigation. Pyramid offers a set of concise API
such that users can easily use Pyramid without knowing the details of
distributed execution. Experiments on large-scale datasets show that Pyramid
produces quality results for similarity search, achieves high query processing
throughput and is robust under node failure and straggler.
",1,4
88,"  This paper introduces a new method of Blockchain formation for reliable
storage of personal data of ID-card holders. In particular, the model of the
information system is presented, the new structure of smart ID-cards and
information on these cards are proposed. The new structure of Blockchain -
""Blockchain Tree"" allows not only to store information from ID-cards but also
to increase the level of security and access control to this information. The
proposed Subchains system allows to integrate Blockchain of the lower level to
Blockchain of the higher level, allowing to create a multilevel protected
system.
",6,0
89,"  Traditional control environments connected to physical systems are being
upgraded with novel information and communication technologies. The resulting
systems need to be adequately protected. Experimental testbeds are crucial for
the study and analysis of ongoing threats against those resulting
cyber-physical systems. The research presented in this paper discusses some
actions towards the development of a replicable and affordable cyber-physical
testbed for training and research. The architecture of the testbed is based on
real-world components, and emulates cyber-physical scenarios commanded by SCADA
(Supervisory Control And Data Acquisition) technologies. We focus on two
representative protocols, Modbus and DNP3. The paper reports as well the
development of some adversarial scenarios, in order to evaluate the testbed
under cyber-physical threat situations. Some detection strategies are evaluated
using our proposed testbed.
",6,5
90,"  This work studies the problem of GPU thread mapping for a Sierpi\'nski gasket
fractal embedded in a discrete Euclidean space of $n \times n$. A block-space
map $\lambda: \mathbb{Z}_{\mathbb{E}}^{2} \mapsto \mathbb{Z}_{\mathbb{F}}^{2}$
is proposed, from Euclidean parallel space $\mathbb{E}$ to embedded fractal
space $\mathbb{F}$, that maps in $\mathcal{O}(\log_2 \log_2(n))$ time and uses
no more than $\mathcal{O}(n^\mathbb{H})$ threads with $\mathbb{H} \approx
1.58...$ being the Hausdorff dimension, making it parallel space efficient.
When compared to a bounding-box map, $\lambda(\omega)$ offers a sub-exponential
improvement in parallel space and a monotonically increasing speedup once $n >
n_0$. Experimental performance tests show that in practice $\lambda(\omega)$
can produce performance improvement at any block-size once $n > n_0 = 2^8$,
reaching approximately $10\times$ of speedup for $n=2^{16}$ under optimal block
configurations.
",1,4
91,"  We provide a closed form upper bound formulation for the average
pairwise-error probability (PEP) of selective decode and forward (SDF)
cooperation protocol for a keyhole (pinhole) channel condition. We have
employed orthogonal space-time block-code scheme (OSTBC) in conjunction with
multi-antenna (MIMO) technology. We have used moment generating function (MGF)
based approach for deriving the upper bound of PEP. PEP expression provides
information regarding the performance of the wireless system with respect to
the channel conditions. We have included simulation results which confirm the
analytical results of our proposed upper bound. Simulation results show that
due to keyhole effect performance of wireless system degrades.
",3,5
92,"  Now a day ad hoc mobile networks (MANETs) have lots of routing protocols, but
no one can meet maximum performance. Some are good in a small network; some are
suitable in large networks, and some give better performance in location or
global networks. Today modern and innovative applications for health care
environments based on a wireless network are being developed in the commercial
sectors. The emerging wireless networks are rapidly becoming a fundamental part
of every single field of life. Our proposed DEERP framework gives a better
performance as compared to other routing protocol.
",2,5
93,"  The flexible flow shop scheduling problem is an NP-hard problem and it
requires significant resolution time to find optimal or even adequate solutions
when dealing with large size instances. Thus, this paper proposes a dual island
genetic algorithm consisting of a parallel cellular model and a parallel pseudo
model. This is a two-level parallelization highly consistent with the
underlying architecture and is well suited for parallelizing inside or between
GPUs and a multi-core CPU. At the higher level, the efficiency of island GAs is
improved by exploring new regions within the search space utilizing different
methods. In the meantime, the cellular model keeps the population diversity by
decentralization and the pseudo model enhances the search ability by the
complementary parent strategy at the lower level. To encourage the information
sharing between islands, a penetration inspired migration policy is designed
which sets the topology, the rate, the interval and the strategy adaptively.
Finally, the proposed method is tested on some large size flexible flow shop
scheduling instances in comparison with other parallel algorithms. The
computational results show that it cannot only obtain competitive results but
also reduces execution time.
",1,5
94,"  In this paper, we investigate the code-switching detection performance of a
code-switching (CS) automatic speech recognition (ASR) system with
data-augmented acoustic and language models. We focus on the recognition of
Frisian-Dutch radio broadcasts where one of the mixed languages, namely
Frisian, is under-resourced. Recently, we have explored how the acoustic
modeling (AM) can benefit from monolingual speech data belonging to the
high-resourced mixed language. For this purpose, we have trained
state-of-the-art AMs on a significantly increased amount of CS speech by
applying automatic transcription and monolingual Dutch speech. Moreover, we
have improved the language model (LM) by creating CS text in various ways
including text generation using recurrent LMs trained on existing CS text.
Motivated by the significantly improved CS ASR performance, we delve into the
CS detection performance of the same ASR system in this work by reporting CS
detection accuracies together with a detailed detection error analysis.
",1,5
95,"  We give a strongly polynomial time algorithm which determines whether or not
a bivariate polynomial is real stable. As a corollary, this implies an
algorithm for testing whether a given linear transformation on univariate
polynomials preserves real-rootedness. The proof exploits properties of
hyperbolic polynomials to reduce real stability testing to testing
nonnegativity of a finite number of polynomials on an interval.
",5,4
96,"  A minimum path cover (MPC) of a directed acyclic graph (DAG) G = (V,E) is a
minimum-size set of paths that together cover all the vertices of the DAG. The
size k of a MPC is also called the width of G. Computing a MPC is a basic
problem, dating back to Dilworth's and Fulkerson's results in the 1950s, and is
solvable in quadratic time in the worst case. Since the width of the DAG can be
small in practical applications (e.g., from bioinformatics), research has also
studied algorithms whose complexity is parameterized on k. Despite these
efforts, it is a major open problem whether there exists a linear-time
$O(f(k)(|E| + |V|))$ parameterized algorithm. We present here two significant
results in this direction.
  First, we obtain an $O(|E| + k^2|V|\log{|V|})$-time algorithm, which in
particular is faster than all existing MPC algorithms when $k =
o(\sqrt{|V|}/\log{|V|})$ and $|E| = \omega(k|V|)$ but $|E| = o(|V|^2)$. We
obtain this by a new combination of three techniques: transitive edge
sparsification, divide-and-conquer, and shrinking. This algorithm is also
simple and can be parallelized, making it ideal for practical use. We also show
that some basic problems on DAGs (reachability queries, longest increasing /
common subsequence, co-linear chaining) get faster algorithms as immediate
corollaries of this result.
  Second, we obtain an $O(poly(k)(2^k|E| + 4^k|V|))$-time algorithm for the
dual problem of computing the width of the DAG. This is based on the notion of
frontier antichains, generalizing the standard notion of right-most maximum
antichain. As we process the vertices in a topological order, these at most
$2^k$ frontier antichains can be maintained with the help of several
combinatorial properties. As such, it is enough to sweep the graph once from
left to right, paying only f(k) along the way, which is a new surprising
insight into the classical MPC problem.
",1,6
97,"  In the FAME! project, we aim to develop an automatic speech recognition (ASR)
system for Frisian-Dutch code-switching (CS) speech extracted from the archives
of a local broadcaster with the ultimate goal of building a spoken document
retrieval system. Unlike Dutch, Frisian is a low-resourced language with a very
limited amount of manually annotated speech data. In this paper, we describe
several automatic annotation approaches to enable using of a large amount of
raw bilingual broadcast data for acoustic model training in a semi-supervised
setting. Previously, it has been shown that the best-performing ASR system is
obtained by two-stage multilingual deep neural network (DNN) training using 11
hours of manually annotated CS speech (reference) data together with speech
data from other high-resourced languages. We compare the quality of
transcriptions provided by this bilingual ASR system with several other
approaches that use a language recognition system for assigning language labels
to raw speech segments at the front-end and using monolingual ASR resources for
transcription. We further investigate automatic annotation of the speakers
appearing in the raw broadcast data by first labeling with (pseudo) speaker
tags using a speaker diarization system and then linking to the known speakers
appearing in the reference data using a speaker recognition system. These
speaker labels are essential for speaker-adaptive training in the proposed
setting. We train acoustic models using the manually and automatically
annotated data and run recognition experiments on the development and test data
of the FAME! speech corpus to quantify the quality of the automatic
annotations. The ASR and CS detection results demonstrate the potential of
using automatic language and speaker tagging in semi-supervised bilingual
acoustic model training.
",5,1
98,"  We present a novel neural model HyperVec to learn hierarchical embeddings for
hypernymy detection and directionality. While previous embeddings have shown
limitations on prototypical hypernyms, HyperVec represents an unsupervised
measure where embeddings are learned in a specific order and capture the
hypernym$-$hyponym distributional hierarchy. Moreover, our model is able to
generalize over unseen hypernymy pairs, when using only small sets of training
data, and by mapping to other languages. Results on benchmark datasets show
that HyperVec outperforms both state$-$of$-$the$-$art unsupervised measures and
embedding models on hypernymy detection and directionality, and on predicting
graded lexical entailment.
",1,4
99,"  We believe that there is no real data protection without our own tools.
Therefore, our permanent aim is to have more of our own codes. In order to
achieve that, it is necessary that a lot of young researchers become interested
in cryptography. We believe that the encoding of cryptographic algorithms is an
important step in that direction, and it is the main reason why in this paper
we present a software implementation of finding the inverse element, the
operation which is essentially related to both ECC (Elliptic Curve
Cryptography) and the RSA schemes of digital signature.
",5,1
100,"  We investigate computational issues in the distributed model Amoebots of
programmable matter. In this model, the computational entities, called
particles, are anonymous finite-state machines that operate and move on an
hexagonal tasselation of the plane. In this paper we show how a constant number
of such weak particles can simulate a powerful Turing-complete entity that is
able to move on the plane while computing. We then show an application of our
tool to the classical Shape-Formation problem, providing a new and much more
general distributed solution protocol. Indeed, the existing algorithms would
allow to form only shapes made of arrangements of segments and triangles. Our
algorithm allows the particles to form more abstract and general connected
shapes, including circles and spirals, as well as fractal objects of
non-integer dimension, such as the Sierpinski triangle or the Koch snowflake.
In lieu of the existing limitation on the formability of a shape depending on
the symmetry of the initial configuration of the particles, our result provides
a complete characterization of the connected shapes that can be formed by an
initially simply connected set of particles. Furthermore, in the case of
non-connected shapes, we give almost-matching necessary and sufficient
conditions for their formability.
",2,5
101,"  Scenario-based development and test processes are a promising approach for
verifying and validating automated driving functions. For this purpose,
scenarios have to be generated during the development process in a traceable
manner. In early development stages, the operating scenarios of the item to be
developed are usually described in an abstract, linguistic way.Within the scope
of a simulation-assisted test process, these linguistically described scenarios
have to be transformed into a state space representation and converted into
data formats which can be used with the respective simulation environment.
Currently, this step of detailing scenarios takes a considerable manual effort.
Furthermore, a standardized interpretation of the linguistically described
scenarios and a consistent transformation into the data formats are not
guaranteed due to multiple authors as well as many constraints between the
scenario parameters. In this paper, the authors present an approach to
automatically detail a keyword-based scenario description for execution in a
simulation environment and provide a basis for test case generation. As a first
step, the keyword-based description is transformed into a parameter space
representation. At the same time, constraints regarding the selection and
combination of parameter values are documented for the following process steps
(e. g. evolutionary or stochastic test methods). As a second step, the
parameter space representation is converted into data formats required by the
simulation environment. As an example, the authors use scenarios on German
freeways and convert them into the data formats OpenDRIVE (description of the
road) and OpenSCENARIO (description of traffic participants and environmental
conditions) for execution in the simulation environment Virtual Test Drive.
",1,3
102,"  The problem of finding maximum (or minimum) witnesses of the Boolean product
of two Boolean matrices (MW for short) has a number of important applications,
in particular the all-pairs lowest common ancestor (LCA) problem in directed
acyclic graphs (dags). The best known upper time-bound on the MW problem for
n\times n Boolean matrices of the form O(n^{2.575}) has not been substantially
improved since 2006. In order to obtain faster algorithms for this problem, we
study quantum algorithms for MW and approximation algorithms for MW (in the
standard computational model). Some of our quantum algorithms are input or
output sensitive. Our fastest quantum algorithm for the MW problem, and
consequently for the related problems, runs in time
\tilde{O}(n^{2+\lambda/2})=\tilde{O}(n^{2.434}), where \lambda satisfies the
equation \omega(1, \lambda, 1) = 1 + 1.5 \, \lambda and \omega(1, \lambda, 1)
is the exponent of the multiplication of an n \times n^{\lambda}$ matrix by an
n^{\lambda} \times n matrix. Next, we consider a relaxed version of the MW
problem (in the standard model) asking for reporting a witness of bounded rank
(the maximum witness has rank 1) for each non-zero entry of the matrix product.
First, by adapting the fastest known algorithm for maximum witnesses, we obtain
an algorithm for the relaxed problem that reports for each non-zero entry of
the product matrix a witness of rank at most \ell in time
\tilde{O}((n/\ell)n^{\omega(1,\log_n \ell,1)}). Then, by reducing the relaxed
problem to the so called k-witness problem, we provide an algorithm that
reports for each non-zero entry C[i,j] of the product matrix C a witness of
rank O(\lceil W_C(i,j)/k\rceil ), where W_C(i,j) is the number of witnesses for
C[i,j], with high probability. The algorithm runs in
\tilde{O}(n^{\omega}k^{0.4653} +n^2k) time, where \omega=\omega(1,1,1).
",4,3
103,"  We evaluate the character-level translation method for neural semantic
parsing on a large corpus of sentences annotated with Abstract Meaning
Representations (AMRs). Using a sequence-to-sequence model, and some trivial
preprocessing and postprocessing of AMRs, we obtain a baseline accuracy of 53.1
(F-score on AMR-triples). We examine five different approaches to improve this
baseline result: (i) reordering AMR branches to match the word order of the
input sentence increases performance to 58.3; (ii) adding part-of-speech tags
(automatically produced) to the input shows improvement as well (57.2); (iii)
So does the introduction of super characters (conflating frequent sequences of
characters to a single character), reaching 57.4; (iv) optimizing the training
process by using pre-training and averaging a set of models increases
performance to 58.7; (v) adding silver-standard training data obtained by an
off-the-shelf parser yields the biggest improvement, resulting in an F-score of
64.0. Combining all five techniques leads to an F-score of 71.0 on holdout
data, which is state-of-the-art in AMR parsing. This is remarkable because of
the relative simplicity of the approach.
",2,6
104,"  Identifying the causal relationships between subjects or variables remains an
important problem across various scientific fields. This is particularly
important but challenging in complex systems, such as those involving human
behavior, sociotechnical contexts, and natural ecosystems. By exploiting state
space reconstruction via lagged embedding of time series, convergent cross
mapping (CCM) serves as an important method for addressing this problem. While
powerful, CCM is computationally costly; moreover, CCM results are highly
sensitive to several parameter values. While best practice entails exploring a
range of parameter settings when assessing casual relationships, the resulting
computational burden can raise barriers to practical use, especially for long
time series exhibiting weak causal linkages. We demonstrate here several means
of accelerating CCM by harnessing the distributed Apache Spark platform. We
characterize and report on results of several experiments with parallelized
solutions that demonstrate high scalability and a capacity for over an order of
magnitude performance improvement for the baseline configuration. Such
economies in computation time can speed learning and robust identification of
causal drivers in complex systems.
",5,6
105,"  Collective communications are ubiquitous in parallel applications. We present
two new algorithms for performing a reduction. The operation associated with
our reduction needs to be associative and commutative. The two algorithms are
developed under two different communication models (unidirectional and
bidirectional). Both algorithms use a greedy scheduling scheme. For a
unidirectional, fully connected network, we prove that our greedy algorithm is
optimal when some realistic assumptions are respected. Previous algorithms fit
the same assumptions and are only appropriate for some given configurations.
Our algorithm is optimal for all configurations. We note that there are some
configuration where our greedy algorithm significantly outperform any existing
algorithms. This result represents a contribution to the state-of-the art. For
a bidirectional, fully connected network, we present a different greedy
algorithm. We verify by experimental simulations that our algorithm matches the
time complexity of an optimal broadcast (with addition of the computation).
Beside reversing an optimal broadcast algorithm, the greedy algorithm is the
first known reduction algorithm to experimentally attain this time complexity.
Simulations show that this greedy algorithm performs well in practice,
outperforming any state-of-the-art reduction algorithms. Positive experiments
on a parallel distributed machine are also presented.
",6,0
106,"  We give deterministic distributed $(1+\epsilon)$-approximation algorithms for
Minimum Vertex Coloring and Maximum Independent Set on chordal graphs in the
LOCAL model. Our coloring algorithm runs in $O(\frac{1}{\epsilon} \log n)$
rounds, and our independent set algorithm has a runtime of
$O(\frac{1}{\epsilon}\log(\frac{1}{\epsilon})\log^* n)$ rounds. For coloring,
existing lower bounds imply that the dependencies on $\frac{1}{\epsilon}$ and
$\log n$ are best possible. For independent set, we prove that
$O(\frac{1}{\epsilon})$ rounds are necessary.
  Both our algorithms make use of a tree decomposition of the input chordal
graph. They iteratively peel off interval subgraphs, which are identified via
the tree decomposition of the input graph, thereby partitioning the vertex set
into $O(\log n)$ layers. For coloring, each interval graph is colored
independently, which results in various coloring conflicts between the layers.
These conflicts are then resolved in a separate phase, using the particular
structure of our partitioning. For independent set, only the first $O( \log
\frac{1}{\epsilon})$ layers are required as they already contain a large enough
independent set. We develop a $(1+\epsilon)$-approximation maximum independent
set algorithm for interval graphs, which we then apply to those layers.
  This work raises the question as to how useful tree decompositions are for
distributed computing.
",1,5
107,"  The incompleteness of data collected from BGP route collecting projects is a
well-known issue which potentially affects every research activity carried out
on the analysis of the Internet inter-domain routing. Recent works explained
that one of the possible solutions is to increase the number of ASes feeding
these projects from the Internet periphery, in order to reveal the hidden
portion of peering connectivity of their upstream providers. The main problem
is that these projects are currently not appealing enough for the network
administrators of these ASes, which are typically not aware of their existence
or not interested enough to share their data. Our contribution is Isolario, a
project based on the do-ut-des principle which aims at persuading network
administrators to share their routing information by offering services in
return, ranging from real-time analyses of the incoming BGP session(s) to
historic analyses of routing reachability. To the best of our knowledge,
Isolario is the only route collecting project publicly available which offers a
set of services to its users to encourage their participation, aiming at
increasing the amount of BGP data publicly available for research purposes.
",6,2
108,"  There are many case studies for which the formulation of RDF constraints and
the validation of RDF data conforming to these constraint is very important. As
a part of the collaboration with the W3C and the DCMI working groups on RDF
validation, we identified major RDF validation requirements and initiated an
RDF validation requirements database which is available to contribute at
http://purl.org/net/rdf-validation. The purpose of this database is to
collaboratively collect case studies, use cases, requirements, and solutions
regarding RDF validation. Although, there are multiple constraint languages
which can be used to formulate RDF constraints (associated with these
requirements), there is no standard way to formulate them. This paper serves to
evaluate to which extend each requirement is satisfied by each of these
constraint languages. We take reasoning into account as an important
pre-validation step and therefore map constraints to DL in order to show that
each constraint can be mapped to an ontology describing RDF constraints
generically.
",3,2
109,"  This paper introduced NgViz, a tool that examines DNS traffic and shows
anomalies in n-gram frequencies. This is accomplished by comparing input files
against a fingerprint of legitimate traffic. Both quantitative analysis and
visual aids are provided that allow the user to make determinations about the
legitimacy of the DNS traffic.
",2,3
110,"  Delay tolerant network (DTN) is opportunistic network where each node
searches best opportunity to deliver the message called bundle to the
destination. DTN implements a store and forward message switching system by
simply introducing another new protocol layer called the Bundle Layer on top of
the transport layer. The bundle layer is responsible for storing and forwarding
entire message in message segments called bundles between source node and
destination node. This paper evaluates the performance of delay tolerant
network layer in heterogeneous highly dense mobile node environment. The
heterogeneous network is created with the help of stationary wired node and
Base Station node by introducing dynamic dense Mobile node network. Mobile
nodes are assigned with continuous mobility. Three parameters are suggested
$\Delta$, $\Theta$ and $\lambda$ to correlate the results obtained using
rigorous simulation. Results show that after some threshold values, dense
feature about mobile node does not pretend the delay cause for delay tolerant
network packets. Also, increase in number of mobile node and number of File
Transfer connection rarely change the overall performance of the delay tolerant
network.
",0,6
111,"  In the \textit{Matroid Secretary Problem} (MSP), the elements of the ground
set of a Matroid are revealed on-line one by one, each together with its value.
An algorithm for the MSP is \textit{Matroid-Unknown} if, at every stage of its
execution: (i) it only knows the elements that have been revealed so far and
their values, and (ii) it has access to an oracle for testing whether or not
any subset of the elements that have been revealed so far is an independent
set. An algorithm is \textit{Known-Cardinality} if, in addition to (i) and
(ii), it also initially knows the cardinality of the ground set of the Matroid.
We present here a Known-Cardinality and \textit{Order-Oblivious} algorithm
that, with constant probability, selects an independent set of elements, whose
value is at least the optimal value divided by $O(\log{\log{\rho}})$, where
$\rho$ is the rank of the Matroid; that is, the algorithm has a
\textit{competitive-ratio} of $O(\log{\log{\rho}})$. The best previous results
for a Known-Cardinality algorithm are a competitive-ratio of $O(\log{\rho})$,
by Babaioff \textit{et al.} (2007), and a competitive-ratio of
$O(\sqrt{\log{\rho}})$, by Chakraborty and Lachish (2012). In many non-trivial
cases the algorithm we present has a competitive-ratio that is better than the
$O(\log{\log{\rho}})$. The cases in which it fails to do so are easily
characterized. Understanding these cases may lead to improved algorithms for
the problem or, conversely, to non-trivial lower bounds.
",5,1
112,"  We focus on multiple-choice question answering (QA) tasks in subject areas
such as science, where we require both broad background knowledge and the facts
from the given subject-area reference corpus. In this work, we explore simple
yet effective methods for exploiting two sources of external knowledge for
subject-area QA. The first enriches the original subject-area reference corpus
with relevant text snippets extracted from an open-domain resource (i.e.,
Wikipedia) that cover potentially ambiguous concepts in the question and answer
options. As in other QA research, the second method simply increases the amount
of training data by appending additional in-domain subject-area instances.
  Experiments on three challenging multiple-choice science QA tasks (i.e.,
ARC-Easy, ARC-Challenge, and OpenBookQA) demonstrate the effectiveness of our
methods: in comparison to the previous state-of-the-art, we obtain absolute
gains in accuracy of up to 8.1%, 13.0%, and 12.8%, respectively. While we
observe consistent gains when we introduce knowledge from Wikipedia, we find
that employing additional QA training instances is not uniformly helpful:
performance degrades when the added instances exhibit a higher level of
difficulty than the original training data. As one of the first studies on
exploiting unstructured external knowledge for subject-area QA, we hope our
methods, observations, and discussion of the exposed limitations may shed light
on further developments in the area.
",2,3
113,"  Out of the very few studies that paid proper attention to the harmful health
impacts in millimeter-wave (mmW) communications, most of them are concerned
about uplink cases due to closer contact with the human body. Our recent study
revealed that even the human exposure to radio frequency (RF) fields in
downlink mmW technology is not very minimum to be ignored. There were a few RF
exposure mitigation techniques for uplinks, but the downlink scenario is hardly
paid any attention. However, this paper proposes a downlink protocol for mmW
cellular communications that achieves the maximum data rate while keeping the
impacts on human health minimized. Our results show that the proposed technique
lowers both power density (PD) and specific absorption rate (SAR) compared to
the typical protocol, with only slight sacrifice in data rates.
",4,6
114,"  Energy efficiency can have a significant influence on user experience of
mobile devices such as smartphones and tablets. Although energy is consumed by
hardware, software optimization plays an important role in saving energy, and
thus software developers have to participate in the optimization process. The
source code is the interface between the developer and hardware resources. In
this paper, we propose an energy-optimization framework guided by a source code
energy model that allows developers to be aware of energy usage induced by the
code and to apply very targeted source-level refactoring strategies. The
framework also lays a foundation for the code optimization by automatic tools.
To the best of our knowledge, our work is the first that achieves this for a
high-level language such as Java. In a case study, the experimental evaluation
shows that our approach is able to save from 6.4% to 50.2% of the CPU energy
consumption in various application scenarios.
",2,1
115,"  The Skolem problem and the related Positivity problem for linear recurrence
sequences are outstanding number-theoretic problems whose decidability has been
open for many decades. In this paper, the inherent mathematical difficulty of a
series of optimization problems on Markov decision processes (MDPs) is shown by
a reduction from the Positivity problem to the associated decision problems
which establishes that the problems are also at least as hard as the Skolem
problem as an immediate consequence. The optimization problems under
consideration are two non-classical variants of the stochastic shortest path
problem (SSPP) in terms of expected partial or conditional accumulated weights,
the optimization of the conditional value-at-risk for accumulated weights, and
two problems addressing the long-run satisfaction of path properties, namely
the optimization of long-run probabilities of regular co-safety properties and
the model-checking problem of the logic frequency-LTL. To prove the Positivity-
and hence Skolem-hardness for the latter two problems, a new auxiliary path
measure, called weighted long-run frequency, is introduced and the
Positivity-hardness of the corresponding decision problem is shown as an
intermediate step. For the partial and conditional SSPP on MDPs with
non-negative weights and for the optimization of long-run probabilities of
constrained reachability properties (a U b), solutions are known that rely on
the identification of a bound on the accumulated weight or the number of
consecutive visits to certain sates, called a saturation point, from which on
optimal schedulers behave memorylessly. In this paper, it is shown that also
the optimization of the conditional value-at-risk for the classical SSPP and of
weighted long-run frequencies on MDPs with non-negative weights can be solved
in pseudo-polynomial time exploiting the existence of a saturation point.
",5,2
116,"  Smartphone users and application behaviors add high pressure to apply smart
techniques that stabilize the network capacity and consequently improve the
end-user experience. The massive increase in smartphone penetration engenders
signalling load, which exceeds the network capacity in terms of signaling. The
signalling load leads to network congestion, degradation in the network KPIs.
The classical way to tackle the signalling is by network expansion. However,
this approach is not efficient in terms of capital expenditure (CAPEX) and also
in terms of efficient utilization of the network resources. More specifically,
the signaling domain becomes overloaded while the data domain are
underutilized. In this paper, two UMTS air-interface features; Cell-PCH (paging
channel) and enhanced fast dormancy (E-FD) are analyzed to mitigate the
signalling load. Practical performance analysis is conducted based on results
from commercial UMTS networks. The deployment of these features offers major
improvement in network KPIs and significant relief in the signaling load. It is
concluded that these two features in addition to several optimization
techniques, discussed in this paper, provide solution to the smartphone
signaling load and efficiently utilize the available network and spectrum
resources while providing users with better always-on connectivity and improved
battery consumption.
",4,0
117,"  Entity linking (EL) is the task of disambiguating mentions in text by
associating them with entries in a predefined database of mentions (persons,
organizations, etc). Most previous EL research has focused mainly on one
language, English, with less attention being paid to other languages, such as
Spanish or Chinese. In this paper, we introduce LIEL, a Language Independent
Entity Linking system, which provides an EL framework which, once trained on
one language, works remarkably well on a number of different languages without
change. LIEL makes a joint global prediction over the entire document,
employing a discriminative reranking framework with many domain and
language-independent feature functions. Experiments on numerous benchmark
datasets, show that the proposed system, once trained on one language, English,
outperforms several state-of-the-art systems in English (by 4 points) and the
trained model also works very well on Spanish (14 points better than a
competitor system), demonstrating the viability of the approach.
",5,0
118,"  In this paper, we present the annotation pipeline and the guidelines we wrote
as part of an effort to create a large manually annotated Arabic author
profiling dataset from various social media sources covering 16 Arabic
countries and 11 dialectal regions. The target size of the annotated ARAP-Tweet
corpus is more than 2.4 million words. We illustrate and summarize our general
and dialect-specific guidelines for each of the dialectal regions selected. We
also present the annotation framework and logistics. We control the annotation
quality frequently by computing the inter-annotator agreement during the
annotation process. Finally, we describe the issues encountered during the
annotation phase, especially those related to the peculiarities of Arabic
dialectal varieties as used in social media.
",1,2
119,"  Locating bugs is challenging but one of the most important activities in
software development and maintenance phase because there are no certain rules
to identify all types of bugs. Existing automatic bug localization tools use
various heuristics based on test coverage, pre-determined buggy patterns, or
textual similarity with bug report, to rank suspicious program elements.
However, since these techniques rely on information from single source, they
often suffer when the respective source information is inadequate. For
instance, the popular spectrum based bug localization may not work well under
poorly written test suite. In this paper, we propose a new approach, EnSpec,
that guides spectrum based bug localization using code entropy, a metric that
basically represents naturalness of code derived from a statistical language
model. Our intuition is that since buggy code are high entropic, spectrum based
bug localization with code entropy would be more robust in discriminating buggy
lines vs. non-buggy lines. We realize our idea in a prototype, and performed an
extensive evaluation on two popular publicly available benchmarks. Our results
demonstrate that EnSpec outperforms a state-of-the-art spectrum based bug
localization technique.
",1,5
120,"  Principal component analysis (PCA) and related techniques have been
successfully employed in natural language processing. Text mining applications
in the age of the online social media (OSM) face new challenges due to
properties specific to these use cases (e.g. spelling issues specific to texts
posted by users, the presence of spammers and bots, service announcements,
etc.). In this paper, we employ a Robust PCA technique to separate typical
outliers and highly localized topics from the low-dimensional structure present
in language use in online social networks. Our focus is on identifying
geospatial features among the messages posted by the users of the Twitter
microblogging service. Using a dataset which consists of over 200 million
geolocated tweets collected over the course of a year, we investigate whether
the information present in word usage frequencies can be used to identify
regional features of language use and topics of interest. Using the PCA pursuit
method, we are able to identify important low-dimensional features, which
constitute smoothly varying functions of the geographic location.
",5,2
121,"  With the introduction of the smart grid, Advanced Metering Infrastructure
(AMI) has become a main component in the present power system. The effective
implementation of AMI depends widely on its communication infrastructure and
protocols providing trustworthy two-way communications. In this paper we study
two routing protocols philosophies for low power and lossy networks (LLNs) and
their application for a smart metering scenario. This study purposes a detailed
evaluation of two routing protocols proposed by IETF the proactive candidate
namely RPL (IPv6 Routing Protocol for Low-Power and Lossy Networks) and the
reactive candidate named LOADng (LLN On-demand Ad-hoc Distance vector routing
protocol - next generation) recently proposed as an Internet Draft, still in
its design phase and is part of the ITU-T G.9903 recommendation. In the course
of this study, we also implemented an extension version of LOADng named
LOADng-CTP specified by an IETF draft extended with a collection tree for
efficient data acquisition in LLNs. We performed checks on control overhead;
End to End Delay and Packet delivery ratio for the two protocols related to
multipoint-to-point (MP2P), and point-to-multi point (P2MP) traffic flow in a
realistic smart metering architecture.
",2,3
122,"  In the k-Apex problem the task is to find at most k vertices whose deletion
makes the given graph planar. The graphs for which there exists a solution form
a minor closed class of graphs, hence by the deep results of Robertson and
Seymour, there is an O(n^3) time algorithm for every fixed value of k. However,
the proof is extremely complicated and the constants hidden by the big-O
notation are huge. Here we give a much simpler algorithm for this problem with
quadratic running time, by iteratively reducing the input graph and then
applying techniques for graphs of bounded treewidth.
",2,6
123,"  Although cloud storage platforms promise a convenient way for users to share
files and engage in collaborations, they require all files to have a single
owner who unilaterally makes access control decisions. Existing clouds are,
thus, agnostic to shared ownership. This can be a significant limitation in
many collaborations because one owner can, for example, delete files and revoke
access without consulting the other collaborators.
  In this paper, we first formally define a notion of shared ownership within a
file access control model. We then propose a solution, called Commune, to the
problem of distributively enforcing shared ownership in agnostic clouds, so
that access grants require the support of a pre-arranged threshold of owners.
Commune can be used in existing clouds without requiring any modifications to
the platforms. We analyze the security of our solution and evaluate its
scalability and performance by means of an implementation integrated with
Amazon S3.
",1,6
124,"  Distributed Denial of Service (DDoS) attack has become one of the most
destructive network attacks which can pose a mortal threat to Internet
security. Existing detection methods can not effectively detect early attacks.
In this paper, we propose a detection method of DDoS attacks based on
generalized multiple kernel learning (GMKL) combining with the constructed
parameter R. The super-fusion feature value (SFV) and comprehensive degree of
feature (CDF) are defined to describe the characteristic of attack flow and
normal flow. A method for calculating R based on SFV and CDF is proposed to
select the combination of kernel function and regularization paradigm. A DDoS
attack detection classifier is generated by using the trained GMKL model with R
parameter. The experimental results show that kernel function and
regularization parameter selection method based on R parameter reduce the
randomness of parameter selection and the error of model detection, and the
proposed method can effectively detect DDoS attacks in complex environments
with higher detection rate and lower error rate.
",0,6
125,"  In the classic Dial-a-Ride Problem, a server travels in some metric space to
serve requests for rides. Each request has a source, destination, and release
time. We study a variation of this problem where each request also has a
revenue that is earned if the request is satisfied. The goal is to serve
requests within a time limit such that the total revenue is maximized. We first
prove that the version of this problem where edges in the input graph have
varying weights is NP-complete. We also prove that no algorithm can be
competitive for this problem. We therefore consider the version where edges in
the graph have unit weight and develop a 2-competitive algorithm for this
problem.
",1,4
126,"  The concept of Network Function Virtualization (NFV) has been introduced as a
new paradigm in the recent few years. NFV offers a number of benefits including
significantly increased maintainability and reduced deployment overhead.
Several works have been done to optimize deployment (also called embedding) of
virtual network functions (VNFs). However, no work to date has looked into
optimizing the selection of cloud instances for a given VNF and its specific
requirements. In this paper, we evaluate the performance of VNFs when embedded
on different Amazon EC2 cloud instances. Specifically, we evaluate three VNFs
(firewall, IDS, and NAT) in terms of arrival packet rate, resources
utilization, and packet loss. Our results indicate that performance varies
across instance types, departing from the intuition of ""you get what you pay
for"" with cloud instances. We also find out that CPU is the critical resource
for the tested VNFs, although their peak packet processing capacities differ
considerably from each other. Finally, based on the obtained results, we
identify key research challenges related to VNF instance selection and service
chain provisioning.
",1,5
127,"  Inductions and game semantics are two useful extensions to traditional logic
programming. To be specific, inductions can capture a wider class of provable
formulas in logic programming. Adopting game semantics can make logic
programming more interactive.
  In this paper, we propose an execution model for a logic language with these
features. This execution model follows closely the reasoning process in real
life.
",2,6
128,"  Statistics pedagogy values using a variety of examples. Thanks to text
resources on the Web, and since statistical packages have the ability to
analyze string data, it is now easy to use language-based examples in a
statistics class. Three such examples are discussed here. First, many types of
wordplay (e.g., crosswords and hangman) involve finding words with letters that
satisfy a certain pattern. Second, linguistics has shown that idiomatic pairs
of words often appear together more frequently than chance. For example, in the
Brown Corpus, this is true of the phrasal verb to throw up (p-value=7.92E-10.)
Third, a pangram contains all the letters of the alphabet at least once. These
are searched for in Charles Dickens' A Christmas Carol, and their lengths are
compared to the expected value given by the unequal probability coupon
collector's problem as well as simulations.
",2,3
129,"  Given a straight-line drawing $\Gamma$ of a graph $G=(V,E)$, for every vertex
$v$ the ply disk $D_v$ is defined as a disk centered at $v$ where the radius of
the disk is half the length of the longest edge incident to $v$. The ply number
of a given drawing is defined as the maximum number of overlapping disks at
some point in $\mathbb{R}^2$. Here we present a tool to explore and evaluate
the ply number for graphs with instant visual feedback for the user. We
evaluate our methods in comparison to an existing ply computation by De Luca et
al. [WALCOM'17]. We are able to reduce the computation time from seconds to
milliseconds for given drawings and thereby contribute to further research on
the ply topic by providing an efficient tool to examine graphs extensively by
user interaction as well as some automatic features to reduce the ply number.
",2,4
130,"  This work presents, to the best of our knowledge of the literature, the first
analytic model to address the performance of an LRU (Least Recently Used)
implementing cache under non-stationary traffic conditions, i.e., when the
popularity of content evolves with time. We validate the accuracy of the model
using Monte Carlo simulations. We show that the model is capable of accurately
estimating the cache hit probability, when the popularity of content is
non-stationary.
  We find that there exists a dependency between the performance of an LRU
implementing cache and i) the lifetime of content in a system, ii) the volume
of requests associated with it, iii) the distribution of content request
volumes and iv) the shape of the popularity profile over time.
",1,5
131,"  Environmental science is often fragmented: data is collected using mismatched
formats and conventions, and models are misaligned and run in isolation. Cloud
computing offers a lot of potential in the way of resolving such issues by
supporting data from different sources and at various scales, by facilitating
the integration of models to create more sophisticated software services, and
by providing a sustainable source of suitable computational and storage
resources. In this paper, we highlight some of our experiences in building the
Environmental Virtual Observatory pilot (EVOp), a tailored cloud-based
infrastructure and associated web-based tools designed to enable users from
different backgrounds to access data concerning different environmental issues.
We review our architecture design, the current deployment and prototypes. We
also reflect on lessons learned. We believe that such experiences are of
benefit to other scientific communities looking to assemble virtual
observatories or similar virtual research environments.
",2,5
132,"  Approximating Subset Sum is a classic and fundamental problem in computer
science and mathematical optimization. The state-of-the-art approximation
scheme for Subset Sum computes a $(1-\varepsilon)$-approximation in time
$\tilde{O}(\min\{n/\varepsilon, n+1/\varepsilon^2\})$ [Gens, Levner'78,
Kellerer et al.'97]. In particular, a $(1-1/n)$-approximation can be computed
in time $O(n^2)$.
  We establish a connection to the Min-Plus-Convolution problem, which is of
particular interest in fine-grained complexity theory and can be solved naively
in time $O(n^2)$. Our main result is that computing a $(1-1/n)$-approximation
for Subset Sum is subquadratically equivalent to Min-Plus-Convolution. Thus,
assuming the Min-Plus-Convolution conjecture from fine-grained complexity
theory, there are no approximation schemes for Subset Sum with strongly
subquadratic dependence on $n$ and $1/\varepsilon$. In the other direction, our
reduction allows us to transfer known lower order improvements from
Min-Plus-Convolution to Subset Sum, which yields a mildly subquadratic
approximation scheme. This adds the first approximation problem to the list of
Min-Plus-Convolution-equivalent problems.
",3,4
133,"  Designing hardware is a time-consuming and complex process. Realization of
both, embedded and high-performance applications can benefit from a design
process on a higher level of abstraction. This helps to reduce development time
and allows to iteratively test and optimize the hardware design during
development, as common in software development. We present our tool, OCLAcc,
which allows the generation of entire FPGA-based hardware accelerators from
OpenCL and discuss the major novelties of OpenCL 2.0 and how they can be
realized in hardware using OCLAcc.
",5,2
134,"  Service virtualization is an approach that uses virtualized environments to
automatically test enterprise services in production-like conditions. Many
techniques have been proposed to provide such a realistic environment for
enterprise services. The Internet-of-Things (IoT) is an emerging field which
connects a diverse set of devices over different transport layers, using a
variety of protocols. Provisioning a virtual testbed of IoT devices can
accelerate IoT application development by enabling automated testing without
requiring a continuous connection to the physical devices. One solution is to
expand existing enterprise service virtualization to IoT environments. There
are various structural differences between the two environments that should be
considered to implement appropriate service virtualization for IoT. This paper
examines the structural differences between various IoT protocols and
enterprise protocols and identifies key technical challenges that need to be
addressed to implement service virtualization in IoT environments.
",5,2
135,"  ""Handover"" is one of the techniques used to achieve the service continuity in
Fourth generation wireless networks (FGWNs). Seamless continuity is the main
goal in fourth generation Wireless networks (FGWNs), when a mobile terminal
(MT) is in overlapping area for service continuity Handover mechanism are
mainly used While moving in the heterogeneous wireless networks continual
connection is the main challenge. Vertical handover is used as a technique to
minimize the processing delay in heterogeneous wireless networks this paper,
Vertical handover decision schemes are compared and Technique of order
preference by similarity to ideal solution (TOPSIS) in a distributed manner.
TOPSIS is used to choose the best network from the available Visitor networks
(VTs) for the continuous connection by the mobile terminal. In our work we
mainly concentrated to the handover decision Phase and to reduce the processing
delay in the period of handover
",1,5
136,"  The third oriental language recognition (OLR) challenge AP18-OLR is
introduced in this paper, including the data profile, the tasks and the
evaluation principles. Following the events in the last two years, namely
AP16-OLR and AP17-OLR, the challenge this year focuses on more challenging
tasks, including (1) short-duration utterances, (2) confusing languages, and
(3) open-set recognition. The same as the previous events, the data of AP18-OLR
is also provided by SpeechOcean and the NSFC M2ASR project. Baselines based on
both the i-vector model and neural networks are constructed for the
participants' reference. We report the baseline results on the three tasks and
demonstrate that the three tasks are truly challenging. All the data is free
for participants, and the Kaldi recipes for the baselines have been published
online.
",4,6
137,"  Neural machine translation (NMT) systems operate primarily on words (or
sub-words), ignoring lower-level patterns of morphology. We present a
character-aware decoder designed to capture such patterns when translating into
morphologically rich languages. We achieve character-awareness by augmenting
both the softmax and embedding layers of an attention-based encoder-decoder
model with convolutional neural networks that operate on the spelling of a
word. To investigate performance on a wide variety of morphological phenomena,
we translate English into 14 typologically diverse target languages using the
TED multi-target dataset. In this low-resource setting, the character-aware
decoder provides consistent improvements with BLEU score gains of up to
$+3.05$. In addition, we analyze the relationship between the gains obtained
and properties of the target language and find evidence that our model does
indeed exploit morphological patterns.
",4,6
138,"  Accessing data from distributed computing is essential in many workflows, but
can be complicated for users of cyberinfrastructure. They must perform multiple
steps to make data available to distributed computing using unfamiliar tools.
Further, most research on data distribution has focused on the efficiency of
providing data to computing resources rather than considering the ease of use
for distributing data. Creating an easy to use data distribution method can
reduce the time researchers spend learning cyberinfrastructure and increase its
usefulness.
  Microsoft OneDrive is a online storage solution providing both file storage
and sharing. OneDrive provides many different clients to access data stored in
the service. It provides many features that users of cyberinfrastructure could
find useful such as automatic synchronization with desktop clients.
  A barrier to using services such as OneDrive is the credential management
necessary to access the service. Recent innovations in HTCondor have allowed
the management of OAuth credentials to be handled by the scheduler on the
user's behalf. The user no longer has to copy credentials along with the job,
HTCondor will handle the acquisition, renewal, and secure transfer of
credentials on the user's behalf.
  In this paper, I will focus on providing an easy to use data distribution
method utilizing Microsoft OneDrive. Measuring ease of use is difficult,
therefore I will will describe the features and advantages of using OneDrive.
Additionally, I will compare it to measurements of data distribution methods
currently used on a national cyberinfastructure, the Open Science Grid.
",5,2
139,"  The IoT is a network of interconnected everyday objects called things that
have been augmented with a small measure of computing capabilities. Lately, the
IoT has been affected by a variety of different botnet activities. As botnets
have been the cause of serious security risks and financial damage over the
years, existing Network forensic techniques cannot identify and track current
sophisticated methods of botnets. This is because commercial tools mainly
depend on signature-based approaches that cannot discover new forms of botnet.
In literature, several studies have conducted the use of Machine Learning ML
techniques in order to train and validate a model for defining such attacks,
but they still produce high false alarm rates with the challenge of
investigating the tracks of botnets. This paper investigates the role of ML
techniques for developing a Network forensic mechanism based on network flow
identifiers that can track suspicious activities of botnets. The experimental
results using the UNSW-NB15 dataset revealed that ML techniques with flow
identifiers can effectively and efficiently detect botnets attacks and their
tracks.
",2,1
140,"  The requirements elicited from stakeholders suffer from various afflictions,
including informality, incompleteness, ambiguity, vagueness, inconsistencies,
and more. It is the task of requirements engineering (RE) processes to derive
from these an eligible (formal, complete enough, unambiguous, consistent,
measurable, satisfiable, modifiable and traceable) requirements specification
that truly captures stakeholder needs.
  We propose Desiree, a refinement calculus for systematically transforming
stakeholder require-ments into an eligible specification. The core of the
calculus is a rich set of requirements operators that iteratively transform
stakeholder requirements by strengthening or weakening them, thereby reducing
incompleteness, removing ambiguities and vagueness, eliminating unattainability
and conflicts, turning them into an eligible specification. The framework also
includes an ontology for modeling and classifying requirements, a
description-based language for representing requirements, as well as a
systematic method for applying the concepts and operators. In addition, we
define the semantics of the requirements concepts and operators, and develop a
graphical modeling tool in support of the entire framework.
  To evaluate our proposal, we have conducted a series of empirical
evaluations, including an ontology evaluation by classifying a large public
requirements set, a language evaluation by rewriting the large set of
requirements using our description-based syntax, a method evaluation through a
realistic case study, and an evaluation of the entire framework through three
controlled experiments. The results of our evaluations show that our ontology,
language, and method are adequate in capturing requirements in practice, and
offer strong evidence that with sufficient training, our framework indeed helps
people conduct more effective requirements engineering.
",1,6
141,"  Consumers of Internet content typically pay an Internet Service Provider
(ISP) to connect to the Internet. A content provider (CP) may charge consumers
for its content or may earn via advertising revenue. In such settings, a matter
of continuing debate, under the umbrella of net neutrality regulations, is
whether an ISP serving a consumer may in addition charge the CPs not directly
connected to the ISP for delivering their content to consumers connected to the
ISP. We attempt an answer by looking at the problem through the lens of a
regulator whose mandate is to maximize the cumulative welfare of ISPs, CPs, and
consumers.
  Specifically, we consider a two-sided market model, in which a local monopoly
ISP prices Internet access to consumers and possibly to CPs as well. The CPs
then decide whether to enter a competitive but differentiated market and the
consumers decide whether to connect to the ISP. Unlike prior works, we model
competition between the CPs together with consumer valuation of content and
quality-of-service provided by the ISP. We do so by using a novel fusion of
classical spatial differentiation models, namely the Hotelling and the Salop
models, in addition to simple queue theoretic delay modeling. Via extensive
simulations, we show that the equilibrium in the non-neutral setting that
allows an ISP to charge a CP welfare-dominates the neutral equilibrium.
",1,6
142,"  The millimeter wave (mmWave) frequencies offer the availability of huge
bandwidths to provide unprecedented data rates to next-generation cellular
mobile terminals. However, mmWave links are highly susceptible to rapid channel
variations and suffer from severe free-space pathloss and atmospheric
absorption. To address these challenges, the base stations and the mobile
terminals will use highly directional antennas to achieve sufficient link
budget in wide area networks. The consequence is the need for precise alignment
of the transmitter and the receiver beams, an operation which may increase the
latency of establishing a link, and has important implications for control
layer procedures, such as initial access, handover and beam tracking. This
tutorial provides an overview of recently proposed measurement techniques for
beam and mobility management in mmWave cellular networks, and gives insights
into the design of accurate, reactive and robust control schemes suitable for a
3GPP NR cellular network. We will illustrate that the best strategy depends on
the specific environment in which the nodes are deployed, and give guidelines
to inform the optimal choice as a function of the system parameters.
",1,6
143,"  We survey the prospects for an Information Dynamics which can serve as the
basis for a fundamental theory of information, incorporating qualitative and
structural as well as quantitative aspects. We motivate our discussion with
some basic conceptual puzzles: how can information increase in computation, and
what is it that we are actually computing in general? Then we survey a number
of the theories which have been developed within Computer Science, as partial
exemplifications of the kind of fundamental theory which we seek: including
Domain Theory, Dynamic Logic, and Process Algebra. We look at recent work
showing new ways of combining quantitative and qualitative theories of
information, as embodied respectively by Domain Theory and Shannon Information
Theory. Then we look at Game Semantics and Geometry of Interaction, as examples
of dynamic models of logic and computation in which information flow and
interaction are made central and explicit. We conclude by looking briefly at
some key issues for future progress.
",1,6
144,"  Learning causal and temporal relationships between events is an important
step towards deeper story and commonsense understanding. Though there are
abundant datasets annotated with event relations for story comprehension, many
have no empirical results associated with them. In this work, we establish
strong baselines for event temporal relation extraction on two under-explored
story narrative datasets: Richer Event Description (RED) and Causal and
Temporal Relation Scheme (CaTeRS). To the best of our knowledge, these are the
first results reported on these two datasets. We demonstrate that neural
network-based models can outperform some strong traditional linguistic
feature-based models. We also conduct comparative studies to show the
contribution of adopting contextualized word embeddings (BERT) for event
temporal relation extraction from stories. Detailed analyses are offered to
better understand the results.
",5,2
145,"  Finding names of people killed by police has become increasingly important as
police shootings get more and more public attention (police killing detection).
Unfortunately, there has been not much work in the literature addressing this
problem. The early work in this field \cite{keith2017identifying} proposed a
distant supervision framework based on Expectation Maximization (EM) to deal
with the multiple appearances of the names in documents. However, such EM-based
framework cannot take full advantages of deep learning models, necessitating
the use of hand-designed features to improve the detection performance. In this
work, we present a novel deep learning method to solve the problem of police
killing recognition. The proposed method relies on hierarchical LSTMs to model
the multiple sentences that contain the person names of interests, and
introduce supervised attention mechanisms based on semantical word lists and
dependency trees to upweight the important contextual words. Our experiments
demonstrate the benefits of the proposed model and yield the state-of-the-art
performance for police killing detection.
",0,4
146,"  We present an efficient algorithm to find non-empty minimizers of a symmetric
submodular function over any family of sets closed under inclusion. This for
example includes families defined by a cardinality constraint, a knapsack
constraint, a matroid independence constraint, or any combination of such
constraints. Our algorithm make $O(n^3)$ oracle calls to the submodular
function where $n$ is the cardinality of the ground set. In contrast, the
problem of minimizing a general submodular function under a cardinality
constraint is known to be inapproximable within $o(\sqrt{n/\log n})$ (Svitkina
and Fleischer [2008]).
  The algorithm is similar to an algorithm of Nagamochi and Ibaraki [1998] to
find all nontrivial inclusionwise minimal minimizers of a symmetric submodular
function over a set of cardinality $n$ using $O(n^3)$ oracle calls. Their
procedure in turn is based on Queyranne's algorithm [1998] to minimize a
symmetric submodular
",2,3
147,"  We study the complexity of the Channel Assignment problem. By applying the
meet-in-the-middle approach we get an algorithm for the $\ell$-bounded Channel
Assignment (when the edge weights are bounded by $\ell$) running in time
$O^*((2\sqrt{\ell+1})^n)$. This is the first algorithm which breaks the
$(O(\ell))^n$ barrier. We extend this algorithm to the counting variant, at the
cost of slightly higher polynomial factor.
  A major open problem asks whether Channel Assignment admits a $O(c^n)$-time
algorithm, for a constant $c$ independent of $\ell$. We consider a similar
question for Generalized T-Coloring, a CSP problem that generalizes \CA. We
show that Generalized T-Coloring does not admit a
$2^{2^{o\left(\sqrt{n}\right)}} {\rm poly}(r)$-time algorithm, where $r$ is the
size of the instance.
",6,1
148,"  In this work, we describe a system that detects paraphrases in Indian
Languages as part of our participation in the shared Task on detecting
paraphrases in Indian Languages (DPIL) organized by Forum for Information
Retrieval Evaluation (FIRE) in 2016. Our paraphrase detection method uses a
multinomial logistic regression model trained with a variety of features which
are basically lexical and semantic level similarities between two sentences in
a pair. The performance of the system has been evaluated against the test set
released for the FIRE 2016 shared task on DPIL. Our system achieves the highest
f-measure of 0.95 on task1 in Punjabi language.The performance of our system on
task1 in Hindi language is f-measure of 0.90. Out of 11 teams participated in
the shared task, only four teams participated in all four languages, Hindi,
Punjabi, Malayalam and Tamil, but the remaining 7 teams participated in one of
the four languages. We also participated in task1 and task2 both for all four
Indian Languages. The overall average performance of our system including task1
and task2 overall four languages is F1-score of 0.81 which is the second
highest score among the four systems that participated in all four languages.
",6,4
149,"  Cloud service providers are often trusted to be genuine, the damage caused by
being discovered to be attacking their own customers outweighs any benefits
such attacks could reap. On the other hand, it is expected that some cloud
service users may be actively malicious. In such an open system, each location
may run code which has been developed independently of other locations (and
which may be secret). In this paper, we present a typed language which ensures
that the access restrictions put on data on a particular device will be
observed by all other devices running typed code. Untyped, compromised devices
can still interact with typed devices without being able to violate the
policies, except in the case when a policy directly places trust in untyped
locations. Importantly, our type system does not need a middleware layer or all
users to register with a preexisting PKI, and it allows for devices to
dynamically create new identities. The confidentiality property guaranteed by
the language is defined for any kind of intruder: we consider labeled
bisimilarity i.e. an attacker cannot distinguish two scenarios that differ by
the change of a protected value. This shows our main result that, for a device
that runs well typed code and only places trust in other well typed devices,
programming errors cannot cause a data leakage.
",3,4
150,"  In this paper, we consider the question of computing sparse subgraphs for any
input directed graph $G=(V,E)$ on $n$ vertices and $m$ edges, that preserves
reachability and/or strong connectivity structures.
  We show $O(n+\min\{|{\cal P}|\sqrt{n},n\sqrt{|{\cal P}|}\})$ bound on a
subgraph that is an $1$-fault-tolerant reachability preserver for a given
vertex-pair set ${\cal P}\subseteq V\times V$, i.e., it preserves reachability
between any pair of vertices in ${\cal P}$ under single edge (or vertex)
failure. Our result is a significant improvement over the previous best $O(n
|{\cal P}|)$ bound obtained as a corollary of single-source reachability
preserver construction. We prove our upper bound by exploiting the special
structure of single fault-tolerant reachability preserver for any pair, and
then considering the interaction among such structures for different pairs.
  In the lower bound side, we show that a 2-fault-tolerant reachability
preserver for a vertex-pair set ${\cal P}\subseteq V\times V$ of size
$\Omega(n^\epsilon)$, for even any arbitrarily small $\epsilon$, requires at
least $\Omega(n^{1+\epsilon/8})$ edges. This refutes the existence of
linear-sized dual fault-tolerant preservers for reachability for any polynomial
sized vertex-pair set.
  We also present the first sub-quadratic bound of at most $\tilde{O}(k 2^k
n^{2-1/k})$ size, for strong-connectivity preservers of directed graphs under
$k$ failures. To the best of our knowledge no non-trivial bound for this
problem was known before, for a general $k$. We get our result by adopting the
color-coding technique of Alon, Yuster, and Zwick [JACM'95].
",3,2
151,"  A biform theory is a combination of an axiomatic theory and an algorithmic
theory that supports the integration of reasoning and computation. These are
ideal for specifying and reasoning about algorithms that manipulate
mathematical expressions. However, formalizing biform theories is challenging
since it requires the means to express statements about the interplay of what
these algorithms do and what their actions mean mathematically. This paper
describes a project to develop a methodology for expressing, manipulating,
managing, and generating mathematical knowledge as a network of biform
theories. It is a subproject of MathScheme, a long-term project at McMaster
University to produce a framework for integrating formal deduction and symbolic
computation.
",4,3
152,"  In the Prophet Secretary problem, samples from a known set of probability
distributions arrive one by one in a uniformly random order, and an algorithm
must irrevocably pick one of the samples as soon as it arrives. The goal is to
maximize the expected value of the sample picked relative to the expected
maximum of the distributions. This is one of the most simple and fundamental
problems in online decision making that models the process selling one item to
a sequence of costumers. For a closely related problem called the Prophet
Inequality where the order of the random variables is adversarial, it is known
that one can achieve in expectation $1/2$ of the expected maximum, and no
better ratio is possible. For the Prophet Secretary problem, that is, when the
variables arrive in a random order, Esfandiari et al.\ (ESA 2015) showed that
one can actually get $1-1/e$ of the maximum. The $1-1/e$ bound was recently
extended to more general settings (Ehsani et al., 2017). Given these results,
one might be tempted to believe that $1-1/e$ is the correct bound. We show that
this is not the case by providing an algorithm for the Prophet Secretary
problem that beats the $1-1/e$ bound and achieves $1-1/e+1/400$ of the optimum
value. We also prove a hardness result on the performance of algorithms under a
natural restriction which we call deterministic distribution-insensitivity.
",5,2
153,"  Data layouts play a crucial role in determining the performance of a given
application running on a given architecture. Existing parallel programming
frameworks for both multicore and heterogeneous systems leave the onus of
selecting a data layout to the programmer. Therefore, shifting the burden of
data layout selection to optimizing compilers can greatly enhance programmer
productivity and application performance. In this work, we introduce {\ADHA}: a
two-level hierarchal formulation of the data layout problem for modern
heterogeneous architectures. We have created a reference implementation of ADHA
in the Heterogeneous Habanero-C (H2C) parallel programming system. ADHA shows
significant performance benefits of up to 6.92$\times$ compared to manually
specified layouts for two benchmark programs running on a CPU+GPU heterogeneous
platform.
",2,3
154,"  We present an analysis on optimizing performance of a single C++11 source
code using the Alpaka hardware abstraction library. For this we use the general
matrix multiplication (GEMM) algorithm in order to show that compilers can
optimize Alpaka code effectively when tuning key parameters of the algorithm.
We do not intend to rival existing, highly optimized DGEMM versions, but merely
choose this example to prove that Alpaka allows for platform-specific tuning
with a single source code. In addition we analyze the optimization potential
available with vendor-specific compilers when confronted with the heavily
templated abstractions of Alpaka. We specifically test the code for bleeding
edge architectures such as Nvidia's Tesla P100, Intel's Knights Landing (KNL)
and Haswell architecture as well as IBM's Power8 system. On some of these we
are able to reach almost 50\% of the peak floating point operation performance
using the aforementioned means. When adding compiler-specific #pragmas we are
able to reach 5 TFLOPS/s on a P100 and over 1 TFLOPS/s on a KNL system.
",2,6
155,"  In this paper, we present analytical study of routing overhead of reactive
routing protocols for Wireless Multihop Networks (WMhNs). To accomplish the
framework of generalized routing overhead, we choose Ad-Hoc on Demand Distance
Vector (AODV), Dynamic Source Routing (DSR) and Dynamic MANET on Demand (DYMO).
Considering basic themes of these protocols, we enhance the generalized network
models by adding route monitoring overhead. Later, we take different network
parameters and produce framework discussing the impact of variations of these
parameters in network and routing performance. In the second part of our work,
we simulate above mentioned routing protocols and give a brief discussion and
comparison about the environments where these routing protocols perform better.
",4,2
156,"  Our team, NRC-Canada, participated in two shared tasks at the AMIA-2017
Workshop on Social Media Mining for Health Applications (SMM4H): Task 1 -
classification of tweets mentioning adverse drug reactions, and Task 2 -
classification of tweets describing personal medication intake. For both tasks,
we trained Support Vector Machine classifiers using a variety of surface-form,
sentiment, and domain-specific features. With nine teams participating in each
task, our submissions ranked first on Task 1 and third on Task 2. Handling
considerable class imbalance proved crucial for Task 1. We applied an
under-sampling technique to reduce class imbalance (from about 1:10 to 1:2).
Standard n-gram features, n-grams generalized over domain terms, as well as
general-domain and domain-specific word embeddings had a substantial impact on
the overall performance in both tasks. On the other hand, including sentiment
lexicon features did not result in any improvement.
",6,4
157,"  For the vast majority of local graph problems standard dynamic programming
techniques give c^tw V^O(1) algorithms, where tw is the treewidth of the input
graph. On the other hand, for problems with a global requirement (usually
connectivity) the best-known algorithms were naive dynamic programming schemes
running in tw^O(tw) V^O(1) time.
  We breach this gap by introducing a technique we dubbed Cut&Count that allows
to produce c^tw V^O(1) Monte Carlo algorithms for most connectivity-type
problems, including Hamiltonian Path, Feedback Vertex Set and Connected
Dominating Set, consequently answering the question raised by Lokshtanov, Marx
and Saurabh [SODA'11] in a surprising way. We also show that (under reasonable
complexity assumptions) the gap cannot be breached for some problems for which
Cut&Count does not work, like CYCLE PACKING.
  The constant c we obtain is in all cases small (at most 4 for undirected
problems and at most 6 for directed ones), and in several cases we are able to
show that improving those constants would cause the Strong Exponential Time
Hypothesis to fail.
  Our results have numerous consequences in various fields, like FPT
algorithms, exact and approximate algorithms on planar and H-minor-free graphs
and algorithms on graphs of bounded degree. In all these fields we are able to
improve the best-known results for some problems.
",2,3
158,"  Many fundamental NP-hard problems can be formulated as integer linear
programs (ILPs). A famous algorithm by Lenstra solves ILPs in time that is
exponential only in the dimension of the program, and polynomial in the size of
the ILP. That algorithm became a ubiquitous tool in the design of
fixed-parameter algorithms for NP-hard problems, where one wishes to isolate
the hardness of a problemby some parameter. However, in many cases using
Lenstra's algorithm has two drawbacks: First, the run time of the resulting
algorithms is often doubly-exponential in the parameter, and second, an ILP
formulation in small dimension cannot easily express problems involving many
different costs.
  Inspired by the work of Hemmecke, Onn and Romanchuk [Math. Prog. 2013], we
develop a single-exponential algorithm for so-called combinatorial n-fold
integer programs, which are remarkably similar to prior ILP formulations for
various problems, but unlike them, also allow variable dimension. We then apply
our algorithm to a few representative problems like Closest String, Swap
Bribery, Weighted Set Multicover, and obtain exponential speedups in the
dependence on the respective parameters, the input size, or both.
  Unlike Lenstra's algorithm, which is essentially a bounded search tree
algorithm, our result uses the technique of augmenting steps. At its heart is a
deep result stating that in combinatorial n-fold IPs, existence of an
augmenting step implies existence of a \local"" augmenting step, which can be
found using dynamic programming. Our results provide an important insight into
many problems by showing that they exhibit this phenomenon, and highlights the
importance of augmentation techniques.
",2,5
159,"  In this paper, we propose a new Blockchain-based message and revocation
accountability system called Blackchain. Combining a distributed ledger with
existing mechanisms for security in V2X communication systems, we design a
distributed event data recorder (EDR) that satisfies traditional accountability
requirements by providing a compressed global state. Unlike previous
approaches, our distributed ledger solution provides an accountable revocation
mechanism without requiring trust in a single misbehavior authority, instead
allowing a collaborative and transparent decision making process through
Blackchain. This makes Blackchain an attractive alternative to existing
solutions for revocation in a Security Credential Management System (SCMS),
which suffer from the traditional disadvantages of PKIs, notably including
centralized trust. Our proposal becomes scalable through the use of
hierarchical consensus: individual vehicles dynamically create clusters, which
then provide their consensus decisions as input for road-side units (RSUs),
which in turn publish their results to misbehavior authorities. This authority,
which is traditionally a single entity in the SCMS, responsible for the
integrity of the entire V2X network, is now a set of authorities that
transparently perform a revocation, whose result is then published in a global
Blackchain state. This state can be used to prevent the issuance of
certificates to previously malicious users, and also prevents the authority
from misbehaving through the transparency implied by a global system state.
",2,6
160,"  Mobile app development involves a unique set of challenges including device
fragmentation and rapidly evolving platforms, making testing a difficult task.
The design space for a comprehensive mobile testing strategy includes features,
inputs, potential contextual app states, and large combinations of devices and
underlying platforms. Therefore, automated testing is an essential activity of
the development process. However, current state of the art of automated testing
tools for mobile apps poses limitations that has driven a preference for manual
testing in practice. As of today, there is no comprehensive automated solution
for mobile testing that overcomes fundamental issues such as automated oracles,
history awareness in test cases, or automated evolution of test cases.
  In this perspective paper we survey the current state of the art in terms of
the frameworks, tools, and services available to developers to aid in mobile
testing, highlighting present shortcomings. Next, we provide commentary on
current key challenges that restrict the possibility of a comprehensive,
effective, and practical automated testing solution. Finally, we offer our
vision of a comprehensive mobile app testing framework, complete with research
agenda, that is succinctly summarized along three principles: Continuous,
Evolutionary and Large-scale (CEL).
",1,6
161,"  This article provides an interesting exploration of character-level
convolutional neural network solving Chinese corpus text classification
problem. We constructed a large-scale Chinese language dataset, and the result
shows that character-level convolutional neural network works better on Chinese
corpus than its corresponding pinyin format dataset. This is the first time
that character-level convolutional neural network applied to text
classification problem.
",4,2
162,"  Data centers traffic is composed by numerous latency-sensitive ""mice"" flows,
which is consisted of only several packets, and a few throughput-sensitive
""elephant"" flows, which occupy more than 80% of overall load. Generally, the
short-lived ""mice"" flows induce transient congestion and the long-lived
""elephant"" flows cause persistent congestion. The network congestion is a major
performance inhibitor. Conventionally, the hop-by-hop and end-to-end flow
control mechanisms are employed to relief transient and persistent congestion,
respectively. However, in face of the mixture of elephants and mice, we find
the hybrid congestion control scheme including hop-by-hop and end-to-end flow
control mechanisms suffers from serious performance impairments. As a step
further, our in-depth analysis reveals that the hybrid scheme performs poor at
latency of mice and throughput of elephant. Motivated by this understanding, we
argue for isolating mice and elephants in different queues, such that the
hop-by-hop and end-to-end flow control mechanisms are independently imposed to
short-lived and long-lived flows, respectively. Our solution is
readily-deployable and compatible with current commodity network devices and
can leverage various congestion control mechanisms. Extensive simulations show
that our proposal of isolation can simultaneously improve the latency of mice
by at least 30% and the link utilization to almost 100%.
",2,6
163,"  Nowadays, the size of the Internet is experiencing rapid growth. As of
December 2014, the number of global Internet websites has more than 1 billion
and all kinds of information resources are integrated together on the Internet,
however,the search engine is to be a necessary tool for all users to retrieve
useful information from vast amounts of web data. Generally speaking, a
complete search engine includes the crawler system, index building systems,
sorting systems and retrieval system. At present there are many open source
implementation of search engine, such as lucene, solr, katta, elasticsearch,
solandra and so on. The crawler system and sorting system is indispensable for
any kind of search engine and in order to guarantee its efficiency, the former
needs to update crawled vast amounts of data and the latter requires real-time
to build index on newly crawled web pages and calculae its corresponding
PageRank value. It is unlikely to accomplish such huge computation tasks
depending on a single hardware implementation of the crawler system and sorting
system,from which aspect, the distributed cluster technology is brought to the
front. In this paper, we use the hadoop Map - Reduce computing framework to
implement a distributed crawler system, and use the GraphLite, a distributed
synchronous graph-computing framework, to achieve the real-time computation in
getting the PageRank value of the new crawled web page.
",2,5
164,"  We present an algorithm for computing $F_p$, the $p$th moment of an
$n$-dimensional frequency vector of a data stream, for $2 < p < \log (n) $, to
within $1\pm \epsilon$ factors, $\epsilon \in [n^{-1/p},1]$ with high constant
probability. Let $m$ be the number of stream records and $M$ be the largest
magnitude of a stream update.
  The algorithm uses space in bits $$ O(p^2\epsilon^{-2}n^{1-2/p}E(p,n) \log
(n) \log (nmM)/\min(\log (n),\epsilon^{4/p-2}))$$ where, $E(p,n) =
(1-2/p)^{-1}(1-n^{-4(1-2/p})$. Here $E(p,n)$ is $ O(1)$ for $p = 2+\Omega(1)$
and $ O(\log n)$ for $p = 2 + O(1/\log (n)$. This improves upon the space
required by current algorithms
\cite{iw:stoc05,bgks:soda06,ako:arxiv10,bo:arxiv10} by a factor of at least
$\Omega(\epsilon^{-4/p} \min(\log (n), \epsilon^{4/p-2}))$. The update time is
$O(\log (n))$. We use a new technique for designing estimators for functions of
the form $\psi(\expect{X})$, where, $X$ is a random variable and $\psi$ is a
smooth function, based on a low-degree Taylor polynomial expansion of
$\psi(\expect{X})$ around an estimate of $\expect{X}$.
",2,6
165,"  We present a novel approach to finding the $k$-sink on dynamic path networks
with general edge capacities. Our first algorithm runs in $O(n \log n + k^2
\log^4 n)$ time, where $n$ is the number of vertices on the given path, and our
second algorithm runs in $O(n \log^3 n)$ time. Together, they improve upon the
previously most efficient $O(kn \log^2 n)$ time algorithm due to Arumugam et
al. for all values of $k$. In the case where all the edges have the same
capacity, we again present two algorithms that run in $O(n + k^2 \log^2n)$ time
and $O(n \log n)$ time, respectively, and they together improve upon the
previously best $O(kn)$ time algorithm due to Higashikawa et al. for all values
of $k$.
",2,1
166,"  Future 5G services are characterised by unprecedented need for high rate,
ubiquitous availability, ultra-low latency and high reliability. The fragmented
network view that is widespread in current networks will not stand the
challenge posed by next generations of users. A new vision is required, and
this paper provides an insight on how network convergence and
application-centric approaches will play a leading role towards enabling the 5G
vision. The paper, after expressing the view on the need for an end-to-end
approach to network design, brings the reader into a journey on the expected 5G
network requirements and outlines some of the work currently carried out by
main standardisation bodies. It then proposes the use of the concept of network
convergence for providing the overall architectural framework to bring together
all the different technologies within a unifying and coherent network
ecosystem. The novel interpretation of multi-dimensional convergence we
introduce leads us to the exploration of aspects of node consolidation and
converged network architectures, delving into details of optical-wireless
integration and future convergence of optical data centre and access-metro
networks. We then discuss how ownership models enabling network sharing will be
instrumental in realising the 5G vision. The paper concludes with final remarks
on the role SDN will play in 5G and on the need for new business models that
reflect the application-centric view of the network. Finally, we provide some
insight on growing research areas in 5G networking.
",4,2
167,"  This article formalizes an abstraction of input/output relations, based on
parameterized zonotopes, which we call affine sets. We describe the abstract
transfer functions and prove their correctness, which allows the generation of
accurate numerical invariants. Other applications range from compositional
reasoning to proofs of user-defined complex invariants and test case
generation.
",2,5
168,"  Multipath routing is the use of multiple potential paths through a network in
order to enhance fault tolerance, optimize bandwidth use, and improve security.
Selecting data flow paths based on cost addresses performance issues but
ignores security threats. Attackers can disrupt the data flows by attacking the
links along the paths. Denial-of-service, remote exploitation, and other such
attacks launched on any single link can severely limit throughput. Networks can
be secured using a secure quality of service approach in which a sender
disperses data along multiple secure paths. In this secure multi-path approach,
a portion of the data from the sender is transmitted over each path and the
receiver assembles the data fragments that arrive. One of the largest
challenges in secure multipath routing is determining the security threat level
along each path and providing a commensurate level of encryption along that
path. The research presented explores the effects of real-world attack
scenarios in systems, and gauges the threat levels along each path. Optimal
sampling and compression of network data is provided via compressed sensing.
The probability of the presence of specific attack signatures along a network
path is determined using machine learning techniques. Using these
probabilities, information assurance levels are derived such that security
measures along vulnerable paths are increased.
",6,1
169,"  Clustering a lexicon of words is a well-studied problem in natural language
processing (NLP). Word clusters are used to deal with sparse data in
statistical language processing, as well as features for solving various NLP
tasks (text categorization, question answering, named entity recognition and
others).
  Spectral clustering is a widely used technique in the field of image
processing and speech recognition. However, it has scarcely been explored in
the context of NLP; specifically, the method used in this (Meila and Shi, 2001)
has never been used to cluster a general word lexicon.
  We apply spectral clustering to a lexicon of words, evaluating the resulting
clusters by using them as features for solving two classical NLP tasks:
semantic role labeling and dependency parsing. We compare performance with
Brown clustering, a widely-used technique for word clustering, as well as with
other clustering methods. We show that spectral clusters produce similar
results to Brown clusters, and outperform other clustering methods. In
addition, we quantify the overlap between spectral and Brown clusters, showing
that each model captures some information which is uncaptured by the other.
",2,5
170,"  Conventional cellular wireless networks were designed with the purpose of
providing high throughput for the user and high capacity for the service
provider, without any provisions of energy efficiency. As a result, these
networks have an enormous Carbon footprint. In this note, we describe the
sources of the inefficiencies in such networks. First we quantify how much
Carbon footprint such networks generate. We also discuss how much more mobile
traffic is expected to increase so that this Carbon footprint will even
increase tremendously more. We then discuss specific sources of inefficiency
and potential sources of improvement at the physical layer as well as higher
layers of the communication protocol hierarchy. In particular, considering that
most of the energy inefficiency in wireless cellular networks is at the base
stations, we discuss multi-tier networks and point to the potential of
exploiting mobility patterns in order to use base station energy judiciously.
",0,4
171,"  Electronic information is increasingly often shared among entities without
complete mutual trust. To address related security and privacy issues, a few
cryptographic techniques have emerged that support privacy-preserving
information sharing and retrieval. One interesting open problem in this context
involves two parties that need to assess the similarity of their datasets, but
are reluctant to disclose their actual content. This paper presents an
efficient and provably-secure construction supporting the privacy-preserving
evaluation of sample set similarity, where similarity is measured as the
Jaccard index. We present two protocols: the first securely computes the
(Jaccard) similarity of two sets, and the second approximates it, using MinHash
techniques, with lower complexities. We show that our novel protocols are
attractive in many compelling applications, including document/multimedia
similarity, biometric authentication, and genetic tests. In the process, we
demonstrate that our constructions are appreciably more efficient than prior
work.
",5,6
172,"  Tying the weights of the target word embeddings with the target word
classifiers of neural machine translation models leads to faster training and
often to better translation quality. Given the success of this parameter
sharing, we investigate other forms of sharing in between no sharing and hard
equality of parameters. In particular, we propose a structure-aware output
layer which captures the semantic structure of the output space of words within
a joint input-output embedding. The model is a generalized form of weight tying
which shares parameters but allows learning a more flexible relationship with
input word embeddings and allows the effective capacity of the output layer to
be controlled. In addition, the model shares weights across output classifiers
and translation contexts which allows it to better leverage prior knowledge
about them. Our evaluation on English-to-Finnish and English-to-German datasets
shows the effectiveness of the method against strong encoder-decoder baselines
trained with or without weight tying.
",2,6
173,"  Given an arbitrary bitstream, we consider the problem of finding the longest
substring whose ratio of ones to zeroes equals a given value. The central
result of this paper is an algorithm that solves this problem in linear time.
The method involves (i) reformulating the problem as a constrained walk through
a sparse matrix, and then (ii) developing a data structure for this sparse
matrix that allows us to perform each step of the walk in amortised constant
time. We also give a linear time algorithm to find the longest substring whose
ratio of ones to zeroes is bounded below by a given value. Both problems have
practical relevance to cryptography and bioinformatics.
",6,2
174,"  This paper presents an approach to model features and function nets of
automotive systems comprehensively. In order to bridge the gap between feature
requirements and function nets, we describe an approach to describe both using
a SysML-based notation. If requirements on the automotive system are changed by
several developers responsible for different features, it is important for
developers to have a good overview and understanding of the functions affected.
We show that this can be comprehensively modeled using so called ""feature
views"". In order to validate these views against the complete function nets,
consistency checks are provided.
",2,1
175,"  We develop a timeout based extension of propositional linear temporal logic
(which we call TLTL) to specify timing properties of timeout based models of
real time systems. TLTL formulas explicitly refer to a running global clock
together with static timing variables as well as a dynamic variable abstracting
the timeout behavior. We extend LTL with the capability to express timeout
constraints. From the expressiveness view point, TLTL is not comparable with
important known clock based real-time logics including TPTL, XCTL, and MTL,
i.e., TLTL can specify certain properties, which cannot be specified in these
logics (also vice-versa). We define a corresponding timeout tableau for
satisfiability checking of the TLTL formulas. Also a model checking algorithm
over timeout Kripke structure is presented. Further we prove that the validity
checking for such an extended logic remains PSPACE-complete even in the
presence of timeout constraints and infinite state models. Under discrete time
semantics, with bounded timeout increments, the model-checking problem that if
a TLTL-formula holds in a timeout Kripke structure is also PSPACE complete. We
further prove that when TLTL is interpreted over discrete time, it can be
embedded in the monadic second order logic with time, and when TLTL is
interpreted over dense time without the condition of non-zenoness, the
resulting logic becomes $\Sigma_1^1$-complete.
",2,5
176,"  We consider the problem of generating relevant execution traces to test rich
interactive applications. Rich interactive applications, such as apps on mobile
platforms, are complex stateful and often distributed systems where
sufficiently exercising the app with user-interaction (UI) event sequences to
expose defects is both hard and time-consuming. In particular, there is a
fundamental tension between brute-force random UI exercising tools, which are
fully-automated but offer low relevance, and UI test scripts, which are manual
but offer high relevance. In this paper, we consider a middle way---enabling a
seamless fusion of scripted and randomized UI testing. This fusion is
prototyped in a testing tool called ChimpCheck for programming, generating, and
executing property-based randomized test cases for Android apps. Our approach
realizes this fusion by offering a high-level, embedded domain-specific
language for defining custom generators of simulated user-interaction event
sequences. What follows is a combinator library built on industrial strength
frameworks for property-based testing (ScalaCheck) and Android testing (Android
JUnit and Espresso) to implement property-based randomized testing for Android
development. Driven by real, reported issues in open source Android apps, we
show, through case studies, how ChimpCheck enables expressing effective testing
patterns in a compact manner.
",1,6
177,"  Motivated by alleviating CO$_2$ pollution, Electric Vehicle (EV) based
applications have recently received wide interests from both commercial and
research communities by using electric energy instead of traditional fuel
energy. Although EVs are inherently with limited travelling distance, such
limitation could be overcome by deploying public Charging Stations (CSs) to
recharge EVs battery during their journeys. In this paper we propose a novel
communication framework for on-the-move EV charging scenario, based on the
Publish/Subscribe (P/S) mechanism for disseminating necessary CS information to
EVs, in order for them to make optimized decisions on where to charge. A core
part of our communication framework is the utilization of Road Side Units
(RSUs) to bridge the information flow from CSs to EVs, which has been regarded
as a type of cost-efficient communication infrastructure. Under this design, we
introduce two complementary communication modes of signalling protocols, namely
Push and Pull Modes, in order to enable the required information dissemination
operation. Both analysis and simulation show the advantage of Pull Mode, in
which the information is cached at RSUs to support asynchronous communication.
We further propose a remote reservation service based on the Pull Mode, such
that the CS-selection decision making can utilize the knowledge of EVs'
charging reservation, as published from EVs through RSUs to CSs. Results show
that both the performance at CS and EV sides are further improved based on
using this anticipated information.
",5,3
178,"  Regularization of neural machine translation is still a significant problem,
especially in low-resource settings. To mollify this problem, we propose
regressing word embeddings (ReWE) as a new regularization technique in a system
that is jointly trained to predict the next word in the translation
(categorical value) and its word embedding (continuous value). Such a joint
training allows the proposed system to learn the distributional properties
represented by the word embeddings, empirically improving the generalization to
unseen sentences. Experiments over three translation datasets have showed a
consistent improvement over a strong baseline, ranging between 0.91 and 2.54
BLEU points, and also a marked improvement over a state-of-the-art system.
",2,5
179,"  Mobile (cellular) networks enable innovation, but can also stifle it and lead
to user frustration when network performance falls below expectations. As
mobile networks become the predominant method of Internet access, developer,
research, network operator, and regulatory communities have taken an increased
interest in measuring end-to-end mobile network performance to, among other
goals, minimize negative impact on application responsiveness. In this survey
we examine current approaches to end-to-end mobile network performance
measurement, diagnosis, and application prototyping. We compare available tools
and their shortcomings with respect to the needs of researchers, developers,
regulators, and the public. We intend for this survey to provide a
comprehensive view of currently active efforts and some auspicious directions
for future work in mobile network measurement and mobile application
performance evaluation.
",1,5
180,"  Edge computing is a novel paradigm designed toimprove the quality of service
for latency sensitive cloud applications. However, the state-of-the-art edge
services are designedfor specific applications, which are isolated from each
other.To better improve the utilization level of edge nodes, publicresource
sharing among edges from distinct service providersshould be encouraged
economically. In this work, we employ thepayment channel techniques to design
and implement EdgeToll,a blockchain-based toll collection system for
heterogeneous public edge sharing. Test-bed has been developed to validate
theproposal and preliminary experiments have been conducted todemonstrate the
time and cost efficiency of the system.
",6,4
181,"  QoS-aware networking applications such as real-time streaming and video
surveillance systems require nearly fixed average end-to-end delay over long
periods to communicate efficiently, although may tolerate some delay variations
in short periods. This variability exhibits complex dynamics that makes rate
control of such applications a formidable task. This paper addresses rate
allocation for heterogeneous QoS-aware applications that preserves the
long-term end-to-end delay constraint while, similar to Dynamic Network Utility
Maximization (DNUM), strives to achieve the maximum network utility aggregated
over a fixed time interval. Since capturing temporal dynamics in QoS
requirements of sources is allowed in our system model, we incorporate a novel
time-coupling constraint in which delay-sensitivity of sources is considered
such that a certain end-to-end average delay for each source over a
pre-specified time interval is satisfied. We propose DA-DNUM algorithm, as a
dual-based solution, which allocates source rates for the next time interval in
a distributed fashion, given the knowledge of network parameters in advance.
Through numerical experiments, we show that DA-DNUM gains higher average link
utilization and a wider range of feasible scenarios in comparison with the
best, to our knowledge, rate control schemes that may guarantee such
constraints on delay.
",5,1
182,"  Sentiment analysis or opinion mining aims to determine attitudes, judgments
and opinions of customers for a product or a service. This is a great system to
help manufacturers or servicers know the satisfaction level of customers about
their products or services. From that, they can have appropriate adjustments.
We use a popular machine learning method, being Support Vector Machine, combine
with the library in Waikato Environment for Knowledge Analysis (WEKA) to build
Java web program which analyzes the sentiment of English comments belongs one
in four types of woman products. That are dresses, handbags, shoes and rings.
We have developed and test our system with a training set having 300 comments
and a test set having 400 comments. The experimental results of the system
about precision, recall and F measures for positive comments are 89.3%, 95.0%
and 92,.1%; for negative comments are 97.1%, 78.5% and 86.8%; and for neutral
comments are 76.7%, 86.2% and 81.2%.
",2,1
183,"  This paper discusses the key principles of Gigabit Passive Optical Network
(GPON) which is based on Time Division Multiplexing Passive Optical Network
(TDM PON) and Wavelength Division Multiplexing Passive Optical Network (WDM
PON), which is considered to be next generation passive optical network. In the
present day scenario, access to broad- band is increasing at a rapid pace.
Because of the advantages of fiber access in terms of capacity and cost, most
of the countries have started deploying GPON access as an important part of
national strategy. Though GPON is promising, it has few limitations. On the
other hand WDM PON, a next generation network, is quite promising unlike GPON,
it is easily scalable and interoperable with different vendors. This paper
provides an overview of GPON, WDM PON and its key dissimilarities based on
technicalities and cost.
",2,3
184,"  An overview on current developments in post quantum cryptography
",6,4
185,"  Although feature models are widely used in practice, for example,
representing variability in software product lines, their integration is still
a challenge. Many integration techniques have been proposed, although none of
these have proven to be fully effective. Integrating feature models becomes a
difficult, costly, error-prone task. Since their transition occurs in a
generalized and automated way, the techniques applied to compose the models end
up giving rise to a final model, in many cases undesired, without taking into
account the specific needs arising from the requirements determined by the
analysts and developers. Therefore, this work proposes FMIT, a technique for
integrating feature models. The FMIT is based on contemporary model integration
strategies to increase the accuracy and quality of the integrated feature
model. In this way, it will be possible to identify the degree of similarity
between composite feature diagrams, to verify their accuracy, as well as to
identify conflicts. In addition, this work proposes the development of a
prototype based on the set of strategies, used to take decisions according to
the requirements established during the integration of feature models, whether
this is semi-automatic or automatic. To evaluate FMIT, experimental studies
were conducted with 10 participants, including students and professionals.
Participants performed 12 integration scenarios, 6 using the FMIT and 6
manually. The results suggest that FMIT improved accuracy by 43\% of the cases,
as well as reduced the effort by 70\% to perform the integrations.
",1,2
186,"  With the increase in mobile traffic and the bandwidth demand,
Device-to-Device (D2D) communication underlaying Long Term Evolution (LTE)
networks has gained tremendous interest by the researchers, cellular operators
and equipment manufacturers. However, the application of D2D communication has
been limited to emergency services and it needs to be explored in commercial
applications. In this paper, we have introduced a novel commercial D2D
offloading scheme for users who may be at cell edges, inside isolated
environments like basement or large buildings, etc. Our proposed scheme
discovers the available idle D2D neighbors for such poor channel users and
offloads its data to the D2D neighbor, which then relays the data to the eNB.
We have developed a D2D offloading simulation model in MATLAB, have conducted
extensive simulations and have observed that the proposed scheme can provide
better efficiency to the network as well as satisfy the poor channel users
significantly.
",4,0
187,"  Given a metric space $(F \cup C, d)$, we consider star covers of $C$ with
balanced loads. A star is a pair $(f, C_f)$ where $f \in F$ and $C_f \subseteq
C$, and the load of a star is $\sum_{c \in C_f} d(f, c)$. In minimum load
$k$-star cover problem $(\mathrm{MLkSC})$, one tries to cover the set of
clients $C$ using $k$ stars that minimize the maximum load of a star, and in
minimum size star cover $(\mathrm{MSSC})$ one aims to find the minimum number
of stars of load at most $T$ needed to cover $C$, where $T$ is a given
parameter.
  We obtain new bicriteria approximations for the two problems using novel
rounding algorithms for their standard LP relaxations. For $\mathrm{MLkSC}$, we
find a star cover with $(1+\varepsilon)k$ stars and
$O(1/\varepsilon^2)\mathrm{OPT}_{\mathrm{MLk}}$ load where
$\mathrm{OPT}_{\mathrm{MLk}}$ is the optimum load. For $\mathrm{MSSC}$, we find
a star cover with $O(1/\varepsilon^2) \mathrm{OPT}_{\mathrm{MS}}$ stars of load
at most $(2 + \varepsilon) T$ where $\mathrm{OPT}_{\mathrm{MS}}$ is the optimal
number of stars for the problem. Previously, non-trivial bicriteria
approximations were known only when $F = C$.
",6,4
188,"  Distributed information systems are needed to be autonomous, heterogeneous
and adaptable to the context. This is the reason why they resort Web services
based on SOA Based on the advanced technology of SOA. These technologies can
evolve in a dynamic environment in a well-defined context and according to
events automatically, such as time, temperature, location, authentication,
etc... This is what we call self-adaptability to context. In this paper, we are
interested in improving the different needs of this criterion and we propose a
new approach towards a self-adaptability of SOA to the context based on
workflow and showing the feasibility of this approach by integration the
workflow under a platform and test this integration by a case study.
",5,1
189,"  Intruders can infer properties of a system by measuring the time it takes for
the system to respond to some request of a given protocol, that is, by
exploiting time side channels. These properties may help intruders distinguish
whether a system is a honeypot or concrete system helping him avoid defense
mechanisms, or track a user among others violating his privacy. Observational
equivalence is the technical machinery used for verifying whether two systems
are distinguishable. Moreover, efficient symbolic methods have been developed
for automating the check of observational equivalence of systems. This paper
introduces a novel definition of timed observational equivalence which also
distinguishes systems according to their time side channels. Moreover, as our
definition uses symbolic time constraints, it can be automated by using
SMT-solvers.
",5,1
190,"  We study several extensions of linear-time and computation-tree temporal
logics with quantifiers that allow for counting how often certain properties
hold. For most of these extensions, the model-checking problem is undecidable,
but we show that decidability can be recovered by considering flat Kripke
structures where each state belongs to at most one simple loop. Most decision
procedures are based on results on (flat) counter systems where counters are
used to implement the evaluation of counting operators.
",6,4
191,"  Given a finite metric space $(V,d)$, an approximate distance oracle is a data
structure which, when queried on two points $u,v \in V$, returns an
approximation to the the actual distance between $u$ and $v$ which is within
some bounded stretch factor of the true distance. There has been significant
work on the tradeoff between the important parameters of approximate distance
oracles (and in particular between the size, stretch, and query time), but in
this paper we take a different point of view, that of per-instance
optimization. If we are given an particular input metric space and stretch
bound, can we find the smallest possible approximate distance oracle for that
particular input? Since this question is not even well-defined, we restrict our
attention to well-known classes of approximate distance oracles, and study
whether we can optimize over those classes.
  In particular, we give an $O(\log n)$-approximation to the problem of finding
the smallest stretch $3$ Thorup-Zwick distance oracle, as well as the problem
of finding the smallest P\v{a}tra\c{s}cu-Roditty distance oracle. We also prove
a matching $\Omega(\log n)$ lower bound for both problems, and an
$\Omega(n^{\frac{1}{k}-\frac{1}{2^{k-1}}})$ integrality gap for the more
general stretch $(2k-1)$ Thorup-Zwick distance oracle. We also consider the
problem of approximating the best TZ or PR approximate distance oracle
\emph{with outliers}, and show that more advanced techniques (SDP relaxations
in particular) allow us to optimize even in the presence of outliers.
",5,1
192,"  In the context of cloud computing, risks associated with underlying
technologies, risks involving service models and outsourcing, and enterprise
readiness have been recognized as potential barriers for the adoption. To
accelerate cloud adoption, the concrete barriers negatively influencing the
adoption decision need to be identified. Our study aims at understanding the
impact of technical and security-related barriers on the organizational
decision to adopt the cloud. We analyzed data collected through a web survey of
352 individuals working for enterprises consisting of decision makers as well
as employees from other levels within an organization. The comparison of
adopter and non-adopter sample reveals three potential adoption inhibitor,
security, data privacy, and portability. The result from our logistic
regression analysis confirms the criticality of the security concern, which
results in an up to 26-fold increase in the non-adoption likelihood. Our study
underlines the importance of the technical and security perspectives for
research investigating the adoption of technology.
",2,3
193,"  Existing parallel algorithms for wavelet tree construction have a work
complexity of $O(n\log\sigma)$. This paper presents parallel algorithms for the
problem with improved work complexity. Our first algorithm is based on parallel
integer sorting and has either $O(n\log\log n\lceil\log\sigma/\sqrt{\log
n\log\log n}\rceil)$ work and polylogarithmic depth, or
$O(n\lceil\log\sigma/\sqrt{\log n}\rceil)$ work and sub-linear depth. We also
describe another algorithm that has $O(n\lceil\log\sigma/\sqrt{\log n} \rceil)$
work and $O(\sigma+\log n)$ depth. We then show how to use similar ideas to
construct variants of wavelet trees (arbitrary-shaped binary trees and multiary
trees) as well as wavelet matrices in parallel with lower work complexity than
prior algorithms. Finally, we show that the rank and select structures on
binary sequences and multiary sequences, which are stored on wavelet tree
nodes, can be constructed in parallel with improved work bounds, matching those
of the best existing sequential algorithms for constructing rank and select
structures.
",4,3
194,"  During acceptance testing customers assess whether a system meets their
expectations and often identify issues that should be improved. These findings
have to be communicated to the developers a task we observed to be error prone,
especially in distributed teams. Here, it is normally not possible to have
developer representatives from every site attend the test. Developers who were
not present might misunderstand insufficiently documented findings. This
hinders fixing the issues and endangers customer satisfaction. Integrated
feedback systems promise to mitigate this problem. They allow to easily capture
findings and their context. Correctly applied, this technique could improve
feedback, while reducing customer effort. This paper collects our experiences
from comparing acceptance testing with and without feedback systems in a
distributed project. Our results indicate that this technique can improve
acceptance testing if certain requirements are met. We identify key
requirements feedback systems should meet to support acceptance testing.
",1,6
195,"  Cardinality constrained submodular function maximization, which aims to
select a subset of size at most $k$ to maximize a monotone submodular utility
function, is the key in many data mining and machine learning applications such
as data summarization and maximum coverage problems. When data is given as a
stream, streaming submodular optimization (SSO) techniques are desired.
Existing SSO techniques can only apply to insertion-only streams where each
element has an infinite lifespan, and sliding-window streams where each element
has a same lifespan (i.e., window size). However, elements in some data streams
may have arbitrary different lifespans, and this requires addressing SSO over
streams with inhomogeneous-decays (SSO-ID). This work formulates the SSO-ID
problem and presents three algorithms: BasicStreaming is a basic streaming
algorithm that achieves an $(1/2-\epsilon)$ approximation factor; HistApprox
improves the efficiency significantly and achieves an $(1/3-\epsilon)$
approximation factor; HistStreaming is a streaming version of HistApprox and
uses heuristics to further improve the efficiency. Experiments conducted on
real data demonstrate that HistStreaming can find high quality solutions and is
up to two orders of magnitude faster than the naive Greedy algorithm.
",0,3
196,"  This paper presents PUBSUB-SGX, a content-based publish-subscribe system that
exploits trusted execution environments (TEEs), such as Intel SGX, to guarantee
confidentiality and integrity of data as well as anonymity and privacy of
publishers and subscribers. We describe the technical details of our Python
implementation, as well as the required system support introduced to deploy our
system in a container-based runtime. Our evaluation results show that our
approach is sound, while at the same time highlighting the performance and
scalability trade-offs. In particular, by supporting just-in-time compilation
inside of TEEs, Python programs inside of TEEs are in general faster than when
executed natively using standard CPython.
",4,6
197,"  Targeted sentiment classification aims at determining the sentimental
tendency towards specific targets. Most of the previous approaches model
context and target words with RNN and attention. However, RNNs are difficult to
parallelize and truncated backpropagation through time brings difficulty in
remembering long-term patterns. To address this issue, this paper proposes an
Attentional Encoder Network (AEN) which eschews recurrence and employs
attention based encoders for the modeling between context and target. We raise
the label unreliability issue and introduce label smoothing regularization. We
also apply pre-trained BERT to this task and obtain new state-of-the-art
results. Experiments and analysis demonstrate the effectiveness and lightweight
of our model.
",6,2
198,"  The need for optical parallelization is driven by the imminent optical
capacity crunch, where the spectral efficiency required in the coming decades
will be beyond the Shannon limit. To this end, the emerging high-speed Ethernet
services at 100 Gbps, have already standardized options to utilize parallel
optics to parallelize interfaces referred to as Multi-lane Distribution (MLD).
OFDM-based optical network is a promising transmission option towards the goal
of Ethernet parallelization. It can allocate optical resource tailored for a
variety of bandwidth requirements and that in a fundamentally parallel fashion
with each sub-carrier utilizing a frequency slot at a lower rate than if serial
transmission was used. In this paper, we propose a novel parallel transmission
framework designed for elastic (OFDM-based) optical networks to support
high-speed Ethernet services, in-line with IEEE and ITU-T standards. We
formulate an ILP optimization model based on integer linear programming, with
consideration of various constraints, including spectrum fragmentation,
differential delay and guard-band constraints. We also propose a heuristic
algorithm which can be applied when the optimization model becomes intractable.
The numerical results show the effectiveness and high suitability of elastic
optical networks to support parallel transmission in high-speed Ethernet. To
the best of our knowledge, this is the first attempt to investigate the
parallel transmission in elastic optical networks to support standardized
high-speed Ethernet.
",4,3
199,"  Unsupervised learning has been an attractive method for easily deriving
meaningful data representations from vast amounts of unlabeled data. These
representations, or embeddings, often yield superior results in many tasks,
whether used directly or as features in subsequent training stages. However,
the quality of the embeddings is highly dependent on the assumed knowledge in
the unlabeled data and how the system extracts information without supervision.
Domain portability is also very limited in unsupervised learning, often
requiring re-training on other in-domain corpora to achieve robustness. In this
work we present a multitask paradigm for unsupervised contextual learning of
behavioral interactions which addresses unsupervised domain adaption. We
introduce an online multitask objective into unsupervised learning and show
that sentence embeddings generated through this process increases performance
of affective tasks.
",1,6
200,"  Logical frameworks are meta-formalisms in which the syntax and semantics of
object logics and related formal systems can be defined. This allows object
logics to inherit implementations from the framework including, e.g., parser,
type checker, or module system. But if the desired object logic falls outside
the comfort zone of the logical framework, these definitions may become
cumbersome or infeasible.
  Therefore, the MMT system abstracts even further than previous frameworks: it
assumes no type system or logic at all and allows its kernel algorithms to be
customized by almost arbitrary sets of rules. In particular, this allows
implementing standard logical frameworks like LF in MMT. But it does so without
chaining users to one particular meta-formalism: users can flexibly adapt MMT
whenever the object logic demands it.
  In this paper, we present a series of case studies that do just that,
defining increasingly complex object logics in MMT. We use elegant declarative
logic definitions wherever possible, but inject entirely new rules into the
kernel when necessary. Our experience shows that the MMT approach allows
deriving prototype implementations of very diverse formal systems very easily
and quickly.
",6,5
201,"  Linguists and psychologists have long been studying cross-linguistic
transfer, the influence of native language properties on linguistic performance
in a foreign language. In this work we provide empirical evidence for this
process in the form of a strong correlation between language similarities
derived from structural features in English as Second Language (ESL) texts and
equivalent similarities obtained from the typological features of the native
languages. We leverage this finding to recover native language typological
similarity structure directly from ESL text, and perform prediction of
typological features in an unsupervised fashion with respect to the target
languages. Our method achieves 72.2% accuracy on the typology prediction task,
a result that is highly competitive with equivalent methods that rely on
typological resources.
",3,2
202,"  MANET is a cooperative network in which nodes are responsible for forwarding
as well as routing. Noncooperation is still a big challenge that certainly
degrades the performance and reliability of a MANET. This paper presents a
novel methodology to overcome routing misbehavior in MANET using Retaliation
Model. In this model node misbehavior is watched and an equivalent misbehavior
is given in return. This model employs several parameters such as number of
packets forwarded, number of packets received for forwarding, packet forwarding
ratio etc. to calculate Grade and Bonus Points. The Grade is used to isolate
selfish nodes from the routing paths and the Bonus Points defines the number of
packets dropped by an honest node in retaliation over its misconducts. The
implementation is done in ""GloMoSim"" on top of the DSR protocol. We obtained up
to 40% packet delivery ratio with a cost of a minimum of 7.5% overhead compared
to DSR. To minimize total control traffic overhead we have included the FG
Model with our model and it reduces the overhead up to 75%. This model enforces
cooperation due to its stricter punishment strategy and justifies its name.
",1,3
203,"  We propose a parallel graph-based data clustering algorithm using CUDA GPU,
based on exact clustering of the minimum spanning tree in terms of a minimum
isoperimetric criteria. We also provide a comparative performance analysis of
our algorithm with other related ones which demonstrates the general
superiority of this parallel algorithm over other competing algorithms in terms
of accuracy and speed.
",3,1
204,"  We observe today a large diversity of proof systems. This diversity has the
negative consequence that a lot of theorems are proved many times. Unlike
programming languages, it is difficult for these systems to co-operate because
they do not implement the same logic. Logical frameworks are a class of theorem
provers that overcome this issue by their capacity of implementing various
logics. In this work, we study the STTforall logic, an extension of Simple Type
Theory that has been encoded in the logical framework Dedukti. We present a
translation from this logic to OpenTheory, a proof system and interoperability
tool between provers of the HOL family. We have used this translation to export
an arithmetic library containing Fermat's little theorem to OpenTheory and to
two other proof systems that are Coq and Matita.
",3,1
205,"  We study conditions for a concurrent construction of proof-nets in the
framework developed by Andreoli in recent papers. We define specific
correctness criteria for that purpose. We first study closed modules (i.e.
validity of the execution of a logic program), then extend the criterion to
open modules (i.e. validity during the execution) distinguishing criteria for
acyclicity and connectability in order to allow incremental verification.
",1,6
206,"  Research on quality issues of business process models has recently begun to
explore the process of creating process models. As a consequence, the question
arises whether different ways of creating process models exist. In this vein,
we observed 115 students engaged in the act of modeling, recording all their
interactions with the modeling environment using a specialized tool. The
recordings of process modeling were subsequently clustered. Results presented
in this paper suggest the existence of three distinct modeling styles,
exhibiting significantly different characteristics. We believe that this
finding constitutes another building block toward a more comprehensive
understanding of the process of process modeling that will ultimately enable us
to support modelers in creating better business process models.
",2,6
207,"  This paper describes the use of Naive Bayes to address the task of assigning
function tags and context free grammar (CFG) to parse Myanmar sentences. Part
of the challenge of statistical function tagging for Myanmar sentences comes
from the fact that Myanmar has free-phrase-order and a complex morphological
system. Function tagging is a pre-processing step for parsing. In the task of
function tagging, we use the functional annotated corpus and tag Myanmar
sentences with correct segmentation, POS (part-of-speech) tagging and chunking
information. We propose Myanmar grammar rules and apply context free grammar
(CFG) to find out the parse tree of function tagged Myanmar sentences.
Experiments show that our analysis achieves a good result with parsing of
simple sentences and three types of complex sentences.
",6,4
208,"  Recently, Liaw et al. proposed a remote user authentication scheme using
smart cards. Their scheme has claimed a number of features e.g. mutual
authentication, no clock synchronization, no verifier table, flexible user
password change, etc. We show that Liaw et al.'s scheme is completely insecure.
By intercepting a valid login message in Liaw et al.'s scheme, any unregistered
user or adversary can easily login to the remote system and establish a session
key.
",2,1
209,"  Data streams are a sequence of data flowing between source and destination
processes. Streaming is widely used for signal, image and video processing for
its efficiency in pipelining and effectiveness in reducing demand for memory.
The goal of this work is to extend the use of data streams to support both
conventional scientific applications and emerging data analytic applications
running on HPC platforms. We introduce an extension called MPIStream to the
de-facto programming standard on HPC, MPI. MPIStream supports data streams
either within a single application or among multiple applications. We present
three use cases using MPI streams in HPC applications together with their
parallel performance. We show the convenience of using MPI streams to support
the needs from both traditional HPC and emerging data analytics applications
running on supercomputers.
",1,5
210,"  Consider a network in which $n$ distributed nodes are connected to a single
server. Each node continuously observes a data stream consisting of one value
per discrete time step. The server has to continuously monitor a given
parameter defined over all information available at the distributed nodes. That
is, in any time step $t$, it has to compute an output based on all values
currently observed across all streams. To do so, nodes can send messages to the
server and the server can broadcast messages to the nodes. The objective is the
minimisation of communication while allowing the server to compute the desired
output.
  We consider monitoring problems related to the domain $D_t$ defined to be the
set of values observed by at least one node at time $t$. We provide randomised
algorithms for monitoring $D_t$, (approximations of) the size $|D_t|$ and the
frequencies of all members of $D_t$. Besides worst-case bounds, we also obtain
improved results when inputs are parameterised according to the similarity of
observations between consecutive time steps. This parameterisation allows to
exclude inputs with rapid and heavy changes, which usually lead to the
worst-case bounds but might be rather artificial in certain scenarios.
",4,3
211,"  Algorithms for frequent pattern mining, a popular informatics application,
have unique requirements that are not met by any of the existing parallel
tools. In particular, such applications operate on extremely large data sets
and have irregular memory access patterns. For efficient parallelization of
such applications, it is necessary to support dynamic load balancing along with
scheduling mechanisms that allow users to exploit data locality. Given these
requirements, task parallelism is the most promising of the available parallel
programming models. However, existing solutions for task parallelism schedule
tasks implicitly and hence, custom scheduling policies that can exploit data
locality cannot be easily employed. In this paper we demonstrate and
characterize the speedup obtained in a frequent pattern mining application
using a custom clustered scheduling policy in place of the popular Cilk-style
policy. We present PFunc, a novel task parallel library whose customizable task
scheduling and task priorities facilitated the implementation of our clustered
scheduling policy.
",1,6
212,"  Effectively using full syntactic parsing information in Neural Networks (NNs)
to solve relational tasks, e.g., question similarity, is still an open problem.
In this paper, we propose to inject structural representations in NNs by (i)
learning an SVM model using Tree Kernels (TKs) on relatively few pairs of
questions (few thousands) as gold standard (GS) training data is typically
scarce, (ii) predicting labels on a very large corpus of question pairs, and
(iii) pre-training NNs on such large corpus. The results on Quora and SemEval
question similarity datasets show that NNs trained with our approach can learn
more accurate models, especially after fine tuning on GS.
",5,2
213,"  Immersive computing (IC) technologies such as virtual reality and augmented
reality are gaining tremendous popularity. In this poster, we present CoIC, a
Cooperative framework for mobile Immersive Computing. The design of CoIC is
based on a key insight that IC tasks among different applications or users
might be similar or redundant. CoIC enhances the performance of mobile IC
applications by caching and sharing computation-intensive IC results on the
edge. Our preliminary evaluation results on an AR application show that CoIC
can reduce the recognition and rendering latency by up to 52.28% and 75.86%
respectively on current mobile devices.
",5,6
214,"  We present a deep generative model of bilingual sentence pairs for machine
translation. The model generates source and target sentences jointly from a
shared latent representation and is parameterised by neural networks. We
perform efficient training using amortised variational inference and
reparameterised gradients. Additionally, we discuss the statistical
implications of joint modelling and propose an efficient approximation to
maximum a posteriori decoding for fast test-time predictions. We demonstrate
the effectiveness of our model in three machine translation scenarios:
in-domain training, mixed-domain training, and learning from a mix of
gold-standard and synthetic data. Our experiments show consistently that our
joint formulation outperforms conditional modelling (i.e. standard neural
machine translation) in all such scenarios.
",6,1
215,"  We analyze priority queues including DecreaseKey method in its interface. The
paper is inspired by Strict Fibonacci Heaps [2], where G. S. Brodal, G.
Lagogiannis, and R. E. Tarjan implemented the heap with DecreaseKey and Meld
interface in assymptotically optimal worst case times (based on key
comparisons). At the end of the paper there are mentioned possible variants of
other structural properties an violations than they have used in the analysis.
In the main variant a lot of information is wasted during violation reduction
steps. Our goal is to concentrate on other variants and to invent natural
strategy not losing that much in the information value. In other words we try
to choose among them one which corresponds to superexpensive comparision
principle as much as possible. The principle was described in [5] of myself,
but after publication I have found these ideas in [4] of H. Kaplan, R. E.
Tarjan, and U. Zwick.
",5,2
216,"  Todays modern society is extremely dependent on computer based information
systems. Many of the organizations would simply not be able to function
properly without services provided by these systems, just like financing
organizations. Although interruption might decrease the efficiency of an
organization, theft or unintentional disclosure of entrusted private data could
have more serious consequences, such as legal actions as well as loss of
business due to lack of trust from potential users. This dependence on
information systems has lead to a need for securing these systems and this in
turn has created a need for knowing how secure they are. The introduction of
the information society has changed how people interact with government
agencies. Government agencies are now encouraged to uphold a 24-hour electronic
service to the citizens. The introduction of government services on the
Internet is meant to facilitate communication with agencies, decrease service
times and to lessen the amount of papers that needs to be processed. The
increased connectivity to the Internet results in a rising demand for
information security in these systems. In this article, we have discussed about
many file data breaches in the past and current history and they are going to
increase day by day as the reports by DataLossDB (Open Security Foundation)
organization, a non-profit organization in US.
",5,3
217,"  We study and develop a robust control framework for malware filtering and
network security. We investigate the malware filtering problem by capturing the
tradeoff between increased security on one hand and continued usability of the
network on the other. We analyze the problem using a linear control system
model with a quadratic cost structure and develop algorithms based on H
infinity-optimal control theory. A dynamic feedback filter is derived and shown
via numerical analysis to be an improvement over various heuristic approaches
to malware filtering. The results are verified and demonstrated with packet
level simulations on the Ns-2 network simulator.
",5,3
218,"  Falmagne recently introduced the concept of a medium, a combinatorial object
encompassing hyperplane arrangements, topological orderings, acyclic
orientations, and many other familiar structures. We find efficient solutions
for several algorithmic problems on media: finding short reset sequences,
shortest paths, testing whether a medium has a closed orientation, and listing
the states of a medium given a black-box description.
",1,3
219,"  Designing energy-efficient all-to-all multicasting protocols is of of great
importance for multi-hop wireless networks such as wireless sensor networks and
wireless ad hoc networks. In an all-to-all multicast session, there exists a
set of wireless destination nodes, and each destination node needs to send some
data packets to all other destination nodes. We consider the problem of
building a shared multicast tree spanning the destination nodes such that the
total energy consumption of realizing an all-to-all multicast session using the
shared multicast tree is minimized. Since building such a multicast tree has
been proved to be NP-complete, we provide both centralized and distributed
approximation algorithms with provable approximation ratios for it. When the
transmission power of each wireless node is fixed, our centralized and
distributed algorithms have the approximation ratios of $4ln(\Delta+1)+7$ and
13, respectively, where $\Delta$ is the maximum node degree in the network.
When the transmission power of each wireless node is adjustable, both of our
centralized and distributed algorithms have the constant approximation ratio of
145.
",2,5
220,"  The article suggests a description of a system of tables with a set of
special lists absorbing a semantics of data and reflects a fullness of data. It
shows how their parallel processing can be constructed based on the
descriptions. The approach also might be used for definition intermediate
targets for data mining and unstructured data processing.
",2,4
221,"  The Message Passing Interface (MPI) is the prevalent programming model used
on today's supercomputers. Therefore, MPI library developers are looking for
the best possible performance (shortest run-time) of individual MPI functions
across many different supercomputer architectures. Several MPI benchmark suites
have been developed to assess the performance of MPI implementations.
Unfortunately, the outcome of these benchmarks is often neither reproducible
nor statistically sound. To overcome these issues, we show which experimental
factors have an impact on the run-time of blocking collective MPI operations
and how to control them. We address the problem of process and clock
synchronization in MPI benchmarks. Finally, we present a new experimental
method that allows us to obtain reproducible and statistically sound MPI
measurements.
",6,5
222,"  In this paper we present our approach to tackle the Implicit Emotion Shared
Task (IEST) organized as part of WASSA 2018 at EMNLP 2018. Given a tweet, from
which a certain word has been removed, we are asked to predict the emotion of
the missing word. In this work, we experiment with neural Transfer Learning
(TL) methods. Our models are based on LSTM networks, augmented with a
self-attention mechanism. We use the weights of various pretrained models, for
initializing specific layers of our networks. We leverage a big collection of
unlabeled Twitter messages, for pretraining word2vec word embeddings and a set
of diverse language models. Moreover, we utilize a sentiment analysis dataset
for pretraining a model, which encodes emotion related information. The
submitted model consists of an ensemble of the aforementioned TL models. Our
team ranked 3rd out of 30 participants, achieving an F1 score of 0.703.
",1,5
223,"  We initiate the study of Access Control Encryption (ACE), a novel
cryptographic primitive that allows fine-grained access control, by giving
different rights to different users not only in terms of which messages they
are allowed to receive, but also which messages they are allowed to send.
  Classical examples of security policies for information flow are the well
known Bell-Lapadula [BL73] or Biba [Bib75] model: in a nutshell, the
Bell-Lapadula model assigns roles to every user in the system (e.g., public,
secret and top-secret). A users' role specifies which messages the user is
allowed to receive (i.e., the no read-up rule, meaning that users with public
clearance should not be able to read messages marked as secret or top-secret)
but also which messages the user is allowed to send (i.e., the no write-down
rule, meaning that a user with top-secret clearance should not be able to write
messages marked as secret or public).
  To the best of our knowledge, no existing cryptographic primitive allows for
even this simple form of access control, since no existing cryptographic
primitive enforces any restriction on what kind of messages one should be able
to encrypt.
  Our contributions are: - Introducing and formally defining access control
encryption (ACE); - A construction of ACE with complexity linear in the number
of the roles based on classic number theoretic assumptions (DDH, Paillier); - A
construction of ACE with complexity polylogarithmic in the number of roles
based on recent results on cryptographic obfuscation;
",4,3
224,"  Authentication and authorization are two key elements of a software
application. In modern day, OAuth 2.0 framework and OpenID Connect protocol are
widely adopted standards fulfilling these requirements. These protocols are
implemented into authorization servers. It is common to call these
authorization servers as identity servers or identity providers since they hold
user identity information. Applications registered to an identity provider can
use OpenID Connect to retrieve ID token for authentication. Access token
obtained along with ID token allows the application to consume OAuth 2.0
protected resources. In this approach, the client application is bound to a
single identity provider. If the client needs to consume a protected resource
from a different domain, which only accepts tokens of a defined identity
provider, then the client must again follow OpenID Connect protocol to obtain
new tokens. This requires user identity details to be stored in the second
identity provider as well. This paper proposes an extension to OpenID Connect
protocol to overcome this issue. It proposes a client-centric mechanism to
exchange identity information as token grants against a trusted identity
provider. Once a grant is accepted, resulting token response contains an access
token, which is good enough to access protected resources from token issuing
identity provider's domain.
",6,2
225,"  The Scalable Systems Laboratory (SSL), part of the IRIS-HEP Software
Institute, provides Institute participants and HEP software developers
generally with a means to transition their R&D from conceptual toys to testbeds
to production-scale prototypes. The SSL enables tooling, infrastructure, and
services supporting the innovation of novel analysis and data architectures,
development of software elements and tool-chains, reproducible functional and
scalability testing of service components, and foundational systems R&D for
accelerated services developed by the Institute. The SSL is constructed with a
core team having expertise in scale testing and deployment of services across a
wide range of cyberinfrastructure. The core team embeds and partners with other
areas in the Institute, and with LHC and other HEP development and operations
teams as appropriate, to define investigations and required service deployment
patterns. We describe the approach and experiences with early application
deployments, including analysis platforms and intelligent data delivery
systems.
",6,1
226,"  We show that a special case of the Feferman-Vaught composition theorem gives
rise to a natural notion of automata for finite words over an infinite
alphabet, with good closure and decidability properties, as well as several
logical characterizations. We also consider a slight extension of the
Feferman-Vaught formalism which allows to express more relations between
component values (such as equality), and prove related decidability results.
  From this result we get new classes of decidable logics for words over an
infinite alphabet.
",5,1
227,"  Syntax has been demonstrated highly effective in neural machine translation
(NMT). Previous NMT models integrate syntax by representing 1-best tree outputs
from a well-trained parsing system, e.g., the representative Tree-RNN and
Tree-Linearization methods, which may suffer from error propagation. In this
work, we propose a novel method to integrate source-side syntax implicitly for
NMT. The basic idea is to use the intermediate hidden representations of a
well-trained end-to-end dependency parser, which are referred to as
syntax-aware word representations (SAWRs). Then, we simply concatenate such
SAWRs with ordinary word embeddings to enhance basic NMT models. The method can
be straightforwardly integrated into the widely-used sequence-to-sequence
(Seq2Seq) NMT models. We start with a representative RNN-based Seq2Seq baseline
system, and test the effectiveness of our proposed method on two benchmark
datasets of the Chinese-English and English-Vietnamese translation tasks,
respectively. Experimental results show that the proposed approach is able to
bring significant BLEU score improvements on the two datasets compared with the
baseline, 1.74 points for Chinese-English translation and 0.80 point for
English-Vietnamese translation, respectively. In addition, the approach also
outperforms the explicit Tree-RNN and Tree-Linearization methods.
",1,5
228,"  Femtocells are recognized effective for improving network coverage and
capacity, and reducing power consumption due to the reduced range of wireless
transmissions. Although highly appealing, a plethora of challenging problems
need to be addressed for fully harvesting its potential. In this paper, we
investigate the problem of cell association and service scheduling in femtocell
networks. In addition to the general goal of offloading macro base station
(MBS) traffic, we also aim to minimize the latency of service requested by
users, while considering both open and closed access strategies. We show the
cell association problem is NP-hard, and propose several near-optimal solution
algorithms for assigning users to base stations (BS), including a sequential
fixing algorithm, a rounding approximation algorithm, a greedy approximation
algorithm, and a randomized algorithm. For the service scheduling problem, we
develop an optimal algorithm to minimize the average waiting time for the users
associated with the same BS. The proposed algorithms are analyzed with respect
to performance bounds, approximation ratios, and optimality, and are evaluated
with simulations.
",4,1
229,"  With rapid advancements in sensing, networking, and computing technologies,
recent years have witnessed the emergence of cyber-physical systems (CPS) in a
broad range of application domains. CPS is a new class of engineered systems
that features the integration of computation, communications, and control. In
contrast to general-purpose computing systems, many cyber-physical applications
are safety-cricial. These applications impose considerable requirements on
quality of service (QoS) of the employed networking infrastruture. Since IEEE
802.15.4 has been widely considered as a suitable protocol for CPS over
wireless sensor and actuator networks, it is of vital importance to evaluate
its performance extensively. Serving for this purpose, this paper will analyze
the performance of IEEE 802.15.4 standard operating in different modes
respectively. Extensive simulations have been conducted to examine how network
QoS will be impacted by some critical parameters. The results are presented and
analyzed, which provide some useful insights for network parameter
configuration and optimization for CPS design.
",2,3
230,"  While one of the first steps in many NLP systems is selecting what
pre-trained word embeddings to use, we argue that such a step is better left
for neural networks to figure out by themselves. To that end, we introduce
dynamic meta-embeddings, a simple yet effective method for the supervised
learning of embedding ensembles, which leads to state-of-the-art performance
within the same model class on a variety of tasks. We subsequently show how the
technique can be used to shed new light on the usage of word embeddings in NLP
systems.
",2,5
231,"  We introduce a formal distinction between contradictions and disagreements in
natural language texts, motivated by the need to formally reason about
contradictory medical guidelines. This is a novel and potentially very useful
distinction, and has not been discussed so far in NLP and logic. We also
describe a NLP system capable of automated finding contradictory medical
guidelines; the system uses a combination of text analysis and information
retrieval modules. We also report positive evaluation results on a small corpus
of contradictory medical recommendations.
",1,0
232,"  Classical logic is embedded into constructive logic, through a definition of
the classical connectives and quantifiers in terms of the constructive ones.
",2,5
233,"  We describe a software framework for solving user equilibrium traffic
assignment problems. The design is based on the formulation of the problem as a
variational inequality. The software implements these as well as several
numerical methods for find equilirbria. We compare the solutions obtained under
several models: static, Merchant-Nemhauser, `CTM with instantaneous travel
time', and `CTM with actual travel time'. Some important differences are
demonstrated.
",6,2
234,"  We present an approximate sampling framework and discuss how risk-limiting
audits can compensate for these approximations, while maintaining their
""risk-limiting"" properties. Our framework is general and can compensate for
counting mistakes made during audits.
  Moreover, we present and analyze a simple approximate sampling
method,""$k$-cut"", for picking a ballot randomly from a stack, without counting.
Our method involves doing $k$ ""cuts"", each involving moving a random portion of
ballots from the top to the bottom of the stack, and then picking the ballot on
top. Unlike conventional methods of picking a ballot at random, $k$-cut does
not require identification numbers on the ballots or counting many ballots per
draw. We analyze how close the distribution of chosen ballots is to the uniform
distribution, and design different mitigation procedures. We show that $k=6$
cuts is enough for an risk-limiting election audit, based on empirical data,
which would provide a significant increase in efficiency.
",6,4
235,"  Seminal work by Cortadella, Kondratyev, Lavagno, and Sotiriou includes a
hand-written proof that a particular handshaking protocol preserves flow
equivalence, a notion of equivalence between synchronous latch-based
specifications and their desynchronized bundled-data asynchronous
implementations. In this work we identify a counterexample to Cortadella et
al.'s proof illustrating how their protocol can in fact lead to a violation of
flow equivalence. However, two of the less concurrent protocols identified in
their paper do preserve flow equivalence. To verify this fact, we formalize
flow equivalence in the Coq proof assistant and provide mechanized,
machine-checkable proofs of our results.
",1,3
236,"  In distributed software-defined networks (SDN), multiple physical SDN
controllers, each managing a network domain, are implemented to balance
centralized control, scalability and reliability requirements. In such
networking paradigm, controllers synchronize with each other to maintain a
logically centralized network view. Despite various proposals of distributed
SDN controller architectures, most existing works only assume that such
logically centralized network view can be achieved with some synchronization
designs, but the question of how exactly controllers should synchronize with
each other to maximize the benefits of synchronization under the eventual
consistency assumptions is largely overlooked. To this end, we formulate the
controller synchronization problem as a Markov Decision Process (MDP) and apply
reinforcement learning techniques combined with deep neural network to train a
smart controller synchronization policy, which we call the Deep-Q (DQ)
Scheduler. Evaluation results show that DQ Scheduler outperforms the
antientropy algorithm implemented in the ONOS controller by up to 95.2% for
inter-domain routing tasks.
",5,1
237,"  String matching algorithms are among one of the most widely used algorithms
in computer science. Traditional string matching algorithms efficiency of
underlaying string matching algorithm will greatly increase the efficiency of
any application. In recent years, Graphics processing units are emerged as
highly parallel processor. They out perform best of the central processing
units in scientific computation power. By combining recent advancement in
graphics processing units with string matching algorithms will allows to speed
up process of string matching. In this paper we proposed modified parallel
version of Rabin-Karp algorithm using graphics processing unit. Based on that,
result of CPU as well as parallel GPU implementations are compared for
evaluating effect of varying number of threads, cores, file size as well as
pattern size.
",6,0
238,"  Natural language generation plays a critical role in spoken dialogue systems.
We present a new approach to natural language generation for task-oriented
dialogue using recurrent neural networks in an encoder-decoder framework. In
contrast to previous work, our model uses both lexicalized and delexicalized
components i.e. slot-value pairs for dialogue acts, with slots and
corresponding values aligned together. This allows our model to learn from all
available data including the slot-value pairing, rather than being restricted
to delexicalized slots. We show that this helps our model generate more natural
sentences with better grammar. We further improve our model's performance by
transferring weights learnt from a pretrained sentence auto-encoder. Human
evaluation of our best-performing model indicates that it generates sentences
which users find more appealing.
",5,6
239,"  Running off-site software middleboxes at third-party service providers has
been a popular practice. However, routing large volumes of raw traffic, which
may carry sensitive information, to a remote site for processing raises severe
security concerns. Prior solutions often abstract away important factors
pertinent to real-world deployment. In particular, they overlook the
significance of metadata protection and stateful processing. Unprotected
traffic metadata like low-level headers, size and count, can be exploited to
learn supposedly encrypted application contents. Meanwhile, tracking the states
of 100,000s of flows concurrently is often indispensable in production-level
middleboxes deployed at real networks.
  We present LightBox, the first system that can drive off-site middleboxes at
near-native speed with stateful processing and the most comprehensive
protection to date. Built upon commodity trusted hardware, Intel SGX, LightBox
is the product of our systematic investigation of how to overcome the inherent
limitations of secure enclaves using domain knowledge and customization. First,
we introduce an elegant virtual network interface that allows convenient access
to fully protected packets at line rate without leaving the enclave, as if from
the trusted source network. Second, we provide complete flow state management
for efficient stateful processing, by tailoring a set of data structures and
algorithms optimized for the highly constrained enclave space. Extensive
evaluations demonstrate that LightBox, with all security benefits, can achieve
10Gbps packet I/O, and that with case studies on three stateful middleboxes, it
can operate at near-native speed.
",1,3
240,"  The present work analyzes the redundancy of sets of combinatorial objects
produced by a weighted random generation algorithm proposed by Denise et al.
This scheme associates weights to the terminals symbols of a weighted
context-free grammar, extends this weight definition multiplicatively on words,
and draws words of length $n$ with probability proportional their weight. We
investigate the level of redundancy within a sample of $k$ word, the proportion
of the total probability covered by $k$ words (coverage), the time (number of
generations) of the first collision, and the time of the full collection. For
these four questions, we use an analytic urn analogy to derive asymptotic
estimates and/or polynomially computable exact forms. We illustrate these tools
by an analysis of an RNA secondary structure statistical sampling algorithm
introduced by Ding et al.
",5,2
241,"  Sequential allocation is a simple and widely studied mechanism to allocate
indivisible items in turns to agents according to a pre-specified picking
sequence of agents. At each turn, the current agent in the picking sequence
picks its most preferred item among all items having not been allocated yet.
This problem is well-known to be not strategyproof, i.e., an agent may get more
utility by reporting an untruthful preference ranking of items. It arises the
problem: how to find the best response of an agent?
  It is known that this problem is polynomially solvable for only two agents
and NP-complete for arbitrary number of agents.
  The computational complexity of this problem with three agents was left as an
open problem. In this paper, we give a novel algorithm that solves the problem
in polynomial time for each fixed number of agents. We also show that an agent
can always get at least half of its optimal utility by simply using its
truthful preference as the response.
",1,5
242,"  Prior work has commonly defined argument retrieval from heterogeneous
document collections as a sentence-level classification task. Consequently,
argument retrieval suffers both from low recall and from sentence segmentation
errors making it difficult for humans and machines to consume the arguments. In
this work, we argue that the task should be performed on a more fine-grained
level of sequence labeling. For this, we define the task as Argument Unit
Recognition and Classification (AURC). We present a dataset of arguments from
heterogeneous sources annotated as spans of tokens within a sentence, as well
as with a corresponding stance. We show that and how such difficult argument
annotations can be effectively collected through crowdsourcing with high
interannotator agreement. The new benchmark, AURC-8, contains up to 15% more
arguments per topic as compared to annotations on the sentence level. We
identify a number of methods targeted at AURC sequence labeling, achieving
close to human performance on known domains. Further analysis also reveals
that, contrary to previous approaches, our methods are more robust against
sentence segmentation errors. We publicly release our code and the AURC-8
dataset.
",4,1
243,"  Embedded peripheral devices such as memories, sensors and communications
interfaces are used to perform a function external to a host microcontroller.
The device manufacturer typically specifies worst-case current consumption and
latency estimates for each of these peripheral actions. Peripheral Activity
Completion, Estimation and Recognition (PACER) is introduced as a suite of
algorithms that can be applied to detect completed peripheral operations in
real-time. By detecting activity completion, PACER enables the host to exploit
slack between the worst-case estimate and the actual response time. These
methods were tested independently and in conjunction with IODVS on multiple
common peripheral devices. For the peripheral devices under test, the test
fixture confirmed decreases in energy expenditures of up to 80% and latency
reductions of up to 67%.
",1,4
244,"  While the existence of scheduler side-channels has been demonstrated recently
for fixed-priority real-time systems (RTS), there have been no similar
explorations for dynamic-priority systems. The dynamic nature of such
scheduling algorithms, e.g., EDF, poses a significant challenge in this regard.
In this paper we demonstrate that side-channels exist in dynamic priority
real-time systems. Using this side-channel, our proposed DyPS algorithm is able
to effectively infer, with high precision, critical task information from the
vantage point of an unprivileged (user space) task. Apart from demonstrating
the effectiveness of DyPS, we also explore the various factors that impact such
attack algorithms using a large number of synthetic task sets. We also compare
against the state-of-the-art and demonstrate that our proposed DyPS algorithms
outperform the ScheduLeak algorithms in attacking the EDF RTS.
",5,3
245,"  Predicate intuitionistic logic is a well established fragment of dependent
types. According to the Curry-Howard isomorphism proof construction in the
logic corresponds well to synthesis of a program the type of which is a given
formula. We present a model of automata that can handle proof construction in
full intuitionistic first-order logic. The automata are constructed in such a
way that any successful run corresponds directly to a normal proof in the
logic. This makes it possible to discuss formal languages of proofs or
programs, the closure properties of the automata and their connections with the
traditional logical connectives.
",5,3
246,"  In this paper a class of combinatorial optimization problems is discussed. It
is assumed that a feasible solution can be constructed in two stages. In the
first stage the objective function costs are known while in the second stage
they are uncertain and belong to an interval uncertainty set. In order to
choose a solution, the minmax regret criterion is used. Some general properties
of the problem are established and results for two particular problems, namely
the shortest path and the selection problem, are shown.
",2,5
247,"  Participating in demand response programs is a promising tool for reducing
energy costs in data centers by modulating energy consumption. Towards this
end, data centers can employ a rich set of resource management knobs, such as
workload shifting and dynamic server provisioning. Nonetheless, these knobs may
not be readily available in a cloud data center (CDC) that serves cloud
tenants/users, because workloads in CDCs are managed by tenants themselves who
are typically charged based on a usage-based or flat-rate pricing and often
have no incentive to cooperate with the CDC operator for demand response and
cost saving. Towards breaking such ""split incentive"" hurdle, a few recent
studies have tried market-based mechanisms, such as dynamic pricing, inside
CDCs. However, such mechanisms often rely on complex designs that are hard to
implement and difficult to cope with by tenants. To address this limitation, we
propose a novel incentive mechanism that is not dynamic, i.e., it keeps pricing
for cloud resources unchanged for a long period. While it charges tenants based
on a Usage-based Pricing (UP) as used by today's major cloud operators, it
rewards tenants proportionally based on the time length that tenants set as
deadlines for completing their workloads. This new mechanism is called
Usage-based Pricing with Monetary Reward (UPMR). We demonstrate the
effectiveness of UPMR both analytically and empirically. We show that UPMR can
reduce the CDC operator's energy cost by 12.9% while increasing its profit by
4.9%, compared to the state-of-the-art approaches used by today's CDC operators
to charge their tenants.
",2,5
248,"  We present an approach for verifying systems at runtime. Our approach targets
distributed systems whose components communicate with monitors over unreliable
channels, where messages can be delayed, reordered, or even lost. Furthermore,
our approach handles an expressive specification language that extends the
real-time logic MTL with freeze quantifiers for reasoning about data values.
The logic's main novelty is a new three-valued semantics that is well suited
for runtime verification as it accounts for partial knowledge about a system's
behavior. Based on this semantics, we present online algorithms that reason
soundly and completely about streams where events can occur out of order. We
also evaluate our algorithms experimentally. Depending on the specification,
our prototype implementation scales to out-of-order streams with hundreds to
thousands of events per second.
",1,4
249,"  There has recently been a flood of interest in potential new applications of
blockchains, as well as proposals for more generic designs called public
ledgers. Most of the novel proposals have been in the financial sector.
However, the public ledger is an abstraction that solves several of the
fundamental problems in the design of secure distributed systems: global time
in the form of a strict linear order of past events, globally consistent and
immutable view of the history, and enforcement of some application-specific
safety properties. This paper investigates the applications of public ledgers
to access control and, more specifically, to group management in distributed
systems where entities are represented by their public keys and authorization
is encoded into signed certificates. It is particularly difficult to handle
negative information, such as revocation of certificates or group membership,
in the distributed setting. The linear order of events and global consistency
simplify these problems, but the enforcement of internal constraints in the
ledger implementation often presents problems. We show that different types of
revocation require slightly different properties from the ledger. We compare
the requirements with Bitcoin, the best known blockchain, and describe an
efficient ledger design for membership revocation that combines ideas from
blockchains and from web-PKI monitoring. While we use certificate-based
group-membership management as the case study, the same ideas can be applied
more widely to rights revocation in distributed systems.
",6,2
250,"  Word Sense Disambiguation (WSD) aims to identify the correct meaning of
polysemous words in the particular context. Lexical resources like WordNet
which are proved to be of great help for WSD in the knowledge-based methods.
However, previous neural networks for WSD always rely on massive labeled data
(context), ignoring lexical resources like glosses (sense definitions). In this
paper, we integrate the context and glosses of the target word into a unified
framework in order to make full use of both labeled data and lexical knowledge.
Therefore, we propose GAS: a gloss-augmented WSD neural network which jointly
encodes the context and glosses of the target word. GAS models the semantic
relationship between the context and the gloss in an improved memory network
framework, which breaks the barriers of the previous supervised methods and
knowledge-based methods. We further extend the original gloss of word sense via
its semantic relations in WordNet to enrich the gloss information. The
experimental results show that our model outperforms the state-of-theart
systems on several English all-words WSD datasets.
",2,5
251,"  This paper proposes an application-aware multipath packet forwarding
framework that integrates Machine Learning Techniques (MLT) and Software
Defined Networks (SDN). As the Internet provides a variety of services and
their performance requirement has become heterogeneous, it is common to come
across the scenario of multiple flows competing for a constrained resource such
as bandwidth, less jitter or low latency path. Such factors are application
specific requirement that is beyond the knowledge of a simple combination of
protocol type and port number. Better overall performance could be achieved if
the network is able to prioritize the flows and assign resources based on their
application specific requirement. Our system prioritizes each of the flows
using MLT and routes it through a path according to the flow priority and
network state using SDN. The proof of concept implementation has been done on
OpenvSwitch and evaluation results involving a large number of flows exhibited
a significant improvement over the traditional network setup. We also report
that the port number and protocol are not contributing to determine the
application in the decision-making process of Machine Learning (ML).
",3,5
252,"  This application for learning APPROXIMATION ALGORITHM has been designed in
Java which will make user comfortable in learning the very complex subject
""NP-Completeness"" and the solution to NP-Complete problem using approximation
algorithm.
",6,2
253,"  A new framework to perform routing at the Autonomous System level is proposed
in this paper. This mechanism, called Chain Routing, uses complete orders as
its main topological unit. Since complete orders are acyclic digraphs that
possess a known topology, it is possible to define an acyclic structure to
route packets between a group of Autonomous Systems. The adoption of complete
orders also allows easy identification and avoidance of persistent route
oscillations, eliminates the possibility of developing transient loops in
paths, and provides a structure that facilitates the implementation of traffic
engineering. Moreover, by combining Chain Routing with other mechanisms that
implement complete orders in time, we suggest that it is possible to design a
new routing protocol which could be more reliable and stable than BGP's current
implementation. Although Chain Routing will require an increase of the message
overhead and greater coordination between network administrators, the rewards
in stability and resilience should more than compensate for this effort.
",1,5
254,"  Many natural language questions require recognizing and reasoning with
qualitative relationships (e.g., in science, economics, and medicine), but are
challenging to answer with corpus-based methods. Qualitative modeling provides
tools that support such reasoning, but the semantic parsing task of mapping
questions into those models has formidable challenges. We present QuaRel, a
dataset of diverse story questions involving qualitative relationships that
characterize these challenges, and techniques that begin to address them. The
dataset has 2771 questions relating 19 different types of quantities. For
example, ""Jenny observes that the robot vacuum cleaner moves slower on the
living room carpet than on the bedroom carpet. Which carpet has more friction?""
We contribute (1) a simple and flexible conceptual framework for representing
these kinds of questions; (2) the QuaRel dataset, including logical forms,
exemplifying the parsing challenges; and (3) two novel models for this task,
built as extensions of type-constrained semantic parsing. The first of these
models (called QuaSP+) significantly outperforms off-the-shelf tools on QuaRel.
The second (QuaSP+Zero) demonstrates zero-shot capability, i.e., the ability to
handle new qualitative relationships without requiring additional training
data, something not possible with previous models. This work thus makes inroads
into answering complex, qualitative questions that require reasoning, and
scaling to new relationships at low cost. The dataset and models are available
at http://data.allenai.org/quarel.
",0,6
255,"  Forwarding data by name has been assumed to be a necessary aspect of an
information-centric redesign of the current Internet architecture that makes
content access, dissemination, and storage more efficient. The Named Data
Networking (NDN) and Content-Centric Networking (CCNx) architectures are the
leading examples of such an approach. However, forwarding data by name incurs
storage and communication complexities that are orders of magnitude larger than
solutions based on forwarding data using addresses. Furthermore, the specific
algorithms used in NDN and CCNx have been shown to have a number of
limitations. The Addressable Data Networking (ADN) architecture is introduced
as an alternative to NDN and CCNx. ADN is particularly attractive for
large-scale deployments of the Internet of Things (IoT), because it requires
far less storage and processing in relaying nodes than NDN. ADN allows things
and data to be denoted by names, just like NDN and CCNx do. However, instead of
replacing the waist of the Internet with named-data forwarding, ADN uses an
address-based forwarding plane and introduces an information plane that
seamlessly maps names to addresses without the involvement of end-user
applications. Simulation results illustrate the order of magnitude savings in
complexity that can be attained with ADN compared to NDN.
",6,4
256,"  Current practice of shaping subscriber traffic based on token bucket by
Internet service provider (ISP) allows short-term fluctuations in its shaped
rate and thereby enables a subscriber to transmit traffic at a higher rate than
a negotiated long-term average. The traffic shaping, however, results in
significant waste of network resources, especially when there are only a few
active subscribers, because it cannot allocate excess bandwidth to active
subscribers in the long term. In this letter we investigate the long-term
aspect of resource sharing in ISP traffic control for shared access networks.
We discuss major requirements for the excess bandwidth allocation in shared
access networks and propose ISP traffic control schemes based on core-stateless
fair queueing (CSFQ) and token bucket meters. Simulation results demonstrate
that the proposed schemes allocate excess bandwidth among active subscribers in
a fair and efficient way, while not compromising the service contracts
specified by token bucket for conformant subscribers.
",3,2
257,"  A code clone is a pair of code fragments, within or between software systems
that are similar. Since code clones often negatively impact the maintainability
of a software system, several code clone detection techniques and tools have
been proposed and studied over the last decade. To detect all possible similar
source code patterns in general, the clone detection tools work on the syntax
level while lacking user-specific preferences. This often means the clones must
be manually inspected before analysis in order to remove those false positives
from consideration. This manual clone validation effort is very time-consuming
and often error-prone, in particular for large-scale clone detection. In this
paper, we propose a machine learning approach for automating the validation
process. Our machine learning-based approach is used to automatically validate
clones without human inspection. Thus the proposed approach can be used to
remove the false positive clones from the detection results, automatically
evaluate the precision of any clone detectors for any given set of datasets,
evaluate existing clone benchmark datasets, or even be used to build new clone
benchmarks and datasets with minimum effort. In an experiment with clones
detected by several clone detectors in several different software systems, we
found our approach has an accuracy of up to 87.4% when compared against the
manual validation by multiple expert judges. The proposed method also shows
better results in several comparative studies with the existing related
approaches for clone classification.
",4,1
258,"  We introduce a natural generalization of the golden cryptography, which uses
general unimodular matrices in place of the traditional Q-matrices, and prove
that it preserves the original error correction properties of the encryption.
Moreover, the additional parameters involved in generating the coding matrices
make this unimodular cryptography resilient to the chosen plaintext attacks
that worked against the golden cryptography. Finally, we show that even the
golden cryptography is generally unable to correct double errors in the same
row of the ciphertext matrix, and offer an additional check number which, if
transmitted, allows for the correction.
",2,3
259,"  To keep the competitive advantage and adapt to changes in the market and
technology, companies need to innovate in an organised, purposeful and
systematic manner. However, due to their size and complexity, large companies
tend to focus on maintaining their business, which can potentially lower their
agility to innovate. This study aims to provide an overview of the current
research on innovation initiatives and to identify the challenges of
implementing the initiatives in the context of large software companies. The
investigation was performed using a systematic mapping approach of published
literature on corporate innovation and entrepreneurship. Then it was
complemented with interviews with four experts with rich industry experience.
Our study results suggest that, there is a lack of high quality empirical
studies on innovation initiative in the context of large software companies. A
total of 7 studies are conducted in such context, which reported 5 types of
initiatives: intrapreneurship, bootlegging, internal venture, spin-off and
crowdsourcing. Our study offers three contributions. First, this paper
represents the map of existing literature on innovation initiatives inside
large companies. The second contribution is to provide an innovation initiative
tree. The third contribution is to identify key challenges faced by each
initiative in large software companies. At the strategic and tactical levels,
there is no difference between large software companies and other companies. At
the operational level, large software companies are highly influenced by the
advancement of Internet technology. Large software companies use open
innovation paradigm as part of their innovation initiatives. We envision a
future work is to further empirically evaluate the innovation initiative tree
in large software companies, which involves more practitioners from different
companies.
",1,5
260,"  The usual homogeneous form of equality type in Martin-L\""of Type Theory
contains identifications between elements of the same type. By contrast, the
heterogeneous form of equality contains identifications between elements of
possibly different types. This paper introduces a simple set of axioms for such
types. The axioms are equivalent to the combination of systematic elimination
rules for both forms of equality, albeit with typal (also known as
""propositional"") computation properties, together with Streicher's Axiom K, or
equivalently, the principle of uniqueness of identity proofs.
",1,6
261,"  The expressiveness of communication primitives has been explored in a common
framework based on the pi-calculus by considering four features: synchronism
(asynchronous vs synchronous), arity (monadic vs polyadic data), communication
medium (shared dataspaces vs channel-based), and pattern-matching (binding to a
name vs testing name equality vs intensionality). Here another dimension
coordination is considered that accounts for the number of processes required
for an interaction to occur. Coordination generalises binary languages such as
pi-calculus to joining languages that combine inputs such as the Join Calculus
and general rendezvous calculus. By means of possibility/impossibility of
encodings, this paper shows coordination is unrelated to the other features.
That is, joining languages are more expressive than binary languages, and no
combination of the other features can encode a joining language into a binary
language. Further, joining is not able to encode any of the other features
unless they could be encoded otherwise.
",1,5
262,"  Object Oriented Design methodology is an emerging software development
approach for complex systems with huge set of requirements. Unlike procedural
approach, it captures the requirements as a set of data rather than services,
encapsulated as a single entity. The success such a project relies on major
factors like design patterns framework, key principles, metric standards and
best practices adapted by the industry. The patterns are key structures for
recursive problem bits in the problem domain. The combination of design
patterns forms a framework which suits the problem statement in hand. The
pattern includes static design and dynamic behavior of different types of
entities which can be mapped as a functional diagram with cardinalities between
them. The degree of cardinality represents the coupling factor which the
industry perceives and measures for software design quality. The organization
specific design principles and rich repository of on-the-shelf patterns are the
major design-quality-influencing-factor contribute to software success. These
are the asset of an industry to deliver a quality product to sustain itself in
the competitive market.
",1,6
263,"  We report our progress in scaling deductive synthesis and repair of recursive
functional Scala programs in the Leon tool. We describe new techniques,
including a more precise mechanism for encoding the space of meaningful
candidate programs. Our techniques increase the scope of synthesis by expanding
the space of programs we can synthesize and by reducing the synthesis time in
many cases. As a new example, we present a run-length encoding function for a
list of values, which Leon can now automatically synthesize from specification
consisting of the decoding function and the local minimality property of the
encoded value.
",0,4
264,"  Language style transfer is the problem of migrating the content of a source
sentence to a target style. In many of its applications, parallel training data
are not available and source sentences to be transferred may have arbitrary and
unknown styles. First, each sentence is encoded into its content and style
latent representations. Then, by recombining the content with the target style,
we decode a sentence aligned in the target domain. To adequately constrain the
encoding and decoding functions, we couple them with two loss functions. The
first is a style discrepancy loss, enforcing that the style representation
accurately encodes the style information guided by the discrepancy between the
sentence style and the target style. The second is a cycle consistency loss,
which ensures that the transferred sentence should preserve the content of the
original sentence disentangled from its style. We validate the effectiveness of
our model in three tasks: sentiment modification of restaurant reviews, dialog
response revision with a romantic style, and sentence rewriting with a
Shakespearean style.
",3,2
265,"  Many industries rely on visual insights to support decision- making processes
in their businesses. In mining, the analysis of drills and geological shapes,
represented as 3D geometries, is an important tool to assist geologists on the
search for new ore deposits. Aeronautics manipulate high-resolution geometries
when designing a new aircraft aided by the numerical simulation of
aerodynamics. In common, these industries require scalable databases that
compute spatial relationships and measurements so that decision making can be
conducted without lags. However, as we show in this study, most database
systems either lack support for handling 3D geometries or show poor performance
when given a sheer volume of data to work with. This paper presents a pluggable
acceleration engine for spatial database systems that can improve the
performance of spatial operations by more than 3000x through GPU offloading. We
focus on the design and evaluation of our plug-in for the PostgreSQL database
system.
",1,6
266,"  Wireless Sensor networks (WSN) is an emerging technology and have great
potential to be employed in critical situations like battlefields and
commercial applications such as building, traffic surveillance, habitat
monitoring and smart homes and many more scenarios. One of the major challenges
wireless sensor networks face today is security. While the deployment of sensor
nodes in an unattended environment makes the networks vulnerable to a variety
of potential attacks, the inherent power and memory limitations of sensor nodes
makes conventional security solutions unfeasible. The sensing technology
combined with processing power and wireless communication makes it profitable
for being exploited in great quantity in future. The wireless communication
technology also acquires various types of security threats. This paper
discusses a wide variety of attacks in WSN and their classification mechanisms
and different securities available to handle them including the challenges
faced.
",1,2
267,"  In this paper we consider the Target Set Selection problem. The problem
naturally arises in many fields like economy, sociology, medicine. In the
Target Set Selection problem one is given a graph $G$ with a function
$\operatorname{thr}: V(G) \to \mathbb{N} \cup \{0\}$ and integers $k, \ell$.
The goal of the problem is to activate at most $k$ vertices initially so that
at the end of the activation process there is at least $\ell$ activated
vertices. The activation process occurs in the following way: (i) once
activated, a vertex stays activated forever; (ii) vertex $v$ becomes activated
if at least $\operatorname{thr}(v)$ of its neighbours are activated. The
problem and its different special cases were extensively studied from
approximation and parameterized points of view. For example, parameterizations
by the following parameters were studied: treewidth, feedback vertex set,
diameter, size of target set, vertex cover, cluster editing number and others.
  Despite the extensive study of the problem it is still unknown whether the
problem can be solved in $\mathcal{O}^*((2-\epsilon)^n)$ time for some
$\epsilon >0$. We partially answer this question by presenting several
faster-than-trivial algorithms that work in cases of constant thresholds,
constant dual thresholds or when the threshold value of each vertex is bounded
by one-third of its degree. Also, we show that the problem parameterized by
$\ell$ is W[1]-hard even when all thresholds are constant.
",6,5
268,"  Spin-Transfer Torque RAM (STTRAM) is promising for cache applications.
However, it brings new data security issues that were absent in volatile memory
counterparts such as Static RAM (SRAM) and embedded Dynamic RAM (eDRAM). This
is primarily due to the fundamental dependency of this memory technology on
ambient parameters such as magnetic field and temperature that can be exploited
to tamper with the stored data. In this paper we propose three techniques to
enable error free computation without stalling the system, (a) stalling where
the system is halted during attack; (b) cache bypass during gradually ramping
attack where the last level cache (LLC) is bypassed and the upper level caches
interact directly with the main memory; and, (c) checkpointing along with
bypass during sudden attack where the processor states are saved periodically
and the LLC is written back at regular intervals. During attack the system goes
back to the last checkpoint and the computation continues with bypassed cache.
We performed simulation for different duration and frequency of attack on
SPLASH benchmark suite and the results show an average of 8% degradation in IPC
for a one-time attack lasting for 50% of the execution time. The energy
overhead is 2% for an attack lasting for the entire duration of execution.
",4,0
269,"  With the development of computing technology, CUDA has become a very
important tool. In computer programming, sorting algorithm is widely used.
There are many simple sorting algorithms such as enumeration sort, bubble sort
and merge sort. In this paper, we test some simple sorting algorithm based on
CUDA and draw some useful conclusions.
",3,5
270,"  Non Functional Properties (NFPs) such as security, quality of service and
business related properties enhance the service description and provide
necessary information about the fitness of its behaviour. These properties have
become crucial criteria for efficient selection and composition of Web
services. However, they belong to different domains, are complex, change
frequently and have to be semantically described. The W3C standard
WSPolicy,recommended to describe these properties doesn t define standardized
specifications that cover all NFPs domains. Moreover, it doesn t provide an
easy manner to express them independently of domains, and doesn t support their
semantic. This paper proposes a Model driven approach to describe and
automatically generate enriched Web services including semantic NFPs. It
explores both the use of the OMG Profile for Modelling and Analysis of
Real-Time Embedded Systems (MARTE) and the W3C standards. Mapping rules, from
NFPs profile to WS-Policy and SAWSDL files, transforms NFPs into policies
associated with WSDL elements.
",6,4
271,"  The computation of distance measures between nodes in graphs is inefficient
and does not scale to large graphs. We explore dense vector representations as
an effective way to approximate the same information: we introduce a simple yet
efficient and effective approach for learning graph embeddings. Instead of
directly operating on the graph structure, our method takes structural measures
of pairwise node similarities into account and learns dense node
representations reflecting user-defined graph distance measures, such as
e.g.the shortest path distance or distance measures that take information
beyond the graph structure into account. We demonstrate a speed-up of several
orders of magnitude when predicting word similarity by vector operations on our
embeddings as opposed to directly computing the respective path-based measures,
while outperforming various other graph embeddings on semantic similarity and
word sense disambiguation tasks and show evaluations on the WordNet graph and
two knowledge base graphs.
",6,0
272,"  Software-Defined Networking (SDN) controllers are considered as Network
Operating Systems (NOSs) and often viewed as a single point of failure.
Detecting which SDN controller is managing a target network is a big step for
an attacker to launch specific/effective attacks against it. In this paper, we
demonstrate the feasibility of fingerpirinting SDN controllers. We propose
techniques allowing an attacker placed in the data plane, which is supposed to
be physically separate from the control plane, to detect which controller is
managing the network. To the best of our knowledge, this is the first work on
fingerprinting SDN controllers, with as primary goal to emphasize the necessity
to highly secure the controller. We focus on OpenFlow-based SDN networks since
OpenFlow is currently the most deployed SDN technology by hardware and software
vendors.
",6,2
273,"  This paper proposes a group membership verification protocol preventing the
curious but honest server from reconstructing the enrolled signatures and
inferring the identity of querying clients. The protocol quantizes the
signatures into discrete embeddings, making reconstruction difficult. It also
aggregates multiple embeddings into representative values, impeding
identification. Theoretical and experimental results show the trade-off between
the security and the error rates.
",5,2
274,"  A common situation occurring when dealing with multimedia traffic is having
large data frames fragmented into smaller IP packets, and having these packets
sent independently through the network. For real-time multimedia traffic,
dropping even few packets of a frame may render the entire frame useless. Such
traffic is usually modeled as having {\em inter-packet dependencies}. We study
the problem of scheduling traffic with such dependencies, where each packet has
a deadline by which it should arrive at its destination. Such deadlines are
common for real-time multimedia applications, and are derived from stringent
delay constraints posed by the application. The figure of merit in such
environments is maximizing the system's {\em goodput}, namely, the number of
frames successfully delivered.
  We study online algorithms for the problem of maximizing goodput of
delay-bounded traffic with inter-packet dependencies, and use competitive
analysis to evaluate their performance. We present competitive algorithms for
the problem, as well as matching lower bounds that are tight up to a constant
factor. We further present the results of a simulation study which further
validates our algorithmic approach and shows that insights arising from our
analysis are indeed manifested in practice.
",2,3
275,"  The input language of the answer set solver clingo is based on the definition
of a stable model proposed by Paolo Ferraris. The semantics of the ASP-Core
language, developed by the ASP Standardization Working Group, uses the approach
to stable models due to Wolfgang Faber, Nicola Leone, and Gerald Pfeifer. The
two languages are based on different versions of the stable model semantics,
and the ASP-Core document requires, ""for the sake of an uncontroversial
semantics,"" that programs avoid the use of recursion through aggregates. In
this paper we prove that the absence of recursion through aggregates does
indeed guarantee the equivalence between the two versions of the stable model
semantics, and show how that requirement can be relaxed without violating the
equivalence property. The paper is under consideration for publication in
Theory and Practice of Logic Programming.
",6,2
276,"  The task of sentiment modification requires reversing the sentiment of the
input and preserving the sentiment-independent content. However, aligned
sentences with the same content but different sentiments are usually
unavailable. Due to the lack of such parallel data, it is hard to extract
sentiment independent content and reverse the sentiment in an unsupervised way.
Previous work usually can not reconcile sentiment transformation and content
preservation. In this paper, motivated by the fact the non-emotional context
(e.g., ""staff"") provides strong cues for the occurrence of emotional words
(e.g., ""friendly""), we propose a novel method that automatically extracts
appropriate sentiment information from learned sentiment memories according to
specific context. Experiments show that our method substantially improves the
content preservation degree and achieves the state-of-the-art performance.
",5,0
277,"  This work considers a line-of-sight underwater acoustic sensor network
(UWASN) consisting of $M$ underwater sensor nodes randomly deployed according
to uniform distribution within a vertical half-disc (the so-called trusted
zone). The sensor nodes report their sensed data to a sink node on water
surface on a shared underwater acoustic (UWA) reporting channel in a
time-division multiple-access (TDMA) fashion, while an active-yet-invisible
adversary (so-called Eve) is present in the close vicinity who aims to inject
malicious data into the system by impersonating some Alice node. To this end,
this work first considers an additive white Gaussian noise (AWGN) UWA channel,
and proposes a novel, multiple-features based, two-step method at the sink node
to thwart the potential impersonation attack by Eve. Specifically, the sink
node exploits the noisy estimates of the distance, the angle of arrival, and
the location of the transmit node as device fingerprints to carry out a number
of binary hypothesis tests (for impersonation detection) as well as a number of
maximum likelihood hypothesis tests (for transmitter identification when no
impersonation is detected). We provide closed-form expressions for the error
probabilities (i.e., the performance) of most of the hypothesis tests. We then
consider the case of a UWA with colored noise and frequency-dependent pathloss,
and derive a maximum-likelihood (ML) distance estimator as well as the
corresponding Cramer-Rao bound (CRB). We then invoke the proposed two-step,
impersonation detection framework by utilizing distance as the sole feature.
Finally, we provide detailed simulation results for both AWGN UWA channel and
the UWA channel with colored noise. Simulation results verify that the proposed
scheme is indeed effective for a UWA channel with colored noise and
frequency-dependent pathloss.
",3,2
278,"  In situ lossy compression allowing user-controlled data loss can
significantly reduce the I/O burden. For large-scale N-body simulations where
only one snapshot can be compressed at a time, the lossy compression ratio is
very limited because of the fairly low spatial coherence of the particle data.
In this work, we assess the state-of-the-art single-snapshot lossy compression
techniques of two common N-body simulation models: cosmology and molecular
dynamics. We design a series of novel optimization techniques based on the two
representative real-world N-body simulation codes. For molecular dynamics
simulation, we propose three compression modes (i.e., best speed, best
tradeoff, best compression mode) that can refine the tradeoff between the
compression rate (a.k.a., speed/throughput) and ratio. For cosmology
simulation, we identify that our improved SZ is the best lossy compressor with
respect to both compression ratio and rate. Its compression ratio is higher
than the second-best compressor by 11% with comparable compression rate.
Experiments with up to 1024 cores on the Blues supercomputer at Argonne show
that our proposed lossy compression method can reduce I/O time by 80% compared
with writing data directly to a parallel file system and outperforms the
second-best solution by 60%. Moreover, our proposed lossy compression methods
have the best rate-distortion with reasonable compression errors on the tested
N-body simulation data compared with state-of-the-art compressors.
",1,3
279,"  Cybersecurity of industrial control system is a very complex and challenging
research topic, due to the integration of these systems in national critical
infrastructures. The control systems are now interconnected in industrial
networks and frequently to the Internet. In this context they are becoming
targets of various cyber attacks conducted by malicious people such as hackers,
script kiddies, industrial spies and even foreign armies and intelligence
agencies. In this paper the authors propose a way to model the most frequent
attacker profiles and to estimate the success rate of an attack conducted in
given conditions. The authors use a fuzzy approach for generating attacker
profiles based on attacker attributes such as knowledge, technical resources
and motivation. The attack success rate is obtained by using another fuzzy
inference system that analyzes the attacker profile and system intrinsic
characteristics.
",5,1
280,"  The McEliece public-key encryption scheme has become an interesting
alternative to cryptosystems based on number-theoretical problems. Differently
from RSA and ElGa- mal, McEliece PKC is not known to be broken by a quantum
computer. Moreover, even tough McEliece PKC has a relatively big key size,
encryption and decryption operations are rather efficient. In spite of all the
recent results in coding theory based cryptosystems, to the date, there are no
constructions secure against chosen ciphertext attacks in the standard model -
the de facto security notion for public-key cryptosystems. In this work, we
show the first construction of a McEliece based public-key cryptosystem secure
against chosen ciphertext attacks in the standard model. Our construction is
inspired by a recently proposed technique by Rosen and Segev.
",2,0
281,"  Recurrent Neural Networks (RNNs) are powerful autoregressive sequence models,
but when used to generate natural language their output tends to be overly
generic, repetitive, and self-contradictory. We postulate that the objective
function optimized by RNN language models, which amounts to the overall
perplexity of a text, is not expressive enough to capture the notion of
communicative goals described by linguistic principles such as Grice's Maxims.
We propose learning a mixture of multiple discriminative models that can be
used to complement the RNN generator and guide the decoding process. Human
evaluation demonstrates that text generated by our system is preferred over
that of baselines by a large margin and significantly enhances the overall
coherence, style, and information content of the generated text.
",4,6
282,"  Open-access domestic broadband connection sharing constitutes a voluntary
practice that is associated with societal, economic and public-safety benefits.
Despite this fact, broadband subscribers are usually hesitant to freely share
their broadband connection with guests for a multitude of reasons; one of them
being sharing their network might hinder their own broadband quality of
experience. In this paper, we investigate experimentally the impact of uplink
guest traffic on the sharer's broadband quality of experience under both
generic and broadband-sharing-specific packet scheduling policies. Both
guest-user traffic and access point profiles employed in our study are
developed by analyzing real-world traffic traces and measurements, captured
from actual broadband sharing networking environments. Our results validate the
suitability of hybrid packet scheduling policies for broadband sharing schemes
and show that only a few dozen kilobytes per second of uplink guest traffic can
be tolerated by sharers without hampering their broadband quality of
experience. In this context, we show that the selection of the most appropriate
packet scheduling policy for broadband sharing, as well as its respective
configuration, depend largely on the capacity of the broadband connection and
the policy's packet-dropping behavior on guest traffic.
",1,6
283,"  Recent works have shown promise in using microarchitectural execution
patterns to detect malware programs. These detectors belong to a class of
detectors known as signature-based detectors as they catch malware by comparing
a program's execution pattern (signature) to execution patterns of known
malware programs. In this work, we propose a new class of detectors -
anomaly-based hardware malware detectors - that do not require signatures for
malware detection, and thus can catch a wider range of malware including
potentially novel ones. We use unsupervised machine learning to build profiles
of normal program execution based on data from performance counters, and use
these profiles to detect significant deviations in program behavior that occur
as a result of malware exploitation. We show that real-world exploitation of
popular programs such as IE and Adobe PDF Reader on a Windows/x86 platform can
be detected with nearly perfect certainty. We also examine the limits and
challenges in implementing this approach in face of a sophisticated adversary
attempting to evade anomaly-based detection. The proposed detector is
complementary to previously proposed signature-based detectors and can be used
together to improve security.
",1,6
284,"  Delay-capacity tradeoffs for mobile networks have been analyzed through a
number of research work. However, L\'{e}vy mobility known to closely capture
human movement patterns has not been adopted in such work. Understanding the
delay-capacity tradeoff for a network with L\'{e}vy mobility can provide
important insights into understanding the performance of real mobile networks
governed by human mobility. This paper analytically derives an important point
in the delay-capacity tradeoff for L\'{e}vy mobility, known as the critical
delay. The critical delay is the minimum delay required to achieve greater
throughput than what conventional static networks can possibly achieve (i.e.,
$O(1/\sqrt{n})$ per node in a network with $n$ nodes). The L\'{e}vy mobility
includes L\'{e}vy flight and L\'{e}vy walk whose step size distributions
parametrized by $\alpha \in (0,2]$ are both heavy-tailed while their times
taken for the same step size are different. Our proposed technique involves (i)
analyzing the joint spatio-temporal probability density function of a
time-varying location of a node for L\'{e}vy flight and (ii) characterizing an
embedded Markov process in L\'{e}vy walk which is a semi-Markov process. The
results indicate that in L\'{e}vy walk, there is a phase transition such that
for $\alpha \in (0,1)$, the critical delay is always $\Theta (n^{1/2})$ and for
$\alpha \in [1,2]$ it is $\Theta(n^{\frac{\alpha}{2}})$. In contrast, L\'{e}vy
flight has the critical delay $\Theta(n^{\frac{\alpha}{2}})$ for
$\alpha\in(0,2]$.
",3,2
285,"  In this paper, we consider the problem of allocating cache resources among
multiple content providers. The cache can be partitioned into slices and each
partition can be dedicated to a particular content provider, or shared among a
number of them. It is assumed that each partition employs the LRU policy for
managing content. We propose utility-driven partitioning, where we associate
with each content provider a utility that is a function of the hit rate
observed by the content provider. We consider two scenarios: i)~content
providers serve disjoint sets of files, ii)~there is some overlap in the
content served by multiple content providers. In the first case, we prove that
cache partitioning outperforms cache sharing as cache size and numbers of
contents served by providers go to infinity. In the second case, It can be
beneficial to have separate partitions for overlapped content. In the case of
two providers, it is usually always beneficial to allocate a cache partition to
serve all overlapped content and separate partitions to serve the
non-overlapped contents of both providers. We establish conditions when this is
true asymptotically but also present an example where it is not true
asymptotically. We develop online algorithms that dynamically adjust partition
sizes in order to maximize the overall utility and prove that they converge to
optimal solutions, and through numerical evaluations, we show they are
effective.
",1,5
286,"  In rendezvous, two agents traverse network edges in synchronous rounds and
have to meet at some node. In treasure hunt, a single agent has to find a
stationary target situated at an unknown node of the network. We study
tradeoffs between the amount of information ($\mathit{advice}$) available
$\mathit{a\ priori}$ to the agents and the cost (number of edge traversals) of
rendezvous and treasure hunt. Our goal is to find the smallest size of advice
which enables the agents to solve these tasks at some cost $C$ in a network
with $e$ edges. This size turns out to depend on the initial distance $D$ and
on the ratio $\frac{e}{C}$, which is the $\mathit{relative\ cost\ gain}$ due to
advice. For arbitrary graphs, we give upper and lower bounds of $O(D\log(D\cdot
\frac{e}{C}) +\log\log e)$ and $\Omega(D\log \frac{e}{C})$, respectively, on
the optimal size of advice. For the class of trees, we give nearly tight upper
and lower bounds of $O(D\log \frac{e}{C} + \log\log e)$ and $\Omega (D\log
\frac{e}{C})$, respectively.
",2,5
287,"  How to apply automated verification technology such as model checking and
static program analysis to millions of lines of embedded C/C++ code? How to
package this technology in a way that it can be used by software developers and
engineers, who might have no background in formal verification? And how to
convince business managers to actually pay for such a software? This work
addresses a number of those questions. Based on our own experience on
developing and distributing the Goanna source code analyzer for detecting
software bugs and security vulnerabilities in C/C++ code, we explain the
underlying technology of model checking, static analysis and SMT solving, steps
involved in creating industrial-proof tools.
",1,5
288,"  Exploitation techniques targeting intermediate (transit) network nodes in
public and private networks have been theoretically known and empirically
proven to work for quite some time. However, very little effort has been made
to look into the network-specific risks of compromising the Internet
infrastructure to this date. In this publication, we describe several methods
of hijacking live network traffic following a successful attack on a router or
switch. We demonstrate that modern network platforms are capable of targeted
traffic replication and redirection for online and offline analysis and
modification, which can be a threat far greater than loss of service or other
risks frequently associated with such exploits.
",0,4
289,"  Given a graph $G = (V,E)$ and a subset $T \subseteq V$ of terminals, a
\emph{Steiner tree} of $G$ is a tree that spans $T$. In the vertex-weighted
Steiner tree (VST) problem, each vertex is assigned a non-negative weight, and
the goal is to compute a minimum weight Steiner tree of $G$.
  We study a natural generalization of the VST problem motivated by multi-level
graph construction, the \emph{vertex-weighted grade-of-service Steiner tree
problem} (V-GSST), which can be stated as follows: given a graph $G$ and
terminals $T$, where each terminal $v \in T$ requires a facility of a minimum
grade of service $R(v)\in \{1,2,\ldots\ell\}$, compute a Steiner tree $G'$ by
installing facilities on a subset of vertices, such that any two vertices
requiring a certain grade of service are connected by a path in $G'$ with the
minimum grade of service or better. Facilities of higher grade are more costly
than facilities of lower grade. Multi-level variants such as this one can be
useful in network design problems where vertices may require facilities of
varying priority.
  While similar problems have been studied in the edge-weighted case, they have
not been studied as well in the more general vertex-weighted case. We first
describe a simple heuristic for the V-GSST problem whose approximation ratio
depends on $\ell$, the number of grades of service. We then generalize the
greedy algorithm of [Klein \& Ravi, 1995] to show that the V-GSST problem
admits a $(2 \ln |T|)$-approximation, where $T$ is the set of terminals
requiring some facility. This result is surprising, as it shows that the
(seemingly harder) multi-grade problem can be approximated as well as the VST
problem, and that the approximation ratio does not depend on the number of
grades of service.
",6,3
290,"  Now a day's Heterogeneous wireless network is a promising field of research
interest. Various challenges exist in this hybrid combination like load
balancing, resource management and so on. In this paper we introduce a reliable
load balancing architecture for heterogeneous wireless communications to ensure
certain level of quality of service. To conquer the problem of centralized and
distributed design, a semi distributed load balancing architecture for multiple
access networks is introduced. In this grid based design multiple Load and
Mobile Agent Management Units is incorporated. To prove the compactness of the
design, integrated reliability, signalling overhead and total processing time
is calculated. And finally simulation result shows that overall system
performance is improved by enhancing reliability, reducing signalling overhead
and processing time.
",6,4
291,"  With the ubiquity of IoT devices there is a growing demand for
confidentiality and integrity of data. Solutions based on reconfigurable logic
(CPLD or FPGA) have certain advantages over ASIC and MCU/SoC alternatives.
Programmable logic devices are ideal for both confidentiality and upgradability
purposes. In this context the hardware security aspects of CPLD/FPGA devices
are paramount. This paper shows preliminary evaluation of hardware security in
Intel MAX 10 devices. These FPGAs are one of the most suitable candidates for
applications demanding extensive features and high level of security. Their
strong and week security aspects are revealed and some recommendations are
suggested to counter possible security vulnerabilities in real designs. This is
a feasibility study paper. Its purpose is to highlight the most vulnerable
areas to attacks aimed at data extraction and reverse engineering. That way
further investigations could be performed on specific areas of concern.
",5,3
292,"  Modern blockchain systems support creation of smart contracts -- stateful
programs hosted and executed on a blockchain. Smart contracts hold and transfer
significant amounts of digital currency which makes them an attractive target
for security attacks. It has been shown that many contracts deployed to public
ledgers contain security vulnerabilities. Moreover, the design of blockchain
systems does not allow the code of the smart contract to be changed after it
has been deployed to the system. Therefore, it is important to guarantee the
correctness of smart contracts prior to their deployment.
  Formal verification is widely used to check smart contracts for correctness
with respect to given specification. In this work we consider program synthesis
techniques in which the specification is used to generate
correct-by-construction programs. We focus on one of the special cases of
program synthesis where programs are modeled with finite state machines (FSMs).
We show how FSM synthesis can be applied to the problem of automatic smart
contract generation. Several case studies of smart contracts are outlined:
crowdfunding platform, blinded auction and a license contract. For each case
study we specify the corresponding smart contract with a set of formulas in
linear temporal logic (LTL) and use this specification together with test
scenarios to synthesize a FSM model for that contract. These models are later
used to generate executable Solidity code which can be directly used in a
blockchain system.
",0,1
293,"  In a wireless network, the efficiency of scheduling algorithms over
time-varying channels depends heavily on the accuracy of the Channel State
Information (CSI), which is usually quite ``costly'' in terms of consuming
network resources. Scheduling in such systems is also subject to stringent
constraints such as power and bandwidth, which limit the maximum number of
simultaneous transmissions. In the meanwhile, communication channels in
wireless systems typically fluctuate in a time-correlated manner. We hence
design schedulers to exploit the temporal-correlation inherent in channels with
memory and ARQ-styled feedback from the users for better channel state
knowledge, under the assumption of Markovian channels and the stringent
constraint on the maximum number of simultaneously active users. We model this
problem under the framework of a Partially Observable Markov Decision
Processes.
  In recent work, a low-complexity optimal solution was developed for this
problem under a long-term time-average resource constraint. However, in real
systems with instantaneous resource constraints, how to optimally exploit the
temporal correlation and satisfy realistic stringent constraint on the
instantaneous service remains elusive. In this work, we incorporate a stringent
constraint on the simultaneously scheduled users and propose a low-complexity
scheduling algorithm that dynamically implements user scheduling and dummy
packet broadcasting. We show that the throughput region of the optimal policy
under the long-term average resource constraint can be asymptotically achieved
in the stringent constrained scenario by the proposed algorithm, in the many
users limiting regime.
",6,5
294,"  In order to ensure high availability of Web services, recently, a new
approach was proposed based on the use of communities. In composition, this
approach consists in replacing the failed Web service by another web service
joining a community offering the same functionality of the service failed.
However, this substitution may cause inconsistency in the semantic composition
and alter its mediation initially taken to resolve the semantic heterogeneities
between Web services. This paper presents a context oriented solution to this
problem by forcing the community to adopt the semantic of the failed web
service before the substitution in which all inputs and outputs to/from the
latter must be converted according to this adopted semantic, avoiding any
alteration of a semantic mediation in web service composition.
",5,3
295,"  MaxSAT, the optimization version of the well-known SAT problem, has attracted
a lot of research interest in the last decade. Motivated by the many important
applications and inspired by the success of modern SAT solvers, researchers
have developed many MaxSAT solvers. Since most research is algorithmic, its
significance is mostly evaluated empirically. In this paper we want to address
MaxSAT from the more formal point of view of Proof Complexity. With that aim we
start providing basic definitions and proving some basic results. Then we
analyze the effect of adding split and virtual, two original inference rules,
to MaxSAT resolution. We show that each addition makes the resulting proof
system stronger, with the virtual rule capturing the recently proposed concept
of circular proof.
",1,6
296,"  Submodular optimization generalizes many classic problems in combinatorial
optimization and has recently found a wide range of applications in machine
learning (e.g., feature engineering and active learning). For many large-scale
optimization problems, we are often concerned with the adaptivity complexity of
an algorithm, which quantifies the number of sequential rounds where
polynomially-many independent function evaluations can be executed in parallel.
While low adaptivity is ideal, it is not sufficient for a distributed algorithm
to be efficient, since in many practical applications of submodular
optimization the number of function evaluations becomes prohibitively
expensive. Motivated by these applications, we study the adaptivity and query
complexity of adaptive submodular optimization.
  Our main result is a distributed algorithm for maximizing a monotone
submodular function with cardinality constraint $k$ that achieves a
$(1-1/e-\varepsilon)$-approximation in expectation. This algorithm runs in
$O(\log(n))$ adaptive rounds and makes $O(n)$ calls to the function evaluation
oracle in expectation. The approximation guarantee and query complexity are
optimal, and the adaptivity is nearly optimal. Moreover, the number of queries
is substantially less than in previous works. Last, we extend our results to
the submodular cover problem to demonstrate the generality of our algorithm and
techniques.
",1,4
297,"  In this paper, we propose a new method of channel estimation for asynchronous
additive white Gaussian noise channels in satellite communications. This method
is based on signals correlation and multiuser interference cancellation which
adopts a successive structure. Propagation delays and signals amplitudes are
jointly estimated in order to be used for data detection at the receiver. As, a
multiuser detector, a single stage successive interference cancellation (SIC)
architecture is analyzed and integrated to the channel estimation technique and
the whole system is evaluated. The satellite access method adopted is the
direct sequence code division multiple access (DS CDMA) one. To evaluate the
channel estimation and the detection technique, we have simulated a satellite
uplink with an asynchronous multiuser access.
",3,2
298,"  Steganography is the technique of hiding confidential information within any
media. Steganography is often confused with cryptography because the two are
similar in the way that they both are used to protect confidential information.
The difference between the two is in the appearance in the processed output;
the output of steganography operation is not apparently visible but in
cryptography the output is scrambled so that it can draw attention. Steganlysis
is process to detect of presence of steganography. In this article we have
tried to elucidate the different approaches towards implementation of
steganography using 'multimedia' file (text, static image, audio and video) and
Network IP datagram as cover. Also some methods of steganalysis will be
discussed.
",6,1
299,"  Biometric Authentication has become a very popular method for different
state-of-the-art security architectures. Albeit the ubiquitous acceptance and
constant development of trivial biometric authentication methods such as
fingerprint, palm-print, retinal scan etc., the possibility of producing a
highly competitive performance from somewhat less-popular methods still
remains. Electrocardiogram (ECG) based biometric authentication is such a
method, which, despite its limited appearance in earlier research works, are
currently being observed as equivalently high-performing as other trivial
popular methods. In this paper, we have proposed a model to optimize the
runtime of identification event in ECG based biometric authentication and we
have achieved a maximum of 79.26% time reduction with 100% accuracy.
",5,2
300,"  For any $T \geq 1$, there are constants $R=R(T) \geq 1$ and
$\zeta=\zeta(T)>0$ and a randomized algorithm that takes as input an integer
$n$ and two strings $x,y$ of length at most $n$, and runs in time
$O(n^{1+\frac{1}{T}})$ and outputs an upper bound $U$ on the edit distance
$ED(x,y)$ that with high probability, satisfies $U \leq
R(ED(x,y)+n^{1-\zeta})$. In particular, on any input with $ED(x,y) \geq
n^{1-\zeta}$ the algorithm outputs a constant factor approximation with high
probability.
  A similar result has been proven independently by Brakensiek and Rubinstein
(2019).
",5,2
301,"  To securely leverage the advantages of Cloud Computing, recently a lot of
research has happened in the area of ""Secure Query Processing over Encrypted
Data"". As a concrete use case, many encryption schemes have been proposed for
securely processing k Nearest Neighbors (SkNN) over encrypted data in the
outsourced setting. Recently Zhu et al[25]. proposed a SkNN solution which
claimed to satisfy following four properties: (1)Data Privacy, (2)Key
Confidentiality, (3)Query Privacy, and (4)Query Controllability. However, in
this paper, we present an attack which breaks the Query Controllability claim
of their scheme. Further, we propose a new SkNN solution which satisfies all
the four existing properties along with an additional essential property of
Query Check Verification. We analyze the security of our proposed scheme and
present the detailed experimental results to showcase the efficiency in real
world scenario.
",5,0
302,"  Graphical user interfaces (GUIs) are integral parts of software systems that
require interactions from their users. Software testers have paid special
attention to GUI testing in the last decade, and have devised techniques that
are effective in finding several kinds of GUI errors. However, the introduction
of new types of interactions in GUIs (e.g., direct manipulation) presents new
kinds of errors that are not targeted by current testing techniques. We believe
that to advance GUI testing, the community needs a comprehensive and high level
GUI fault model, which incorporates all types of interactions. The work
detailed in this paper establishes 4 contributions: 1) A GUI fault model
designed to identify and classify GUI faults. 2) An empirical analysis for
assessing the relevance of the proposed fault model against failures found in
real GUIs. 3) An empirical assessment of two GUI testing tools (i.e. GUITAR and
Jubula) against those failures. 4) GUI mutants we've developed according to our
fault model. These mutants are freely available and can be reused by developers
for benchmarking their GUI testing tools.
",2,5
303,"  We study the problem of assigning jobs to applicants. Each applicant has a
weight and provides a preference list ranking a subset of the jobs. A matching
M is popular if there is no other matching M' such that the weight of the
applicants who prefer M' over M exceeds the weight of those who prefer M over
M'. This paper gives efficient algorithms to find a popular matching if one
exists.
",1,2
304,"  Synchronized Random Access Channel (RACH) attempts by Internet of Things
(IoT) devices could result in Radio Access Network (RAN) overload in LTE-A.
3GPP adopted Barring Bitmap Enabled-Extended Access Barring (EAB-BB) mechanism
that announces the EAB information (i.e., a list of barred Access Classes)
through a barring bitmap as the baseline solution to mitigate the RAN overload.
EAB-BB was analyzed for its optimal performance in a recent work. However,
there has been no work that analyzes Barring Factor Enabled-Extended Access
Barring (EAB-BF), an alternative mechanism that was considered during the
standardization process. Due to the modeling complexity involved, not only has
it been difficult to analyze EAB-BF, but also, a much more far-reaching issue,
like the effect of these schemes on key network performance parameter, like
eNodeB energy consumption, has been overlooked. In this regard, for the first
time, we develop a novel analytical model for EAB-BF to obtain its performance
metrics. Results obtained from our analysis and simulation are seen to match
very well. Furthermore, we also build an eNodeB energy consumption model to
serve the IoT RACH requests. We then show that our analytical and energy
consumption models can be combined to obtain EAB-BF settings that can minimize
eNodeB energy consumption, while simultaneously providing optimal Quality of
Service (QoS) performance. Results obtained reveal that the optimal performance
of EAB-BF is better than that of EAB-BB. Furthermore, we also show that not
only all the three 3GPP-proposed EAB-BF settings considered during
standardization provide sub-optimal QoS to devices, but also result in
excessive eNodeB energy consumption, thereby acutely penalizing the network.
Finally, we provide corrections to these 3GPP-settings that can lead to
significant gains in EAB-BF performance.
",2,6
305,"  We prove ""untyping"" theorems: in some typed theories (semirings, Kleene
algebras, residuated lattices, involutive residuated lattices), typed equations
can be derived from the underlying untyped equations. As a consequence, the
corresponding untyped decision procedures can be extended for free to the typed
settings. Some of these theorems are obtained via a detour through fragments of
cyclic linear logic, and give rise to a substantial optimisation of standard
proof search algorithms.
",2,5
306,"  The encode-decoder framework has shown recent success in image captioning.
Visual attention, which is good at detailedness, and semantic attention, which
is good at comprehensiveness, have been separately proposed to ground the
caption on the image. In this paper, we propose the Stepwise Image-Topic
Merging Network (simNet) that makes use of the two kinds of attention at the
same time. At each time step when generating the caption, the decoder
adaptively merges the attentive information in the extracted topics and the
image according to the generated context, so that the visual information and
the semantic information can be effectively combined. The proposed approach is
evaluated on two benchmark datasets and reaches the state-of-the-art
performances.(The code is available at https://github.com/lancopku/simNet)
",5,6
307,"  The rapid evolution in mobile devices and communication technology has
increased the number of mobile device users dramatically. The mobile device has
replaced many other devices and is used to perform many tasks ranging from
establishing a phone call to performing critical and sensitive tasks like money
payments. Since the mobile device is accompanying a person most of his time, it
is highly probably that it includes personal and sensitive data for that
person. The increased use of mobile devices in daily life made mobile systems
an excellent target for attacks. One of the most important attacks is phishing
attack in which an attacker tries to get the credential of the victim and
impersonate him. In this paper, analysis of different types of phishing attacks
on mobile devices is provided. Mitigation techniques - anti-phishing techniques
- are also analyzed. Assessment of each technique and a summary of its
advantages and disadvantages is provided. At the end, important steps to guard
against phishing attacks are provided. The aim of the work is to put phishing
attacks on mobile systems in light, and to make people aware of these attacks
and how to avoid them
",2,5
308,"  Transactions can simplify distributed applications by hiding data
distribution, concurrency, and failures from the application developer. Ideally
the developer would see the abstraction of a single large machine that runs
transactions sequentially and never fails. This requires the transactional
subsystem to provide opacity (strict serializability for both committed and
aborted transactions), as well as transparent fault tolerance with high
availability. As even the best abstractions are unlikely to be used if they
perform poorly, the system must also provide high performance.
  Existing distributed transactional designs either weaken this abstraction or
are not designed for the best performance within a data center. This paper
extends the design of FaRM - which provides strict serializability only for
committed transactions - to provide opacity while maintaining FaRM's high
throughput, low latency, and high availability within a modern data center. It
uses timestamp ordering based on real time with clocks synchronized to within
tens of microseconds across a cluster, and a failover protocol to ensure
correctness across clock master failures. FaRM with opacity can commit 5.4
million neworder transactions per second when running the TPC-C transaction mix
on 90 machines with 3-way replication.
",1,0
309,"  In this paper, we design a framework to obtain efficient algorithms for
several problems with a global constraint (acyclicity or connectivity) such as
Connected Dominating Set, Node Weighted Steiner Tree, Maximum Induced Tree,
Longest Induced Path, and Feedback Vertex Set. We design a meta-algorithm that
solves all these problems and whose running time is upper bounded by
$2^{O(k)}\cdot n^{O(1)}$, $2^{O(k \log(k))}\cdot n^{O(1)}$, $2^{O(k^2)}\cdot
n^{O(1)}$ and $n^{O(k)}$ where $k$ is respectively the clique-width,
$\mathbb{Q}$-rank-width, rank-width and maximum induced matching width of a
given decomposition. Our meta-algorithm simplifies and unifies the known
algorithms for each of the parameters and its running time matches
asymptotically also the running times of the best known algorithms for basic
NP-hard problems such as Vertex Cover and Dominating Set. Our framework is
based on the $d$-neighbor equivalence defined in [Bui-Xuan, Telle and
Vatshelle, TCS 2013]. The results we obtain highlight the importance of this
equivalence relation on the algorithmic applications of width measures.
  We also prove that our framework could be useful for $W[1]$-hard problems
parameterized by clique-width such as Max Cut and Maximum Minimal Cut. For
these latter problems, we obtain $n^{O(k)}$, $n^{O(k)}$ and $n^{2^{O(k)}}$ time
algorithms where $k$ is respectively the clique-width, the
$\mathbb{Q}$-rank-width and the rank-width of the input graph.
",6,2
310,"  In hardware virtualization a hypervisor provides multiple Virtual Machines
(VMs) on a single physical system, each executing a separate operating system
instance. The hypervisor schedules execution of these VMs much as the scheduler
in an operating system does, balancing factors such as fairness and I/O
performance. As in an operating system, the scheduler may be vulnerable to
malicious behavior on the part of users seeking to deny service to others or
maximize their own resource usage.
  Recently, publically available cloud computing services such as Amazon EC2
have used virtualization to provide customers with virtual machines running on
the provider's hardware, typically charging by wall clock time rather than
resources consumed. Under this business model, manipulation of the scheduler
may allow theft of service at the expense of other customers, rather than
merely reallocating resources within the same administrative domain.
  We describe a flaw in the Xen scheduler allowing virtual machines to consume
almost all CPU time, in preference to other users, and demonstrate kernel-based
and user-space versions of the attack. We show results demonstrating the
vulnerability in the lab, consuming as much as 98% of CPU time regardless of
fair share, as well as on Amazon EC2, where Xen modifications protect other
users but still allow theft of service. In case of EC2, following the
responsible disclosure model, we have reported this vulnerability to Amazon;
they have since implemented a fix that we have tested and verified (See
Appendix B). We provide a novel analysis of the necessary conditions for such
attacks, and describe scheduler modifications to eliminate the vulnerability.
  We present experimental results demonstrating the effectiveness of these
defenses while imposing negligible overhead.
",4,6
311,"  In this work, we present our early stage results on a Conflicts Check
Protocol (CCP) that enables preventing potential attacks on bitcoin system.
Based on the observation and discovery of a common symptom that many attacks
may generate, CCP refines the current bitcoin systems by proposing a novel
arbitration mechanism that is capable to determine the approval or abandon of
certain transactions involved in confliction. This work examines the security
issue of bitcoin from a new perspective, which may extend to a larger scope of
attack analysis and prevention
",5,2
312,"  We investigate the problem of reaching majority agreement in a disconnected
network. We obtain conditions under which such an agreement is certainly
possible/impossible, and observe that these coincide in the ternary case.
",6,4
313,"  Multimedia data security is becoming important with the continuous increase
of digital communications on internet. The encryption algorithms developed to
secure text data are not suitable for multimedia application because of the
large data size and real time constraint. In this paper, classification and
description of various video encryption algorithms are presented. Analysis and
Comparison of these algorithms with respect to various parameters like visual
degradation, encryption ratio, speed, compression friendliness, format
compliance and cryptographic security is presented.
",2,5
314,"  The Enterprise Ethereum Client Specification by the Enterprise Ethereum
Alliance defines the requirements which Ethereum Clients offering private smart
contract capabilities should comply with. This specification though ground
breaking, misses some important blockchain requirements and does not fully
consider the requirements of Ethereum Clients offering Private Sidechain
capabilities. This paper presents the case for Private Sidechains and defines
requirements to be complied with to deliver this technology.
  The capabilities of three blockchain clients have been analysed based on the
requirements: Quorum, Parity, and Hyperledger Fabric. Quorum and Hyperledger
Fabric operate as private consortium blockchains where as Parity delivers
private transaction capabilities on top of Ethereum MainNet. These differing
approaches has led to different strengths and weaknesses which has resulted in
each client not complying with one or more key requirement. In particular, none
of the reviewed blockchain clients support the ability to determine bootstrap
information to establish on-demand blockchains and none of the clients support
secure management and pinning from Ethereum MainNet.
  This paper presents Ethereum Private Sidechains and a range of technologies
which allow it to deliver on complex sidechain requirements. Ethereum
Registration Authorities are presented, which allow entities which have not
previously interacted to securely obtain information to bootstrap a sidechain,
and a Management and Pinning strategy is described which allows the state of a
sidechain to be securely pinned to Ethereum MainNet without compromising
privacy.
",5,2
315,"  We propose MetaCP, a Meta Cryptography Protocol verification tool, as an
automated tool simplifying the design of security protocols through a graphical
interface. The graphical interface can be seen as a modern editor of a
non-relational database whose data are protocols. The information of protocols
are stored in XML, enjoying a fixed format and syntax aiming to contain all
required information to specify any kind of protocol. This XML can be seen as
an almost semanticless language, where different plugins confer strict
semantics modelling the protocol into a variety of back-end verification
languages. In this paper, we showcase the effectiveness of this novel approach
by demonstrating how easy MetaCP makes it to design and verify a protocol going
from the graphical design to formally verified protocol using a Tamarin prover
plugin. Whilst similar approaches have been proposed in the past, most famously
the AVISPA Tool, no previous approach provides such as small learning curve and
ease of use even for non security professionals, combined with the flexibility
to integrate with the state of the art verification tools.
",6,0
316,"  Braids groups provide an alternative to number theoretic public cryptography
and can be implemented quite efficiently. The paper proposes five signature
schemes: Proxy Signature, Designated Verifier, Bi-Designated Verifier,
Designated Verifier Proxy Signature And Bi-Designated Verifier Proxy Signature
scheme based on braid groups. We also discuss the security aspects of each of
the proposed schemes.
",3,2
317,"  Recent studies demonstrate that effective healthcare can benefit from using
the human genomic information. For instance, analysis of tumor genomes has
revealed 140 genes whose mutations contribute to cancer. As a result, many
institutions are using statistical analysis of genomic data, which are mostly
based on genome-wide association studies (GWAS). GWAS analyze genome sequence
variations in order to identify genetic risk factors for diseases. These
studies often require pooling data from different sources together in order to
unravel statistical patterns or relationships between genetic variants and
diseases. In this case, the primary challenge is to fulfill one major
objective: accessing multiple genomic data repositories for collaborative
research in a privacy-preserving manner. Due to the sensitivity and privacy
concerns regarding the genomic data, multi-jurisdictional laws and policies of
cross-border genomic data sharing are enforced among different regions of the
world. In this article, we present SAFETY, a hybrid framework, which can
securely perform GWAS on federated genomic datasets using homomorphic
encryption and recently introduced secure hardware component of Intel Software
Guard Extensions (Intel SGX) to ensure high efficiency and privacy at the same
time. Different experimental settings show the efficacy and applicability of
such hybrid framework in secure conduction of GWAS. To the best of our
knowledge, this hybrid use of homomorphic encryption along with Intel SGX is
not proposed or experimented to this date. Our proposed framework, SAFETY is up
to 4.82 times faster than the best existing secure computation technique.
",5,1
318,"  A Petri net is structurally cyclic if every configuration is reachable from
itself in one or more steps. We show that structural cyclicity is decidable in
deterministic polynomial time. For this, we adapt the Kosaraju's approach for
the general reachability problem for Petri nets.
",3,2
319,"  The debts' clearing problem is about clearing all the debts in a group of $n$
entities (e.g. persons, companies) using a minimal number of money transaction
operations. In our previous works we studied the problem, gave a dynamic
programming solution solving it and proved that it is NP-hard. In this paper we
adapt the problem to dynamic graphs and give a data structure to solve it.
Based on this data structure we develop a new algorithm, that improves our
previous one for the static version of the problem.
",1,2
320,"  An O(n+m)-time algorithm is presented for counting the number of models of a
two Conjunctive Normal Form Formula F that represents a Cactus graph, where n
is the number of variables and m is the number of clauses of F. Although, it
was already known that this class of formulas could be computed in polynomial
time, we compare our proposal algorithm with two state of the art
implementations for the same problem, sharpSAT and countAntom. The results of
the comparison show that our algorithm outperforms both implementations, and it
can be considered as a base case for general counting of two Conjunctive Normal
Form Formulas.
",2,6
321,"  This paper identifies a problem in both the TLA+ specification and the
implementation of the Egalitarian Paxos protocol. It is related to how replicas
switch from one ballot to another when computing the dependencies of a command.
The problem may lead replicas to diverge and break the linearizability of the
replicated service.
",6,4
322,"  An-ever increasing number of social media websites, electronic newspapers and
Internet forums allow visitors to leave comments for others to read and
interact. This exchange is not free from participants with malicious
intentions, which do not contribute with the written conversation. Among
different communities users adopt strategies to handle such users. In this
paper we present a comprehensive categorization of the trolling phenomena
resource, inspired by politeness research and propose a model that jointly
predicts four crucial aspects of trolling: intention, interpretation, intention
disclosure and response strategy. Finally, we present a new annotated dataset
containing excerpts of conversations involving trolls and the interactions with
other users that we hope will be a useful resource for the research community.
",2,6
323,"  We describe approximation algorithms in Linial's classic LOCAL model of
distributed computing to find maximum-weight matchings in a hypergraph of rank
$r$. Our main result is a deterministic algorithm to generate a matching which
is an $O(r)$-approximation to the maximum weight matching, running in $\tilde
O(r \log \Delta + \log^2 \Delta + \log^* n)$ rounds. (Here, the $\tilde O()$
notations hides $\text{polyloglog } \Delta$ and $\text{polylog } r$ factors).
This is based on a number of new derandomization techniques extending methods
of Ghaffari, Harris & Kuhn (2017).
  As a main application, we obtain nearly-optimal algorithms for the
long-studied problem of maximum-weight graph matching. Specifically, we get a
$(1+\epsilon)$ approximation algorithm using $\tilde O(\log \Delta / \epsilon^3
+ \text{polylog}(1/\epsilon, \log \log n))$ randomized time and $\tilde
O(\log^2 \Delta / \epsilon^4 + \log^*n / \epsilon)$ deterministic time.
  The second application is a faster algorithm for hypergraph maximal matching,
a versatile subroutine introduced in Ghaffari et al. (2017) for a variety of
local graph algorithms. This gives an algorithm for $(2 \Delta - 1)$-edge-list
coloring in $\tilde O(\log^2 \Delta \log n)$ rounds deterministically or
$\tilde O( (\log \log n)^3 )$ rounds randomly. Another consequence (with
additional optimizations) is an algorithm which generates an edge-orientation
with out-degree at most $\lceil (1+\epsilon) \lambda \rceil$ for a graph of
arboricity $\lambda$; for fixed $\epsilon$ this runs in $\tilde O(\log^6 n)$
rounds deterministically or $\tilde O(\log^3 n )$ rounds randomly.
",0,6
324,"  With densification of nodes in cellular networks, free space optic (FSO)
connections are becoming an appealing low cost and high rate alternative to
copper and fiber as the backhaul solution for wireless communication systems.
To ensure a reliable cellular backhaul, provisions for redundant, disjoint
paths between the nodes must be made in the design phase. This paper aims at
finding a cost-effective solution to upgrade the cellular backhaul with
pre-deployed optical fibers using FSO links and mirror components. Since the
quality of the FSO links depends on several factors, such as transmission
distance, power, and weather conditions, we adopt an elaborate formulation to
calculate link reliability. We present a novel integer linear programming model
to approach optimal FSO backhaul design, guaranteeing $K$-disjoint paths
connecting each node pair. Next, we derive a column generation method to a
path-oriented mathematical formulation. Applying the method in a sequential
manner enables high computational scalability. We use realistic scenarios to
demonstrate our approaches efficiently provide optimal or near-optimal
solutions, and thereby allow for accurately dealing with the trade-off between
cost and reliability.
",5,6
325,"  Modern browsers implement different security policies such as the Content
Security Policy (CSP), a mechanism designed to mitigate popular web
vulnerabilities, and the Same Origin Policy (SOP), a mechanism that governs
interactions between resources of web pages. In this work, we describe how CSP
may be violated due to the SOP when a page contains an embedded iframe from the
same origin. We analyse 1 million pages from 10,000 top Alexa sites and report
that at least 31.1% of current CSP-enabled pages are potentially vulnerable to
CSP violations. Further considering real-world situations where those pages are
involved in same-origin nested browsing contexts, we found that in at least
23.5% of the cases, CSP violations are possible. During our study, we also
identified a divergence among browsers implementations in the enforcement of
CSP in srcdoc sandboxed iframes, which actually reveals a problem in
Gecko-based browsers CSP implementation. To ameliorate the problematic
conflicts of the security mechanisms, we discuss measures to avoid CSP
violations.
",2,3
326,"  Replacing hand-engineered pipelines with end-to-end deep learning systems has
enabled strong results in applications like speech and object recognition.
However, the causality and latency constraints of production systems put
end-to-end speech models back into the underfitting regime and expose biases in
the model that we show cannot be overcome by ""scaling up"", i.e., training
bigger models on more data. In this work we systematically identify and address
sources of bias, reducing error rates by up to 20% while remaining practical
for deployment. We achieve this by utilizing improved neural architectures for
streaming inference, solving optimization issues, and employing strategies that
increase audio and label modelling versatility.
",4,6
327,"  Spreadsheets are used extensively in industry, often for business critical
purposes. In previous work we have analyzed the information needs of
spreadsheet professionals and addressed their need for support with the
transition of a spreadsheet to a colleague with the generation of data flow
diagrams. In this paper we describe the application of these data flow diagrams
for the purpose of understanding a spreadsheet with three example cases. We
furthermore suggest an additional application of the data flow diagrams: the
assessment of the quality of the spreadsheet's design.
",2,6
328,"  Ignoring human values in software development may disadvantage users by
breaching their values and introducing biases in software. This can be
mitigated by informing developers about the value implications of their choices
and taking initiatives to account for human values in software. To this end, we
propose the notion of Value Programming with three principles: (P1) annotating
source code and related artifacts with respect to values; (P2) inspecting
source code to detect conditions that lead to biases and value breaches in
software, i.e., (P3) making recommendations to mitigate biases and value
breaches. To facilitate value programming, we propose a framework that allows
for automated annotation of software code with respect to human values. The
proposed framework lays a solid foundation for inspecting human values in code
and making recommendations to overcome biases and value breaches in software.
",1,2
329,"  Simulation of VoIP (Voice over Internet Protocol) traffic through UMTS
(Universal Mobile Telecommunication System) and WiFi (IEEE 802.11x) alone and
together are analysed for Quality of Service (QoS) performance. The average
jitter of VoIP transiting the WiFi-UMTS network has been found to be lower than
that of either solely through the WiFi and the UMTS networks. It is normally
expected to be higher than traversing through the WiFi network only. Both the
MOS (Mean Opinion Score) and the packet end-to-end delay were also found to be
much lower than expected through the heterogeneous WiFi-UMTS network.
",5,2
330,"  Storage networking technology has enjoyed strong growth in recent years, but
security concerns and threats facing networked data have grown equally fast.
Today, there are many potential threats that are targeted at storage networks,
including data modification, destruction and theft, DoS attacks, malware,
hardware theft and unauthorized access, among others. In order for a Storage
Area Network (SAN) to be secure, each of these threats must be individually
addressed. In this paper, we present a comparative study by implementing
different security methods in IP Storage network.
",1,6
331,"  Most organizations use large and complex spreadsheets that are embedded in
their mission-critical processes and are used for decision-making purposes.
Identification of the various types of errors that can be present in these
spreadsheets is, therefore, an important control that organizations can use to
govern their spreadsheets. In this paper, we propose a taxonomy for
categorizing qualitative errors in spreadsheet models that offers a framework
for evaluating the readiness of a spreadsheet model before it is released for
use by others in the organization. The classification was developed based on
types of qualitative errors identified in the literature and errors committed
by end-users in developing a spreadsheet model for Panko's (1996) ""Wall
problem"". Closer inspection of the errors reveals four logical groupings of the
errors creating four categories of qualitative errors. The usability and
limitations of the proposed taxonomy and areas for future extension are
discussed.
",2,5
332,"  The emergence of many challenges and the rapid development of the means of
communications and computer networks and the Internet. Digital information
revolution has affected a lot on human societies. Data today has become
available in digital format (text, image, audio, and video), which led to the
emergence of many opportunities for creativity for innovation as well as the
emergence of a new kind of challenges
",0,6
333,"  State-of-the-art attacks against cyclic logic obfuscation use satisfiability
solvers that are equipped with a set of cycle avoidance clauses. These cycle
avoidance clauses are generated in a pre-processing step and define various key
combinations that could open or close cycles without making the circuit
oscillating or stateful. In this paper, we show that this pre-processing step
has to generate cycle avoidance conditions on all cycles in a netlist,
otherwise, a missing cycle could trap the solver in an infinite loop or make it
exit with an incorrect key. Then, we propose several techniques by which the
number of cycles is exponentially increased as a function of the number of
inserted feedbacks. We further illustrate that when the number of feedbacks is
increased, the pre-processing step of the attack faces an exponential increase
in complexity and runtime, preventing the correct composition of cycle
avoidance clauses in a reasonable time. On the other hand, if the
pre-processing is not concluded, the attack formulated by the satisfiability
solver will either get stuck or exit with an incorrect key. Hence, when the
cyclic obfuscation under the conditions proposed in this paper is implemented,
it would impose an exponentially difficult problem for the satisfiability
solver based attacks.
",1,3
334,"  Technological advances regarding software integration processes have
revolutionized the communication between government and society, which are
increasingly based on information and communication technologies (ICTs).
Service-Oriented Architecture (SOA) has emerged originating new prospects for
system integration within organizations and external partners, providing
essential information for decision-making process. Brazilian e-Government
initiatives has introduced as a national electronic document model known as
Electronic Transportation Knowledge (CT-e), looking for simplifying additional
obligations of taxpayers and, at the same time, allowing real-time monitoring
of cargo transportation services provided by the Revenue. Nonetheless, there is
a major challenge that prevents Transportation Management Systems (TMS) and
Government to be benefited by this innovation, due to their distinct platforms
and databases that prevent information exchange between them. This paper
proposes an architectural solution to integrate TMS systems with CT-e Web
Service applying concepts of SOA and N-tier architecture. Through a real case
study of a large Cargo carrier, we report an increase in transportation
knowledge management, speeding up in the communication and data validation
process and several costs reduction including paper, printing, document storage
and those involved in the necessary logistics that is needed to recover such
documents.
",4,6
335,"  In the project portfolio management, the project selection phase presents the
greatest interest. In this article, we focus on this important phase by
proposing a new method of projects selection consisting of several steps. We
propose as a first step, a classification of projects based on the three most
important criteria namely the value maximization, risk minimization and
strategic alignment. The second step is building alternatives portfolio by the
portfolio managers taking into account the classification of projects already
completed in the first step. The third and final step enables the
identification of the alternative portfolio to consider the contribution of
projects to achieve the organization objectives as well as interactions between
projects.
",6,4
336,"  Replica placement (RP) intended at producing a set of duplicated data items
across the nodes of a distributed system in order to optimize fault tolerance,
availability, system performance load balancing. Typically, RP formulations
employ dynamic methods to change the replica placement in the system
potentially upon user request profile. Continuous Replica Placement Problem
(CRPP) is an extension of replica placement problem that takes into
consideration the current replication state of the distributed system along
with user request profile to define a new replication scheme, subject to
optimization criteria and constraints. This paper proposes an alternative
technique, named Availability Aware Continuous Replica Placement Problem
(AACRPP).AACRPP can be defined as: Given an already defined replica placement
scheme, a user request profile, and a node failure profile define a new
replication scheme, subject to optimization criteria and constraints. In this
effort we use modified greedy heuristics from the CRPP and investigated the
proposed mechanism using a trace driven java based simulation.
",3,2
337,"  Robotic process automation (RPA) is a technology for centralized automation
of business processes. RPA automates user interaction with graphical user
interfaces, whereby it promises efficiency gains and a reduction of human
negligence during process execution. To harness these benefits, organizations
face the challenge of classifying process activities as viable automation
candidates for RPA. Therefore, this work aims to support practitioners in
evaluating RPA automation candidates. We design a framework that consists of
thirteen criteria grouped into five perspectives which offer different
evaluation aspects. These criteria leverage a profound understanding of the
process step. We demonstrate and evaluate the framework by applying it to a
real-life data set.
",1,3
338,"  Neural personalized recommendation is the corner-stone of a wide collection
of cloud services and products, constituting significant compute demand of the
cloud infrastructure. Thus, improving the execution efficiency of neural
recommendation directly translates into infrastructure capacity saving. In this
paper, we devise a novel end-to-end modeling infrastructure, DeepRecInfra, that
adopts an algorithm and system co-design methodology to custom-design systems
for recommendation use cases. Leveraging the insights from the recommendation
characterization, a new dynamic scheduler, DeepRecSched, is proposed to
maximize latency-bounded throughput by taking into account characteristics of
inference query size and arrival patterns, recommendation model architectures,
and underlying hardware systems. By doing so, system throughput is doubled
across the eight industry-representative recommendation models. Finally,
design, deployment, and evaluation in at-scale production datacenter shows over
30% latency reduction across a wide variety of recommendation models running on
hundreds of machines.
",3,2
339,"  \emph{Population recovery} is the problem of learning an unknown distribution
over an unknown set of $n$-bit strings, given access to independent draws from
the distribution that have been independently corrupted according to some noise
channel. Recent work has intensively studied such problems both for the
bit-flip and erasure noise channels.
  We initiate the study of population recovery under the \emph{deletion
channel}, in which each bit is independently \emph{deleted} with some fixed
probability and the surviving bits are concatenated and transmitted. This is a
far more challenging noise model than bit-flip~noise or erasure noise; indeed,
even the simplest case in which the population size is 1 (corresponding to a
trivial distribution supported on a single string) corresponds to the
\emph{trace reconstruction} problem, a challenging problem that has received
much recent attention (see e.g.~\cite{DOS17,NP17,PZ17,HPP18,HHP18}).
  We give algorithms and lower bounds for population recovery under the
deletion channel when the population size is some $\ell>1$. As our main sample
complexity upper bound, we show that for any $\ell=o(\log n/\log \log n)$, a
population of $\ell$ strings from $\{0,1\}^n$ can be learned under deletion
channel noise using $\smash{2^{n^{1/2+o(1)}}}$ samples. On the lower bound
side, we show that $n^{\Omega(\ell)}$ samples are required to perform
population recovery under the deletion channel, for all $\ell \leq
n^{1/2-\epsilon}$.
  Our upper bounds are obtained via a robust multivariate generalization of a
polynomial-based analysis, due to Krasikov and Roddity \cite{KR97}, of how the
$k$-deck of a bit-string uniquely identifies the string; this is a very
different approach from recent algorithms for trace reconstruction (the
$\ell=1$ case). Our lower bounds build on moment-matching results of
Roos~\cite{Roo00} and Daskalakis and Papadimitriou~\cite{DP15}.
",5,2
340,"  We consider a new problem of designing a network with small $s$-$t$ effective
resistance. In this problem, we are given an undirected graph $G=(V,E)$, two
designated vertices $s,t \in V$, and a budget $k$. The goal is to choose a
subgraph of $G$ with at most $k$ edges to minimize the $s$-$t$ effective
resistance. This problem is an interpolation between the shortest path problem
and the minimum cost flow problem and has applications in electrical network
design.
  We present several algorithmic and hardness results for this problem and its
variants. On the hardness side, we show that the problem is NP-hard, and the
weighted version is hard to approximate within a factor smaller than two
assuming the small-set expansion conjecture. On the algorithmic side, we
analyze a convex programming relaxation of the problem and design a constant
factor approximation algorithm. The key of the rounding algorithm is a
randomized path-rounding procedure based on the optimality conditions and a
flow decomposition of the fractional solution. We also use dynamic programming
to obtain a fully polynomial time approximation scheme when the input graph is
a series-parallel graph, with better approximation ratio than the integrality
gap of the convex program for these graphs.
",5,3
341,"  Developers frequently reuse APIs from existing libraries to implement certain
functionality. However, learning APIs is difficult due to their large scale and
complexity. In this paper, we design an abstract framework NLI2Code to ease the
reuse process. Under the framework, users can reuse library functionalities
with a high-level, automatically-generated NLI (Natural Language Interface)
instead of the detailed API elements. The framework consists of three
components: a functional feature extractor to summarize the frequently-used
library functions in natural language form, a code pattern miner to give a code
template for each functional feature, and a synthesizer to complete code
patterns into well-typed snippets. From the perspective of a user, a reuse task
under NLI2Code starts from choosing a functional feature and our framework will
guide the user to synthesize the desired solution. We instantiated the
framework as a tool to reuse Java libraries. The evaluation shows our tool can
generate a high-quality natural language interface and save half of the coding
time for newcomers to solve real-world programming tasks.
",5,1
342,"  The proliferation of the Internet of Things (IoT) technologies have
strengthen the self-monitoring and autonomous characteristics of the sensor
networks deployed in numerous application areas. The recent developments of the
edge computing paradigms have also enabled on-site processing and managing
capabilities of sensor networks. In this paper, we introduce a system model
that enables end-to-end secure connectivity between low-power IoT devices and
UAVs, that helps to manage data processing tasks of a heterogeneous wireless
sensor networks. The performance of proposed solution is analyzed by using
simulation results. Moreover, in order to demonstrate the practical usability
of the proposed solution, the prototype implementation is presented using
commercial off-the-shelf devices.
",1,2
343,"  We consider the all pairs all shortest paths (APASP) problem, which maintains
the shortest path dag rooted at every vertex in a directed graph G=(V,E) with
positive edge weights. For this problem we present a decremental algorithm
(that supports the deletion of a vertex, or weight increases on edges incident
to a vertex). Our algorithm runs in amortized O(\vstar^2 \cdot \log n) time per
update, where n=|V|, and \vstar bounds the number of edges that lie on shortest
paths through any given vertex. Our APASP algorithm can be used for the
decremental computation of betweenness centrality (BC), a graph parameter that
is widely used in the analysis of large complex networks. No nontrivial
decremental algorithm for either problem was known prior to our work. Our
method is a generalization of the decremental algorithm of Demetrescu and
Italiano [DI04] for unique shortest paths, and for graphs with \vstar =O(n), we
match the bound in [DI04]. Thus for graphs with a constant number of shortest
paths between any pair of vertices, our algorithm maintains APASP and BC scores
in amortized time O(n^2 \log n) under decremental updates, regardless of the
number of edges in the graph.
",2,5
344,"  The growth of wireless communication technologies has been producing the
intense demand for high-speed, efficient, reliable voice & data communication.
As a result, third generation partnership project (3GPP) has implemented next
generation wireless communication technology long term evolution (LTE) which is
designed to increase the capacity and speed of existing mobile telephone & data
networks. LTE has adopted a multicarrier transmission technique known as
orthogonal frequency division multiplexing (OFDM). OFDM meets the LTE
requirement for spectrum flexibility and enables cost-efficient solutions for
very wide carriers. One major generic problem of OFDM technique is high peak to
average power ratio (PAPR) which is defined as the ratio of the peak power to
the average power of the OFDM signal. A trade-off is necessary for reducing
PAPR with increasing bit error rate (BER), computational complexity or data
rate loss etc. In this paper, two clipping based filtering methods have been
implemented & also analyzed their modulation effects on reducing PAPR.
",2,5
345,"  In this paper, we propose a joint architecture that captures language, rhyme
and meter for sonnet modelling. We assess the quality of generated poems using
crowd and expert judgements. The stress and rhyme models perform very well, as
generated poems are largely indistinguishable from human-written poems. Expert
evaluation, however, reveals that a vanilla language model captures meter
implicitly, and that machine-generated poems still underperform in terms of
readability and emotion. Our research shows the importance expert evaluation
for poetry generation, and that future research should look beyond rhyme/meter
and focus on poetic language.
",4,3
346,"  We show that over the class of linear orders with additional binary relations
satisfying some monotonicity conditions, monadic first-order logic has the
three-variable property. This generalizes (and gives a new proof of) several
known results, including the fact that monadic first-order logic has the
three-variable property over linear orders, as well as over (R,<,+1), and
answers some open questions mentioned in a paper from Antonopoulos, Hunter,
Raza and Worrell [FoSSaCS 2015]. Our proof is based on a translation of monadic
first-order logic formulas into formulas of a star-free variant of
Propositional Dynamic Logic, which are in turn easily expressible in monadic
first-order logic with three variables.
",0,6
347,"  The EL family of Description Logics (DLs) has been the subject of interest in
recent years. On the one hand, these DLs are tractable, but fairly
inexpressive. On the other hand, these DLs can be used for designing different
classes of ontologies, most notably ontologies from the medical domain.
Unfortunately, building ontologies is error-prone. As a result, inferable
subsumption relations among concepts may be unintended. In recent years, the
problem of axiom pinpointing has been studied with the purpose of providing
minimal sets of axioms that explain unintended subsumption relations. For the
concrete case of EL and EL+, the most efficient approaches consist of encoding
the problem into propositional logic, specifically as a Horn formula, which is
then analyzed with a dedicated algorithm. This paper builds on this earlier
work, but exploits the important relationship between minimal axioms sets and
minimal unsatisfiable subformulas in the propositional domain. In turn, this
relationship allows applying a vast body of recent work in the propositional
domain to the concrete case of axiom pinpointing for EL and its variants. From
a practical perspective, the algorithms described in this paper are often
several orders of magnitude more efficient that the current state of the art in
axiom pinpointing for the EL family of DLs.
",5,1
348,"  This paper presents the mechanization of a process algebra for Mobile Ad hoc
Networks and Wireless Mesh Networks, and the development of a compositional
framework for proving invariant properties. Mechanizing the core process
algebra in Isabelle/HOL is relatively standard, but its layered structure
necessitates special treatment. The control states of reactive processes, such
as nodes in a network, are modelled by terms of the process algebra. We propose
a technique based on these terms to streamline proofs of inductive invariance.
This is not sufficient, however, to state and prove invariants that relate
states across multiple processes (entire networks). To this end, we propose a
novel compositional technique for lifting global invariants stated at the level
of individual nodes to networks of nodes.
",5,1
349,"  With the worldwide third-generation mobile communication system gradually
implemented, the future development of mobile communications has become a hot
topic and evolution of the problem. This paper introduces the fourth generation
mobile communication system and its performance and network structure and OFDM,
software defined radio, smart antennas, IPv6 and other key technologies, and
analyzes the relationship between 4G mobile communication system for mobile
communications and 3G, and the evolution of communication systems do Prospect.
",2,3
350,"  Symbolic quick error detection (SQED) is a formal pre-silicon verification
technique targeted at processor designs. It leverages bounded model checking
(BMC) to check a design for counterexamples to a self-consistency property:
given the instruction set architecture (ISA) of the design, executing an
instruction sequence twice on the same inputs must always produce the same
outputs. Self-consistency is a universal, design-independent property.
Consequently, in contrast to traditional verification approaches that use
design-specific assertions (often generated manually), SQED does not require a
formal design specification or manually-written properties. Case studies have
shown that SQED is effective on commercial designs and substantially improves
design productivity. However, until now there has been no formal
characterization of its bug-finding capabilities. We aim to close this gap by
laying a formal foundation for SQED. We use a transition-system processor model
and define the notion of a bug using an abstract specification relation. We
prove the soundness of SQED, namely that any bug reported by SQED is in fact a
real bug in the processor. Importantly, this result holds regardless of what
the actual specification relation is. We next describe conditions under which
SQED is complete, that is, what kinds of bugs it is guaranteed to find. We show
that for a large class of bugs, SQED can always find a trace exhibiting the
bug. Our results enable a rigorous understanding of SQED and its bug-finding
capabilities and give insights on how to optimize implementations of SQED in
practice.
",2,3
351,"  Generative models defining joint distributions over parse trees and sentences
are useful for parsing and language modeling, but impose restrictions on the
scope of features and are often outperformed by discriminative models. We
propose a framework for parsing and language modeling which marries a
generative model with a discriminative recognition model in an encoder-decoder
setting. We provide interpretations of the framework based on expectation
maximization and variational inference, and show that it enables parsing and
language modeling within a single implementation. On the English Penn
Treen-bank, our framework obtains competitive performance on constituency
parsing while matching the state-of-the-art single-model language modeling
score.
",2,1
352,"  The Bitcoin cryptocurrency records its transactions in a public log called
the blockchain. Its security rests critically on the distributed protocol that
maintains the blockchain, run by participants called miners. Conventional
wisdom asserts that the protocol is incentive-compatible and secure against
colluding minority groups, i.e., it incentivizes miners to follow the protocol
as prescribed.
  We show that the Bitcoin protocol is not incentive-compatible. We present an
attack with which colluding miners obtain a revenue larger than their fair
share. This attack can have significant consequences for Bitcoin: Rational
miners will prefer to join the selfish miners, and the colluding group will
increase in size until it becomes a majority. At this point, the Bitcoin system
ceases to be a decentralized currency.
  Selfish mining is feasible for any group size of colluding miners. We propose
a practical modification to the Bitcoin protocol that protects against selfish
mining pools that command less than 1/4 of the resources. This threshold is
lower than the wrongly assumed 1/2 bound, but better than the current reality
where a group of any size can compromise the system.
",2,1
353,"  Deadlock and nondeterminism may become increasingly hard to detect in
concurrent and distributed systems. UML activity diagrams are flowcharts that
model sequential and concurrent behavior. Although the UML community widely
adopts such diagrams, there is no standard approach to verify the presence of
deadlock and nondeterministic behavior in activity diagrams. Nondeterminism is
usually neglected in the literature even though it may be considered a very
relevant property. This work proposes a framework for the automatic
verification of deadlock and nondeterminism in UML activity diagrams. It
introduces a compositional CSP semantics for activity diagrams that is used to
automatically generate CSP specifications from UML models. These specifications
are the input for the automatic verification of deadlock and nondeterministic
behavior using the FDR refinement checker. We propose a plugin for the Astah
modeling environment that mechanizes the translation process, and that calls
FDR in the background to perform the verification of properties. The tool keeps
the traceability between a diagram and its CSP specification. It parses the FDR
results to highlight the diagram paths that lead to a deadlock or a
nondeterministic behavior. This framework adds verification capabilities to the
UML modeling tool and keeps the formal semantics transparent to the users.
Therefore, the user does not need to understand or manipulate formal notations
during modeling. We present the results of a case study that applies the
proposed framework for the verification of models in the domain of cloud
computing. We discuss future applications due to the potential of our approach.
",6,4
354,"  As scientific discovery becomes increasingly data-driven, software platforms
are needed to efficiently organize and disseminate data from disparate sources.
This is certainly the case in the field of materials science. For example,
Materials Project has generated computational data on over 60,000 chemical
compounds and has made that data available through a web portal and REST
interface. However, such portals must seek to incorporate community submissions
to expand the scope of scientific data sharing. In this paper, we describe
MPContribs, a computing/software infrastructure to integrate and organize
contributions of simulated or measured materials data from users. Our solution
supports complex submissions and provides interfaces that allow contributors to
share analyses and graphs. A RESTful API exposes mechanisms for book-keeping,
retrieval and aggregation of submitted entries, as well as persistent URIs or
DOIs that can be used to reference the data in publications. Our approach
isolates contributed data from a host project's quality-controlled core data
and yet enables analyses across the entire dataset, programmatically or through
customized web apps. We expect the developed framework to enhance collaborative
determination of material properties and to maximize the impact of each
contributor's dataset. In the long-term, MPContribs seeks to make Materials
Project an institutional, and thus community-wide, memory for computational and
experimental materials science.
",5,2
355,"  Much mathematical writing exists that is, explicitly or implicitly, based on
set theory, often Zermelo-Fraenkel set theory (ZF) or one of its variants. In
ZF, the domain of discourse contains only sets, and hence every mathematical
object must be a set. Consequently, in ZF, with the usual encoding of an
ordered pair ${\langle a, b\rangle}$, formulas like ${\{a\} \in \langle a, b
\rangle}$ have truth values, and operations like ${\mathcal P (\langle a,
b\rangle)}$ have results that are sets. Such 'accidental theorems' do not match
how people think about the mathematics and also cause practical difficulties
when using set theory in machine-assisted theorem proving. In contrast, in a
number of proof assistants, mathematical objects and concepts can be built of
type-theoretic stuff so that many mathematical objects can be, in essence,
terms of an extended typed ${\lambda}$-calculus. However, dilemmas and
frustration arise when formalizing mathematics in type theory.
  Motivated by problems of formalizing mathematics with (1) purely
set-theoretic and (2) type-theoretic approaches, we explore an option with much
of the flexibility of set theory and some of the useful features of type
theory. We present ZFP: a modification of ZF that has ordered pairs as
primitive, non-set objects. ZFP has a more natural and abstract axiomatic
definition of ordered pairs free of any notion of representation. This paper
presents axioms for ZFP, and a proof in ZF (machine-checked in Isabelle/ZF) of
the existence of a model for ZFP, which implies that ZFP is consistent if ZF
is. We discuss the approach used to add this abstraction barrier to ZF.
",4,6
356,"  This paper lays down a proposal of protocol named DREAM_OLSR. The protocol
has been developed so as to effect current OLSR (RFC 3626) [4] protocol. The
protocol establishes an optimized solution hence the name has been manipulated
from Open Link State Routing to DREAM Optimized Link State Routing. DREAM
specifies Distance Routing Effective Algorithm for Mobility wherein it
implements the Distance routing effective algorithm for the optimized solution.
This optimization includes higher efficiency and fewer overheads for the MANET.
",2,5
357,"  We consider the $k$-Center problem and some generalizations. For $k$-Center a
set of $k$ center vertices needs to be found in a graph $G$ with edge lengths,
such that the distance from any vertex of $G$ to its nearest center is
minimized. This problem naturally occurs in transportation networks, and
therefore we model the inputs as graphs with bounded highway dimension, as
proposed by Abraham et al. [SODA 2010].
  We show both approximation and fixed-parameter hardness results, and how to
overcome them using fixed-parameter approximations, where the two paradigms are
combined. In particular, we prove that for any $\varepsilon>0$ computing a
$(2-\varepsilon)$-approximation is W[2]-hard for parameter $k$ and NP-hard for
graphs with highway dimension $O(\log^2 n)$. The latter does not rule out
fixed-parameter $(2-\varepsilon)$-approximations for the highway dimension
parameter $h$, but implies that such an algorithm must have at least doubly
exponential running time in $h$ if it exists, unless the ETH fails. On the
positive side, we show how to get below the approximation factor of $2$ by
combining the parameters $k$ and $h$: we develop a fixed-parameter
$3/2$-approximation with running time $2^{O(kh\log h)}\cdot n^{O(1)}$.
Additionally we prove that, unless P=NP, our techniques cannot be used to
compute fixed-parameter $(2-\varepsilon)$-approximations for only the parameter
$h$.
  We also provide similar fixed-parameter approximations for the weighted
$k$-Center and $(k,\mathcal{F})$-Partition problems, which generalize
$k$-Center.
",2,3
358,"  The future of computation is the Graphical Processing Unit, i.e. the GPU. The
promise that the graphics cards have shown in the field of image processing and
accelerated rendering of 3D scenes, and the computational capability that these
GPUs possess, they are developing into great parallel computing units. It is
quite simple to program a graphics processor to perform general parallel tasks.
But after understanding the various architectural aspects of the graphics
processor, it can be used to perform other taxing tasks as well. In this paper,
we will show how CUDA can fully utilize the tremendous power of these GPUs.
CUDA is NVIDIA's parallel computing architecture. It enables dramatic increases
in computing performance, by harnessing the power of the GPU. This paper talks
about CUDA and its architecture. It takes us through a comparison of CUDA C/C++
with other parallel programming languages like OpenCL and DirectCompute. The
paper also lists out the common myths about CUDA and how the future seems to be
promising for CUDA.
",2,6
359,"  With the increasing number of Quad-Core-based clusters and the introduction
of compute nodes designed with large memory capacity shared by multiple cores,
new problems related to scalability arise. In this paper, we analyze the
overall performance of a cluster built with nodes having a dual Quad-Core
Processor on each node. Some benchmark results are presented and some
observations are mentioned when handling such processors on a benchmark test. A
Quad-Core-based cluster's complexity arises from the fact that both local
communication and network communications between the running processes need to
be addressed. The potentials of an MPI-OpenMP approach are pinpointed because
of its reduced communication overhead. At the end, we come to a conclusion that
an MPI-OpenMP solution should be considered in such clusters since optimizing
network communications between nodes is as important as optimizing local
communications between processors in a multi-core cluster.
",4,2
360,"  Longest common subsequence (LCS) is one of the most fundamental problems in
combinatorial optimization. Apart from theoretical importance, LCS has enormous
applications in bioinformatics, revision control systems, and data comparison
programs. Although a simple dynamic program computes LCS in quadratic time, it
has been recently proven that the problem admits a conditional lower bound and
may not be solved in truly subquadratic time. In addition to this, LCS is
notoriously hard with respect to approximation algorithms. Apart from a trivial
sampling technique that obtains a $n^{x}$ approximation solution in time
$O(n^{2-2x})$ nothing else is known for LCS. This is in sharp contrast to its
dual problem edit distance for which several linear time solutions are obtained
in the past two decades.
",2,5
361,"  A wide array of dynamic bandwidth allocation (DBA) mechanisms have recently
been proposed for improving bandwidth utilization and reducing idle times and
packets delays in passive optical networks (PONs). The DBA evaluation studies
commonly assumed that the report message for communicating the bandwidth
demands of the distributed optical network units (ONUs) to the central optical
line terminal (OLT) is scheduled for the end of an ONU's upstream transmission,
after the ONU's payload data transmissions. In this article, we conduct a
detailed investigation of the impact of the report message scheduling (RMS),
either at the beginning (i.e., before the pay load data) or the end of an ONU
upstream transmission on PON performance. We analytically characterize the
reduction in channel idle time with reporting at the beginning of an upstream
transmission compared to reporting at the end. Our extensive simulation
experiments consider both the Ethernet Passive Optical Networking (EPON)
standard and the Gigabit PON (GPON) standard. We find that for DBAs with
offline sizing and scheduling of ONU upstream transmission grants at the end of
a polling cycle, which processes requests from all ONUs, reporting at the
beginning gives substantial reductions of mean packet delay at high loads. For
high-performing DBAs with online grant sizing and scheduling, which immediately
processes individual ONU requests, or interleaving of ONUs groups, both
reporting at the beginning or end give essentially the same average packet
delays.
",3,2
362,"  For the tree topology, previous studies show the maximum likelihood estimate
(MLE) of a link/path takes a polynomial form with a degree that is one less
than the number of descendants connected to the link/path. Since then, the main
concern is focused on searching for methods to solve the high degree polynomial
without using iterative approximation. An explicit estimator based on the Law
of Large Numbers has been proposed to speed up the estimation. However, the
estimate obtained from the estimator is not a MLE. When $n<\infty$, the
estimate may be noticeable different from the MLE. To overcome this, an
explicit MLE estimator is presented in this paper and a comparison between the
MLE estimator and the explicit estimator proposed previously is presented to
unveil the insight of the MLE estimator and point out the pitfall of the
previous one.
",2,3
363,"  In this paper, we investigate a distributed maximal independent set (MIS)
reconfiguration problem, in which there are two maximal independent sets for
which every node is given its membership status, and the nodes need to
communicate with their neighbors in order to find a reconfiguration schedule
that switches from the first MIS to the second. Such a schedule is a list of
independent sets that is restricted by forbidding two neighbors to change their
membership status at the same step. In addition, these independent sets should
provide some covering guarantee. We show that obtaining an actual MIS (and even
a 3-dominating set) in each intermediate step is impossible. However, we
provide efficient solutions when the intermediate sets are only required to be
independent and 4-dominating, which is almost always possible, as we fully
characterize. Consequently, our goal is to pin down the tradeoff between the
possible length of the schedule and the number of communication rounds. We
prove that a constant length schedule can be found in
$O(\texttt{MIS}+\texttt{R32})$ rounds, where $\texttt{MIS}$ is the complexity
of finding an MIS in a worst-case graph and $\texttt{R32}$ is the complexity of
finding a $(3,2)$-ruling set. For bounded degree graphs, this is $O(\log^*n)$
rounds and we show that it is necessary. On the other extreme, we show that
with a constant number of rounds we can find a linear length schedule.
",2,3
364,"  Word embedding is a key component in many downstream applications in
processing natural languages. Existing approaches often assume the existence of
a large collection of text for learning effective word embedding. However, such
a corpus may not be available for some low-resource languages. In this paper,
we study how to effectively learn a word embedding model on a corpus with only
a few million tokens. In such a situation, the co-occurrence matrix is sparse
as the co-occurrences of many word pairs are unobserved. In contrast to
existing approaches often only sample a few unobserved word pairs as negative
samples, we argue that the zero entries in the co-occurrence matrix also
provide valuable information. We then design a Positive-Unlabeled Learning
(PU-Learning) approach to factorize the co-occurrence matrix and validate the
proposed approaches in four different languages.
",1,5
365,"  Current cloud services are moving away from monolithic designs and towards
graphs of many loosely-coupled, single-concerned microservices. Microservices
have several advantages, including speeding up development and deployment,
allowing specialization of the software infrastructure, and helping with
debugging and error isolation. At the same time they introduce several hardware
and software challenges. Given that most of the performance and efficiency
implications of microservices happen at scales larger than what is available
outside production deployments, studying such effects requires designing the
right simulation infrastructures.
  We present uqSim, a scalable and validated queueing network simulator
designed specifically for interactive microservices. uqSim provides detailed
intra- and inter-microservice models that allow it to faithfully reproduce the
behavior of complex, many-tier applications. uqSim is also modular, allowing
reuse of individual models across microservices and end-to-end applications. We
have validated uqSim both against simple and more complex microservices graphs,
and have shown that it accurately captures performance in terms of throughput
and tail latency. Finally, we use uqSim to model the tail at scale effects of
request fanout, and the performance impact of power management in
latency-sensitive microservices.
",2,3
366,"  Blockchain, which is a technology for distributedly managing ledger
information over multiple nodes without a centralized system, has elicited
increasing attention. Performing experiments on actual blockchains are
difficult because a large number of nodes in wide areas are necessary. In this
study, we developed a blockchain network simulator SimBlock for such
experiments. Unlike the existing simulators, SimBlock can easily change
behavior of node, so that it enables to investigate the influence of nodes'
behavior on blockchains. We compared some simulation results with the measured
values in actual blockchains to demonstrate the validity of this simulator.
Furthermore, to show practical usage, we conducted two experiments which
clarify the influence of neighbor node selection algorithms and relay networks
on the block propagation time. The simulator could depict the effects of the
two techniques on block propagation time. The simulator will be publicly
available in a few months.
",1,5
367,"  Multi-user spatial multiplexing combined with packet aggregation can
significantly increase the performance of Wireless Local Area Networks (WLANs).
In this letter, we present and evaluate a simple technique to perform packet
aggregation in IEEE 802.11ac MU-MIMO (Multi-user Multiple Input Multiple
Output) WLANs. Results show that in non-saturation conditions both the number
of active stations (STAs) and the queue size have a significant impact on the
system performance. If the number of stations is excessively high, the
heterogeneity of destinations in the packets contained in the queue makes it
difficult to take full advantage of packet aggregation. This effect can be
alleviated by increasing the queue size, which increases the chances to
schedule a large number of packets at each transmission, hence improving the
system throughput at the cost of a higher delay.
",2,5
368,"  RSL19BD (Waseda University Sakai Laboratory) participated in the Fourth
Dialogue Breakdown Detection Challenge (DBDC4) and submitted five runs to both
English and Japanese subtasks. In these runs, we utilise the Decision
Tree-based model and the Long Short-Term Memory-based (LSTM-based) model
following the approaches of RSL17BD and KTH in the Third Dialogue Breakdown
Detection Challenge (DBDC3) respectively. The Decision Tree-based model follows
the approach of RSL17BD but utilises RandomForestRegressor instead of
ExtraTreesRegressor. In addition, instead of predicting the mean and the
variance of the probability distribution of the three breakdown labels, it
predicts the probability of each label directly. The LSTM-based model follows
the approach of KTH with some changes in the architecture and utilises
Convolutional Neural Network (CNN) to perform text feature extraction. In
addition, instead of targeting the single breakdown label and minimising the
categorical cross entropy loss, it targets the probability distribution of the
three breakdown labels and minimises the mean squared error. Run 1 utilises a
Decision Tree-based model; Run 2 utilises an LSTM-based model; Run 3 performs
an ensemble of 5 LSTM-based models; Run 4 performs an ensemble of Run 1 and Run
2; Run 5 performs an ensemble of Run 1 and Run 3. Run 5 statistically
significantly outperformed all other runs in terms of MSE (NB, PB, B) for the
English data and all other runs except Run 4 in terms of MSE (NB, PB, B) for
the Japanese data (alpha level = 0.05).
",2,3
369,"  Among the approximation methods for the verification of counter systems, one
of them consists in model-checking their flat unfoldings. Unfortunately, the
complexity characterization of model-checking problems for such operational
models is not always well studied except for reachability queries or for Past
LTL. In this paper, we characterize the complexity of model-checking problems
on flat counter systems for the specification languages including first-order
logic, linear mu-calculus, infinite automata, and related formalisms. Our
results span different complexity classes (mainly from PTime to PSpace) and
they apply to languages in which arithmetical constraints on counter values are
systematically allowed. As far as the proof techniques are concerned, we
provide a uniform approach that focuses on the main issues.
",6,4
370,"  In this paper, we present a method for adversarial decomposition of text
representation. This method can be used to decompose a representation of an
input sentence into several independent vectors, each of them responsible for a
specific aspect of the input sentence. We evaluate the proposed method on two
case studies: the conversion between different social registers and diachronic
language change. We show that the proposed method is capable of fine-grained
controlled change of these aspects of the input sentence. It is also learning a
continuous (rather than categorical) representation of the style of the
sentence, which is more linguistically realistic. The model uses
adversarial-motivational training and includes a special motivational loss,
which acts opposite to the discriminator and encourages a better decomposition.
Furthermore, we evaluate the obtained meaning embeddings on a downstream task
of paraphrase detection and show that they significantly outperform the
embeddings of a regular autoencoder.
",4,3
371,"  This paper focuses on the study of recognizing discontiguous entities.
Motivated by a previous work, we propose to use a novel hypergraph
representation to jointly encode discontiguous entities of unbounded length,
which can overlap with one another. To compare with existing approaches, we
first formally introduce the notion of model ambiguity, which defines the
difficulty level of interpreting the outputs of a model, and then formally
analyze the theoretical advantages of our model over previous existing
approaches based on linear-chain CRFs. Our empirical results also show that our
model is able to achieve significantly better results when evaluated on
standard data with many discontiguous entities.
",1,3
372,"  Read-only caches are widely used in cloud infrastructures to reduce access
latency and load on backend databases. Operators view coherent caches as
impractical at genuinely large scale and many client-facing caches are updated
in an asynchronous manner with best-effort pipelines. Existing solutions that
support cache consistency are inapplicable to this scenario since they require
a round trip to the database on every cache transaction.
  Existing incoherent cache technologies are oblivious to transactional data
access, even if the backend database supports transactions. We propose T-Cache,
a novel caching policy for read-only transactions in which inconsistency is
tolerable (won't cause safety violations) but undesirable (has a cost). T-Cache
improves cache consistency despite asynchronous and unreliable communication
between the cache and the database. We define cache-serializability, a variant
of serializability that is suitable for incoherent caches, and prove that with
unbounded resources T-Cache implements this new specification. With limited
resources, T-Cache allows the system manager to choose a trade-off between
performance and consistency.
  Our evaluation shows that T-Cache detects many inconsistencies with only
nominal overhead. We use synthetic workloads to demonstrate the efficacy of
T-Cache when data accesses are clustered and its adaptive reaction to workload
changes. With workloads based on the real-world topologies, T-Cache detects
43-70% of the inconsistencies and increases the rate of consistent transactions
by 33-58%.
",3,2
373,"  In this paper syntactic objects---concept constructors called part
restrictions which realize rational grading are considered in Description
Logics (DLs). Being able to convey statements about a rational part of a set of
successors, part restrictions essentially enrich the expressive capabilities of
DLs. We examine an extension of well-studied DL ALCQIHR+ with part
restrictions, and prove that the reasoning in the extended logic is still
decidable. The proof uses tableaux technique augmented with indices technique,
designed for dealing with part restrictions.
",5,1
374,"  Shared research infrastructure that is globally distributed and widely
accessible has been a hallmark of the networking community. This paper presents
an initial snapshot of a vision for a possible future of mid-scale distributed
research infrastructure aimed at enabling new types of research and
discoveries. The paper is written from the perspective of ""lessons learned"" in
constructing and operating the Global Environment for Network Innovations
(GENI) infrastructure and attempts to project future concepts and solutions
based on these lessons. The goal of this paper is to engage the community to
contribute new ideas and to inform funding agencies about future research
directions to realize this vision.
",2,6
375,"  This paper presents our approach to the quantitative modeling and analysis of
highly (re)configurable systems, such as software product lines. Different
combinations of the optional features of such a system give rise to
combinatorially many individual system variants. We use a formal modeling
language that allows us to model systems with probabilistic behavior, possibly
subject to quantitative feature constraints, and able to dynamically install,
remove or replace features. More precisely, our models are defined in the
probabilistic feature-oriented language QFLAN, a rich domain specific language
(DSL) for systems with variability defined in terms of features. QFLAN
specifications are automatically encoded in terms of a process algebra whose
operational behavior interacts with a store of constraints, and hence allows to
separate system configuration from system behavior. The resulting probabilistic
configurations and behavior converge seamlessly in a semantics based on
discrete-time Markov chains, thus enabling quantitative analysis. Our analysis
is based on statistical model checking techniques, which allow us to scale to
larger models with respect to precise probabilistic analysis techniques. The
analyses we can conduct range from the likelihood of specific behavior to the
expected average cost, in terms of feature attributes, of specific system
variants. Our approach is supported by a novel Eclipse-based tool which
includes state-of-the-art DSL utilities for QFLAN based on the Xtext framework
as well as analysis plug-ins to seamlessly run statistical model checking
analyses. We provide a number of case studies that have driven and validated
the development of our framework.
",2,5
376,"  In this paper, we study the tradeoff between the approximation guarantee and
adaptivity for the problem of maximizing a monotone submodular function subject
to a cardinality constraint. The adaptivity of an algorithm is the number of
sequential rounds of queries it makes to the evaluation oracle of the function,
where in every round the algorithm is allowed to make polynomially-many
parallel queries. Adaptivity is an important consideration in settings where
the objective function is estimated using samples and in applications where
adaptivity is the main running time bottleneck. Previous algorithms achieving a
nearly-optimal $1 - 1/e - \epsilon$ approximation require $\Omega(n)$ rounds of
adaptivity. In this work, we give the first algorithm that achieves a $1 - 1/e
- \epsilon$ approximation using $O(\ln{n} / \epsilon^2)$ rounds of adaptivity.
The number of function evaluations and additional running time of the algorithm
are $O(n \mathrm{poly}(\log{n}, 1/\epsilon))$.
",2,1
377,"  Symbolic execution has shown its ability to find security-relevant flaws in
software, but faces significant scalability challenges. There is a commonly
held belief that manual intervention by an expert can help alleviate these
limiting factors. However, there has been little formal investigation of this
idea. In this paper, we present our experiences applying the KLEE symbolic
execution engine to a new bug corpus, and of using manual intervention to
alleviate the issues encountered. Our contributions are (1) Hemiptera, a novel
corpus of over 130 bugs in real world software, (2) a comprehensive evaluation
of the KLEE symbolic execution engine on Hemiptera with a categorisation of
frequently occurring software patterns that are problematic for symbolic
execution, and (3) an evaluation of manual mitigations aimed at addressing the
underlying issues of symbolic execution. Our experience shows that manual
intervention can increase both code coverage and bug detection in many
situations. It is not a silver bullet however, and we discuss its limitations
and the challenges encountered.
",5,1
378,"  RFID has been regarded as a time and money-saving solution for a wide variety
of applications, such as manufacturing, supply chain management, and inventory
control, etc. However, there are some security problems on RFID in the product
managements. The most concerned issues are the tracking and the location
privacy. Numerous scholars tried to solve these problems, but their proposals
do not include the after-sales service. In this paper, we propose a purchase
and after-sales service RFID scheme for shopping mall. The location privacy,
confidentiality, data integrity, and some security protection are hold in this
propose mechanism.
",2,5
379,"  Traditional chatbots usually need a mass of human dialogue data, especially
when using supervised machine learning method. Though they can easily deal with
single-turn question answering, for multi-turn the performance is usually
unsatisfactory. In this paper, we present Lingke, an information retrieval
augmented chatbot which is able to answer questions based on given product
introduction document and deal with multi-turn conversations. We will introduce
a fine-grained pipeline processing to distill responses based on unstructured
documents, and attentive sequential context-response matching for multi-turn
conversations.
",2,5
380,"  The success of many natural language processing (NLP) tasks is bound by the
number and quality of annotated data, but there is often a shortage of such
training data. In this paper, we ask the question: ""Can we combine a neural
network (NN) with regular expressions (RE) to improve supervised learning for
NLP?"". In answer, we develop novel methods to exploit the rich expressiveness
of REs at different levels within a NN, showing that the combination
significantly enhances the learning effectiveness when a small number of
training examples are available. We evaluate our approach by applying it to
spoken language understanding for intent detection and slot filling.
Experimental results show that our approach is highly effective in exploiting
the available training data, giving a clear boost to the RE-unaware NN.
",4,2
381,"  This article defines the QoS-guaranteed efficient cloudlet deploy problem in
wireless metropolitan area network, which aims to minimize the average access
delay of mobile users i.e. the average delay when service requests are
successfully sent and being served by cloudlets. Meanwhile, we try to optimize
total deploy cost represented by the total number of deployed cloudlets. For
the first target, both un-designated capacity and constrained capacity cases
are studied, and we have designed efficient heuristic and clustering algorithms
respectively. We show our algorithms are more efficient than the existing
algorithm. For the second target, we formulate an integer linear programming to
minimize the number of used cloudlets with given average access delay
requirement. A clustering algorithm is devised to guarantee the scalability.
For a special case of the deploy cost optimization problem where all cloudlets'
computing capabilities have been given, i.e., designated capacity, a minimal
cloudlets efficient heuristic algorithm is further proposed. We finally
evaluate the performance of proposed algorithms through extensive experimental
simulations. Simulation results demonstrate the proposed algorithms are more
than 46% efficient than existing algorithms on the average cloudlet access
delay. Compared with existing algorithms, our proposed clustering and heuristic
algorithms can reduce the number of deployed cloudlets by about 50% averagely.
",5,6
382,"  Concurrent software for engineering computations consists of multiple
cooperating modules. The behavior of individual modules is described by means
on state diagrams. In the paper, the constraints on state diagrams are
proposed, allowing for the specification of designer's intentions as to the
synchronization of modules. Also, the translation of state diagrams (with
enforcement constraints) into Concurrent State Machines is shown, which
provides formal framework for the verification of inter-module synchronization.
An example of engineering software design based on the method is presented.
",1,4
383,"  This paper describes a new open domain dialogue system Alquist developed as
part of the Alexa Prize competition for the Amazon Echo line of products. The
Alquist dialogue system is designed to conduct a coherent and engaging
conversation on popular topics. We are presenting a hybrid system combining
several machine learning and rule based approaches. We discuss and describe the
Alquist pipeline, data acquisition, and processing, dialogue manager, NLG,
knowledge aggregation and hierarchy of sub-dialogs. We present some of the
experimental results.
",6,3
384,"  Due to the rapid development of the Internet in recent years, the need to
find new tools to reinforce trust and security through the Internet has became
a major concern. The discovery of new pseudo-random number generators with a
strong level of security is thus becoming a hot topic, because numerous
cryptosystems and data hiding schemes are directly dependent on the quality of
these generators. At the conference Internet`09, we have described a generator
based on chaotic iterations, which behaves chaotically as defined by Devaney.
In this paper, the proposal is to improve the speed and the security of this
generator, to make its use more relevant in the Internet security context. To
do so, a comparative study between various generators is carried out and
statistical results are given. Finally, an application in the information
hiding framework is presented, to give an illustrative example of the use of
such a generator in the Internet security field.
",3,2
385,"  Network Function Virtualization (NFV) has recently received significant
attention as an innovative way of deploying network services. By decoupling
network functions from the physical equipment on which they run, NFV has been
proposed as passage towards service agility, better time-to-market, and reduced
Capital Expenses (CAPEX) and Operating Expenses (OPEX). One of the main selling
points of NFV is its promise for better energy efficiency resulting from
consolidation of resources as well as their more dynamic utilization. However,
there are currently no studies or implementations which attach values to energy
savings that can be expected, which could make it hard for Telecommunication
Service Providers (TSPs) to make investment decisions. In this paper, we
utilize Bell Labs' GWATT tool to estimate the energy savings that could result
from the three main NFV use cases Virtualized Evolved Packet Core (VEPC),
Virtualized Customer Premises Equipment (VCPE) and Virtualized Radio Access
Network (VRAN). We determine that the part of the mobile network with the
highest energy utilization prospects is the Evolved Packet Core (EPC) where
virtualization of functions leads to a 22% reduction in energy consumption and
a 32% enhancement in energy efficiency.
",1,5
386,"  In this paper, we present FASE (Faster Asynchronous Systems Evaluation), a
tool for evaluating the worst-case efficiency of asynchronous systems. The tool
is based on some well-established results in the setting of a timed process
algebra (PAFAS: a Process Algebra for Faster Asynchronous Systems). To show the
applicability of FASE to concrete meaningful examples, we consider three
implementations of a bounded buffer and use FASE to automatically evaluate
their worst-case efficiency. We finally contrast our results with previous ones
where the efficiency of the same implementations has already been considered.
",3,2
387,"  Travelling Salesman Problem (TSP) is one of the unsolved problems in computer
science. TSP is NP Hard. Till now the best approximation ratio found for
symmetric TSP is three by two by Christofides Algorithm more than thirty years
ago. There are different approaches to solve this problem. These range from
methods based on neural networks, genetic algorithm, swarm optimization, ant
colony optimization etc. The bound is further reduced from three by two but for
graphic TSP. A factor of thirteen by nine was found for Graphic TSP. A newly
proposed heuristic called k RNN is considered here. It seems from experimental
results that five by four is the approximation ratio. A performance analysis is
done for this heuristic and it confirms experimental bound of five by four.
",5,3
388,"  We analyze some of the fundamental design challenges that impact the
development of a multilingual state-of-the-art named entity transliteration
system, including curating bilingual named entity datasets and evaluation of
multiple transliteration methods. We empirically evaluate the transliteration
task using traditional weighted finite state transducer (WFST) approach against
two neural approaches: the encoder-decoder recurrent neural network method and
the recent, non-sequential Transformer method. In order to improve availability
of bilingual named entity transliteration datasets, we release personal name
bilingual dictionaries minded from Wikidata for English to Russian, Hebrew,
Arabic and Japanese Katakana. Our code and dictionaries are publicly available.
",4,3
389,"  Conflict-free replicated data types (CRDTs) are a natural structure with
which to communicate information about a shared computation in a distributed
setting where coordination overhead may not be tolerated, and individual
participants are allowed to temporarily diverge from the overall computation.
Within this setting, there are two classical approaches: state- and
operation-based CRDTs. The former define a commutative, associative, and
idempotent join operation, and their states a monotone join semi-lattice.
State-based CRDTs may be further distinguished into classical- and
$\delta$-state CRDTs. The former communicate their full state after each
update, whereas the latter communicate only the changed state. Op-based CRDTs
communicate operations (not state), thus making their updates non-idempotent.
Whereas op-based CRDTs require little information to be exchanged, they demand
relatively strong network guarantees (exactly-once message delivery), and
state-based CRDTs suffer the opposite problem. Both satisfy strong eventual
consistency (SEC).
  We posit that $\delta$-state CRDTs both (1) require less communication
overhead from payload size, and (2) tolerate relatively weak network
environments, making them an ideal candidate for real-world use of CRDTs. Our
central intuition is a pair of reductions between state-, $\delta$-state, and
op-based CRDTs. We formalize this intuition in the Isabelle interactive theorem
prover and show that state-based CRDTs achieve SEC. We present a relaxed
network model in Isabelle and show that state-based CRDTs still maintain SEC.
Finally, we extend our work to show that $\delta$-state CRDTs maintain SEC when
only communicating $\delta$-state fragments, even under relatively weak network
conditions.
",2,6
390,"  Transmission Control Protocol (TCP) carries most of the traffic on the
Internet these days. There are several implementations of TCP, and the most
important difference among them is their mechanism for controlling congestion.
One of the methods for determining type of a TCP is active probing. Active
probing considers a TCP implementation as a black box, sends different streams
of data to the appropriate host. According to the response received from the
host, it figures out the type of TCP version implemented.
  TCP Behavior Inference Tool (TBIT) is an implemented tool that uses active
probing to check the running TCP on web servers. It can check several aspects
of the running TCP including initial value of congestion window, congestion
control algorithm, conformant congestion control, response to selective
acknowledgment, response to Explicit Congestion Notification (ECN) and time
wait duration. In this paper we focus on congestion control algorithm aspect of
it, explain the mechanism used by TBIT and present the results.
",5,2
391,"  The flow accumulation problem for grid terrains takes as input a matrix of
flow directions, that specifies for each cell of the grid to which of its eight
neighbours any incoming water would flow. The problem is to compute, for each
cell c, from how many cells of the terrain water would reach c. We show that
this problem can be solved in O(scan(N)) I/Os for a terrain of N cells. Taking
constant factors in the I/O-efficiency into account, our algorithm may be an
order of magnitude faster than the previously known algorithm that is based on
time-forward processing and needs O(sort(N)) I/Os.
",6,5
392,"  Due to hybridization events in evolution, studying two different genes of a
set of species may yield two related but different phylogenetic trees for the
set of species. In this case, we want to measure the dissimilarity of the two
trees. The rooted subtree prune and regraft (rSPR) distance of the two trees
has been used for this purpose. The problem of computing the rSPR distance of
two given trees has many applications but is unfortunately NP-hard. The
previously best approximation algorithm for rSPR distance achieves a ratio of
2.5 and it was open whether a better approximation algorithm for rSPR distance
exists. In this paper, we answer this question in the affirmative by presenting
a cubic-time approximation algorithm for rSPR distance that achieves a ratio of
2. Our algorithm is based on the new notion of key and a number of new
structural lemmas. The algorithm is fairly simple and the proof of its
correctness is intuitively understandable albeit complicated.
",6,4
393,"  Key establishment between any pair of nodes is an essential requirement for
providing secure services in wireless sensor networks. Blom's scheme is a
prominent key management scheme but its shortcomings include large computation
overhead and memory cost. We propose a new scheme in this paper that modifies
Blom's scheme in a manner that reduces memory and computation costs. This paper
also provides the value for secure parameter t such that the network is
resilient.
",5,2
394,"  User distribution in ultra-dense networks (UDNs) plays a crucial role in
affecting the performance of UDNs due to the essential coupling between the
traffic and the service provided by the networks. Existing studies are mostly
based on the assumption that users are uniformly distributed in space. The
non-uniform user distribution has not been widely considered despite that it is
much closer to the real scenario. In this paper, Radiation and Absorbing model
(R&A model) is first adopted to analyze the impact of the non-uniformly
distributed users on the performance of 5G UDNs. Based on the R&A model and
queueing network theory, the stationary user density in each hot area is
investigated. Furthermore, the coverage probability, network throughput and
energy efficiency are derived based on the proposed theoretical model. Compared
with the uniformly distributed assumption, it is shown that non-uniform user
distribution has a significant impact on the performance of UDNs.
",2,5
395,"  Fault-tolerant distributed systems offer high reliability because even if
faults in their components occur, they do not exhibit erroneous behavior.
Depending on the fault model adopted, hardware and software errors that do not
result in a process crashing are usually not tolerated. To tolerate these
rather common failures the usual solution is to adopt a stronger fault model,
such as the arbitrary or Byzantine fault model. Algorithms created for this
fault model, however, are considerably more complex and require more system
resources than the ones developed for less strict fault models. One approach to
reach a middle ground is the non-malicious arbitrary fault model. This model
assumes it is possible to detect and filter faults with a given probability, if
these faults are not created with malicious intent, allowing the isolation and
mapping of these faults to benign faults. In this paper we describe how we
incremented an implementation of active replication in the non-malicious fault
model with a basic type of distributed validation, where a deviation from the
expected algorithm behavior will make a process crash. We experimentally
evaluate this implementation using a fault injection framework showing that it
is feasible to extend the concept of non-malicious failures beyond hardware
failures.
",5,1
396,"  The Longest Common Increasing Subsequence problem (LCIS) is a natural variant
of the celebrated Longest Common Subsequence (LCS) problem. For LCIS, as well
as for LCS, there is an $O(n^2)$-time algorithm and a SETH-based conditional
lower bound of $O(n^{2-\varepsilon})$. For LCS, there is also the
Masek-Paterson $O(n^2 / \log{n})$-time algorithm, which does not seem to adapt
to LCIS in any obvious way. Hence, a natural question arises: does any
(slightly) sub-quadratic algorithm exist for the Longest Common Increasing
Subsequence problem? We answer this question positively, presenting a $O(n^2 /
\log^a{n})$-time algorithm for $a = \frac{1}{6}-o(1)$. The algorithm is not
based on memorizing small chunks of data (often used for logarithmic speedups,
including the ""Four Russians Trick"" in LCS), but rather utilizes a new
technique, bounding the number of significant symbol matches between the two
sequences.
",0,6
397,"  We discuss how to control outputs from deep learning models of text corpora
so as to create contemporary poetic works. We assess whether these controls are
successful in the immediate sense of creating stylo- metric distinctiveness.
The specific context is our piece The Character Thinks Ahead (2016/17); the
potential applications are broad.
",2,5
398,"  We explore the use of SGX enclaves as a means to improve the security of
handling keys and data in storage systems. We study two main configurations for
SGX computations, as they apply to performing data-at-rest encryption in a
storage system. The first configuration aims to protect the encryption keys
used in the encryption process. The second configuration aims to protect both
the encryption keys and the data, thus providing end-to-end security of the
entire data path.
  Our main contribution is an evaluation of the viability of SGX for
data-at-rest encryption from a performance perspective and an understanding of
the details that go into using enclaves in a performance sensitive environment.
Our tests paint a complex picture: On the one hand SGX can indeed achieve high
encryption and decryption throughput, comparable to running without SGX. On the
other hand, there are many subtleties to achieving such performance and careful
design choices and testing are required.
",2,3
399,"  This paper describes the University of Edinburgh's submissions to the WMT17
shared news translation and biomedical translation tasks. We participated in 12
translation directions for news, translating between English and Czech, German,
Latvian, Russian, Turkish and Chinese. For the biomedical task we submitted
systems for English to Czech, German, Polish and Romanian. Our systems are
neural machine translation systems trained with Nematus, an attentional
encoder-decoder. We follow our setup from last year and build BPE-based models
with parallel and back-translated monolingual training data. Novelties this
year include the use of deep architectures, layer normalization, and more
compact models due to weight tying and improvements in BPE segmentations. We
perform extensive ablative experiments, reporting on the effectivenes of layer
normalization, deep architectures, and different ensembling techniques.
",1,2
400,"  We study efficient mechanisms for the query release problem in differential
privacy: given a workload of $m$ statistical queries, output approximate
answers to the queries while satisfying the constraints of differential
privacy. In particular, we are interested in mechanisms that optimally adapt to
the given workload. Building on the projection mechanism of Nikolov, Talwar,
and Zhang, and using the ideas behind Dudley's chaining inequality, we propose
new efficient algorithms for the query release problem, and prove that they
achieve optimal sample complexity for the given workload (up to constant
factors, in certain parameter regimes) with respect to the class of mechanisms
that satisfy concentrated differential privacy. We also give variants of our
algorithms that satisfy local differential privacy, and prove that they also
achieve optimal sample complexity among all local sequentially interactive
private mechanisms.
",1,5
401,"  Canonical models are of central importance in modal logic, in particular as
they witness strong completeness and hence compactness. While the canonical
model construction is well understood for Kripke semantics, non-normal modal
logics often present subtle difficulties - up to the point that canonical
models may fail to exist, as is the case e.g. in most probabilistic logics.
Here, we present a generic canonical model construction in the semantic
framework of coalgebraic modal logic, which pinpoints coherence conditions
between syntax and semantics of modal logics that guarantee strong
completeness. We apply this method to reconstruct canonical model theorems that
are either known or folklore, and moreover instantiate our method to obtain new
strong completeness results. In particular, we prove strong completeness of
graded modal logic with finite multiplicities, and of the modal logic of exact
probabilities.
",1,5
402,"  The IoT (Internet of Things) technology has been widely adopted in recent
years and has profoundly changed the people's daily lives. However, in the
meantime, such a fast-growing technology has also introduced new privacy
issues, which need to be better understood and measured. In this work, we look
into how private information can be leaked from network traffic generated in
the smart home network. Although researchers have proposed techniques to infer
IoT device types or user behaviors under clean experiment setup, the
effectiveness of such approaches become questionable in the complex but
realistic network environment, where common techniques like Network Address and
Port Translation (NAPT) and Virtual Private Network (VPN) are enabled. Traffic
analysis using traditional methods (e.g., through classical machine-learning
models) is much less effective under those settings, as the features picked
manually are not distinctive any more. In this work, we propose a traffic
analysis framework based on sequence-learning techniques like LSTM and
leveraged the temporal relations between packets for the attack of device
identification. We evaluated it under different environment settings (e.g.,
pure-IoT and noisy environment with multiple non-IoT devices). The results
showed our framework was able to differentiate device types with a high
accuracy. This result suggests IoT network communications pose prominent
challenges to users' privacy, even when they are protected by encryption and
morphed by the network gateway. As such, new privacy protection methods on IoT
traffic need to be developed towards mitigating this new issue.
",1,4
403,"  We introduce a new measure of distance between languages based on word
embedding, called word embedding language divergence (WELD). WELD is defined as
divergence between unified similarity distribution of words between languages.
Using such a measure, we perform language comparison for fifty natural
languages and twelve genetic languages. Our natural language dataset is a
collection of sentence-aligned parallel corpora from bible translations for
fifty languages spanning a variety of language families. Although we use
parallel corpora, which guarantees having the same content in all languages,
interestingly in many cases languages within the same family cluster together.
In addition to natural languages, we perform language comparison for the coding
regions in the genomes of 12 different organisms (4 plants, 6 animals, and two
human subjects). Our result confirms a significant high-level difference in the
genetic language model of humans/animals versus plants. The proposed method is
a step toward defining a quantitative measure of similarity between languages,
with applications in languages classification, genre identification, dialect
identification, and evaluation of translations.
",0,4
404,"  The Internet of Things (IoT) is experiencing fast adoption in the society,
from industrial to home applications. The number of deployed sensors and
connected devices to the Internet is changing our perspective and the way we
understand the world. The development and generation of IoT applications is
just starting and they will modify our physical and virtual lives, from how we
control remotely appliances at home to how we deal with insurance companies in
order to start insurance schemes via smart cards. This massive deployment of
IoT devices represents a tremendous economic impact and at the same time offers
multiple opportunities. However, the potential of IoT is underexploited and day
by day this gap between devices and useful applications is getting bigger.
Additionally, the physical and cyber worlds are largely disconnected, requiring
a lot of manual efforts to integrate, find, and use information in a meaningful
way.
  To build a connection between the physical and the virtual, we need a
knowledge framework that allow bilateral understandings, devices producing
data, information systems managing the data and applications transforming
information into meaningful knowledge. The first column in this series in the
previous issue of this magazine titled ""Internet of Things to Smart IoT Through
Semantic, Cognitive, and Perceptual Computing,"" reviews IoT growth and
potential that have energized research and technology development, centered on
aspects of Artificial Intelligence to build future intelligent system. This
column steps back and demonstrates the benefits of using semantic web
technologies to get meaningful knowledge from sensor data to design smart
systems.
",5,2
405,"  The paper focuses on improving the spectrum sharing using NSU, FLS and
Traffic Pattern Prediction and also made comparison that traffic pattern
prediction provides a better way of improving the spectrum utilization and
avoids the spectrum scarcity. This helps to increase the number of active
users, ease of identification of optimal users to use the spectrum with
maximized coverage of the spectrum.. We experimentally evaluated the
effectiveness of our approach using NS2 simulator and showed that after
predicting the traffic, we can accommodate more number of users and avoiding
Interference.
",2,6
406,"  The purpose of incremental cryptography is to allow the updating of
cryptographic forms of documents undergoing modifications, more efficiently
than if we had to recompute them from scratch. This paper defines a framework
for using securely a variant of the incremental hash function designed by
Bok-Min Goi et al. The condition of use of their hash function is somehow
impractical since they assume that the blocks of the message are all distinct.
In this paper we show how we can discard this strong assumption so as to
construct the first practical incremental asymmetric signature scheme that
keeps efficient update operations. Finally, as the proposed scheme has the
defect to severely expand the signature size, we propose a solution which
drastically reduces this drawback.
",3,2
407,"  Assigning a positive or negative score to a word out of context (i.e. a
word's prior polarity) is a challenging task for sentiment analysis. In the
literature, various approaches based on SentiWordNet have been proposed. In
this paper, we compare the most often used techniques together with newly
proposed ones and incorporate all of them in a learning framework to see
whether blending them can further improve the estimation of prior polarity
scores. Using two different versions of SentiWordNet and testing regression and
classification models across tasks and datasets, our learning approach
consistently outperforms the single metrics, providing a new state-of-the-art
approach in computing words' prior polarity for sentiment analysis. We conclude
our investigation showing interesting biases in calculated prior polarity
scores when word Part of Speech and annotator gender are considered.
",3,2
408,"  Traditional password based authentication schemes are mostly considered in
single server environments. They are unfitted for the multi-server environments
from two aspects. On the one hand, users need to register in each server and to
store large sets of data, including identities and passwords. On the other
hand, servers are required to store a verification table containing user
identities and passwords. Recently, On the base on Sood et al.'s
protocol(2011), Li et al. proposed an improved dynamic identity based
authentication and key agreement protocol for multi-server architecture(2012).
Li et al. claims that the proposed scheme can make up the security weaknesses
of Sood et al.'s protocol. Unfortunately, our further research shows that Li et
al.'s protocol contains several drawbacks and can not resist some types of
known attacks, such as replay attack, Deny-of-Service attack, internal attack,
eavesdropping attack, masquerade attack, and so on. In this paper, we further
propose a light dynamic pseudonym identity based authentication and key
agreement protocol for multi-server architecture. In our scheme, service
providing servers don't need to maintain verification tables for users. The
proposed protocol provides not only the declared security features in Li et
al.'s paper, but also some other security features, such as traceability and
identity protection.
",1,5
409,"  We describe a graph visualization tool for visualizing Java bytecode. Our
tool, which we call J-Viz, visualizes connected directed graphs according to a
canonical node ordering, which we call the sibling-first recursive (SFR)
numbering. The particular graphs we consider are derived from applying Shiver's
k-CFA framework to Java bytecode, and our visualizer includes helpful links
between the nodes of an input graph and the Java bytecode that produced it, as
well as a decompiled version of that Java bytecode. We show through several
case studies that the canonical drawing paradigm used in J-Viz is effective for
identifying potential security vulnerabilities and repeated use of the same
code in Java applications.
",1,5
410,"  We present Nematus, a toolkit for Neural Machine Translation. The toolkit
prioritizes high translation accuracy, usability, and extensibility. Nematus
has been used to build top-performing submissions to shared translation tasks
at WMT and IWSLT, and has been used to train systems for production
environments.
",1,4
411,"  Notwithstanding the significant research effort Network Function
Virtualization (NFV) architectures received over the last few years little
attention has been placed on optimizing proactive caching when considering it
as a service chain. Since caching of popular content is envisioned to be one of
the key technologies in emerging 5G networks to increase network efficiency and
overall end user perceived quality of service we explicitly consider in this
paper the interplay and subsequent optimization of caching based VNF service
chains. To this end, we detail a novel mathematical programming framework
tailored to VNF caching chains and detail also a scale-free heuristic to
provide competitive solutions for large network instances since the problem
itself can be seen as a variant of the classical NP-hard Uncapacitated Facility
Location (UFL) problem. A wide set of numerical investigations are presented
for characterizing the attainable system performance of the proposed schemes.
",3,1
412,"  Hadoop is a distributed batch processing infrastructure which is currently
being used for big data management. The foundation of Hadoop consists of Hadoop
Distributed File System or HDFS. HDFS presents a client server architecture
comprised of a NameNode and many DataNodes. The NameNode stores the metadata
for the DataNodes and DataNode stores application data. The NameNode holds file
system metadata in memory, and thus the limit to the number of files in a file
system is governed by the amount of memory on the NameNode. Thus when the
memory on NameNode is full there is no further chance of increasing the cluster
capacity. In this paper we have used the concept of cache memory for handling
the issue of NameNode scalability. The focus of this paper is to highlight our
approach that tries to enhance the current architecture and ensure that
NameNode does not reach its threshold value soon.
",5,1
413,"  Collision detection algorithms are used in aerospace, swarm robotics,
automotive, video gaming, dynamics simulation and other domains. As many
applications of collision detection run online, timing requirements are imposed
on the algorithm runtime: algorithms must, at a minimum, keep up with the
passage of time. In practice, this places a limit on the number of objects, n,
that can be tracked at the same time. In this paper, we improve the scalability
of collision detection, effectively raising the limit n for online object
tracking.
  The key to our approach is the use of a four-dimensional axis-aligned
bounding box (AABB) tree, which stores each object's three-dimensional
occupancy region in space during a one-dimensional interval of time. This
improves efficiency by permitting per-object variable times steps. Further, we
describe partitioning strategies that can decompose the 4D AABB tree search
into several smaller-dimensional problems that can be solved in parallel. We
formalize the collision detection problem and prove our algorithm's
correctness. We demonstrate the feasibility of online collision detection for
an orbital space debris application, using publicly available data on the full
catalog of n=16848 objects provided by www.space-track.org.
",6,2
414,"  We show the reverse greedy algorithm is between a $(2k-2)$- and a
$2k$-approximation for $k$-center.
",2,5
415,"  This paper introduces a new machine architecture for evaluating lambda
expressions using the normal-order reduction, which guarantees that every
lambda expression will be evaluated if the expression has its normal form and
the system has enough memory. The architecture considered here operates using
heap memory only. Lambda expressions are represented as graphs, and all
algorithms used in the processing unit of this machine are non-recursive.
",3,2
416,"  The success of Google's Pregel framework in distributed graph processing has
inspired a surging interest in developing Pregel-like platforms featuring a
user-friendly ""think like a vertex"" programming model. Existing Pregel-like
systems support a fault tolerance mechanism called checkpointing, which
periodically saves computation states as checkpoints to HDFS, so that when a
failure happens, computation rolls back to the latest checkpoint. However, a
checkpoint in existing systems stores a huge amount of data, including vertex
states, edges, and messages sent by vertices, which significantly degrades the
failure-free performance. Moreover, the high checkpointing cost prevents
frequent checkpointing, and thus recovery has to replay all the computations
from a state checkpointed some time ago.
  In this paper, we propose a novel checkpointing approach which only stores
vertex states and incremental edge updates to HDFS as a lightweight checkpoint
(LWCP), so that writing an LWCP is typically tens of times faster than writing
a conventional checkpoint. To recover from the latest LWCP, messages are
generated from the vertex states, and graph topology is recovered by replaying
incremental edge updates. We show how to realize lightweight checkpointing with
minor modifications of the vertex-centric programming interface. We also apply
the same idea to a recently-proposed log-based approach for fast recovery, to
make it work efficiently in practice by significantly reducing the cost of
garbage collection of logs. Extensive experiments on large real graphs verified
the effectiveness of LWCP in improving both failure-free performance and the
performance of recovery.
",2,5
417,"  We propose a formal foundation for reasoning about access control policies
within a Dynamic Coalition, defining an abstraction over existing access
control models and providing mechanisms for translation of those models into
information-flow domain. The abstracted information-flow domain model, called a
Common Representation, can then be used for defining a way to control the
evolution of Dynamic Coalitions with respect to information flow.
",2,3
